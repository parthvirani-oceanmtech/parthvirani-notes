{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Flutter%20Notes/Code%20Practice/Best%20Code%20Pratice%20In%20Flutter%20-%20OceanMTech%20Pvt%20Ltd%20Coding%20Rules/","title":"Best Code Pratice In Flutter   OceanMTech Pvt Ltd Coding Rules","text":"<p>**</p> <ol> <li> <p>Kyarey setState use nathi karavanu always proper state managment use karvanu che.</p> </li> <li> <p>State managment mate bloc(preferred) or provider bemathi aek j use karvanu rehshe.</p> </li> <li> <p>Never use getx for state managment.</p> <ul> <li>Disadvantages: (Ref: Medium)<ul> <li>Getx Provide Simplicity</li> <li>GetX force to replacing Flutter</li> <li>GetX tends to use anti-patterns</li> <li>GetX tries to do everything</li> <li>The level of documentation for GetX needs improvement.</li> <li>The Unusual Number of API Elements in GetX</li> </ul> </li> </ul> </li> <li> <p>Follow proper project architecture(MVC(Medium Project) or MVVM(Large Project))</p> </li> <li>Official Documentation Read karvanu aavdvu jove and ae must required skills che.</li> <li> <p>Effective Dart Programming Writing Style:\u00a0</p> <ul> <li>Saro code write karvo and ae sari code pratice che and most important points che.</li> <li>Consistent naming, ordering and format maintain karvathi ae code no look maintain thay shake che.</li> <li>powerful pattern-matching hardware mate saru pade ke pattern match kari ne aapane suggestion provide kari aape (AI - Github Copilot)</li> <li>and jo consistent code style hoy aakha dart na ecosystem mate to batha mate easier reshe ke aemathi shikhva mate and aek bija na code ma contributation karva mate.</li> <li>and Oceanmtech aa code nu structure maintain karvu ae mandotory che je ne aapde universal code structure na rules par thi j aapde define karel che.</li> <li>Aapde official dart language ni guide ne follow karvanu che:</li> <li>Dart Programming Writing Rules (Read &amp; Implement karvanu che)</li> </ul> </li> <li> <p>OceanMTech na aek pan flutter developer ne jyare code write karta hoy tyare keyboard same jovu na padvu jove aena mate typing speed mate ni daily pratice karvani che and biju keyabord and mouse ni click fast action ma perform thay ae mandatory che.</p> </li> <li> <p>Jyare code write karo cho aena file name and file location always yad rakhvana and aej rite function name variable name ae pan yad rakhvana and aena name pan function na according j lakhvana jenathi jyare ae module nu logic update thay tyare direct file name par thi ae file par jay shakay because jyare projects ma 1000 files hoy tyare manually find karvi ae time consuming process che.</p> </li> <li> <p>Use Descriptive Variable and Function Names:  </p> <ul> <li>koi pan variables, functions, classes and widget na name descriptive choose karvana reshe, je aapane code vadhu samajavama and bija pan easy way ma samji shake and in future aapde aa project par return work start karvi to samajava mate easy reshe. so single-letter or cryptic variable names ne avoid karvu jaruri che, nichenu exaple refer karvu more details mate.</li> <li>Read More - [[Explain Use Descriptive Variable and Function Names]]</li> </ul> </li> <li> <p>Organize Code into Small Functions and Classes:</p> <ul> <li>Je pan code write karo ae small, reusable function and classes ma hovo jaruri che because function find karva ae more easy che instead code find karvo, so aa pratice aek code ne readability and maintainability banavi aape che, and flutter ma widgets ae aek good example che jema aa pratice follow thayeli che.</li> <li>Read More - [[Explain Organize Code into Small Functions and Classes]]</li> </ul> </li> <li> <p>Properly Comment Your Code:</p> <ul> <li>Je pan function create karo aeni uper comment kari ne aeno use explain karvano jenathi code understand karvo easy pade. and /// no use karavano jyare class methods and functions nu documentation write karo tyare.</li> <li>For example jo flutter ae aenu documentation j na banavyu hot to aapde shikhva mate ketlu hard thay so better che ke properly comment karvani code ma.</li> <li>Read More - [[Explain Properly Comment Your Code]]</li> </ul> </li> <li> <p>Use Proper Widget Composition: [(Read More)]    </p> <ul> <li>aapde flutter ma jyare user interfaces(UI) banavi ae tyare aena widget ne composing(Smalled Blocks) kari ne banavviae ae j best pratice che.</li> <li>widget ni hierarchy create karvathi complex ui ne pan easy way ma write kari shakay che.\u00a0</li> <li>everytime setState and provider no nana functions mate use nai karvano so better che aene seprate design kari ne aena buildcontext and updates ne control karvanu.</li> <li>Read More - [[Use Proper Widget Composition]]</li> </ul> </li> <li> <p>Avoid Deep Widget Nesting: </p> <ul> <li>Deep widget nesting means aek widget ni niche second and second ni niche third and third ni niche forth and so on. (example: ListView.builder)</li> <li>Deep widget nesting ae code read and maintain karvama problem create karshe means aene samajvo ae hard thay jay che, so tamne jyare aevu lage ke bow batha widget nested thay che tyare aene smaller widget ma break karvanu consider karvu jove.</li> <li>Read More - [[Avoid Deep Widget Nesting]]</li> </ul> </li> <li> <p>Keep UI and Business Logic Separated: </p> <ul> <li>Separate your UI code from your business logic. This can help with testing and code maintainability. Consider using packages like bloc(preferred) or provider for state management.</li> <li>Read More - [[Keep UI and Business Logic Separated]]</li> </ul> </li> <li> <p>Use Flutter DevTools: </p> <ul> <li>Flutter provides a powerful set of tools for debugging and profiling your application. Familiarize yourself with Flutter DevTools to help identify and resolve issues in your code efficiently.</li> <li>Read More - [[Use Flutter DevTools]]</li> </ul> </li> <li> <p>Test Your Code:</p> <ul> <li>Write unit tests and widget tests to ensure that your code functions as expected. Flutter has built-in support for testing, making it easy to write and run tests.</li> <li>Read More - [[Test Your Code]]</li> </ul> </li> <li> <p>Stay Up-to-Date:</p> <ul> <li>Flutter is an evolving framework, and best practices can change over time. Keep up-to-date with Flutter updates, new packages, and community recommendations to improve your code quality.</li> <li>Read More - [[Stay Up-to-Date]]</li> </ul> </li> <li> <p>Analyze and Format Your Code:</p> <ul> <li>Use tools like dart analyze and dart format to ensure your code adheres to best practices and formatting guidelines.</li> <li>Read More - [[Explain Analyse and Format Your Code]]</li> </ul> </li> <li> <p>Document Your Code:</p> <ul> <li>Use documentation comments to describe the purpose, parameters, and return values of functions and classes. This helps other developers understand how to use your code.</li> <li>Read More - [[Explain Document Your Code]]</li> </ul> </li> <li> <p>Use Version Control:</p> <ul> <li>Always use version control systems like Git to track changes in your code. This helps you collaborate with others and revert changes if needed.</li> <li>Read More - [[Explain Use Version Control]]</li> </ul> </li> </ol> <p>Remember that good code practices may vary depending on the project and team, but these guidelines can serve as a solid foundation for writing clean and maintainable Flutter code. **</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Avoid%20Deep%20Widget%20Nesting/","title":"Avoid Deep Widget Nesting","text":"<p>\"Avoid Deep Widget Nesting\" is a best practice in Flutter development that suggests minimizing the depth of widget hierarchies in your user interface (UI) to improve code readability, performance, and maintainability. Deep widget nesting occurs when you have a complex hierarchy of nested widgets, making your code harder to understand and potentially leading to performance issues.</p> <p>Here's why you should avoid deep widget nesting and some tips on how to do so:</p> <ol> <li> <p>Readability: Deeply nested widget hierarchies can be challenging to read and understand, especially for other developers who may work on the code or for yourself when you revisit the code later. Keeping the widget tree shallow makes it easier to follow the flow of your UI.</p> </li> <li> <p>Debugging: Debugging becomes more difficult with deep nesting. When an issue arises in your UI, locating the source of the problem and understanding the context becomes more challenging. Shallow hierarchies simplify debugging.</p> </li> <li> <p>Performance: Although Flutter is designed to be fast, excessive widget nesting can lead to performance problems. Each widget in the hierarchy comes with some overhead. Deep nesting can result in unnecessary widget rebuilds, leading to reduced performance.</p> </li> <li> <p>State Management: Dealing with state management in deep widget hierarchies can be tricky. Passing data down the widget tree becomes cumbersome, and you may be tempted to use global state management solutions excessively. Shallow hierarchies make it easier to manage and pass state.</p> </li> </ol> <p>To avoid deep widget nesting in your Flutter code:</p> <ol> <li> <p>Create Custom Widgets: Break down your UI into small, reusable custom widgets. Each widget should have a single responsibility. This practice allows you to compose your UI from simpler building blocks instead of creating complex widgets.</p> </li> <li> <p>Use Layout Widgets: Flutter provides layout widgets like <code>Column</code>, <code>Row</code>, <code>ListView</code>, <code>Stack</code>, and <code>GridView</code> to help you arrange and position child widgets efficiently. Utilize these layout widgets to structure your UI without excessive nesting.</p> </li> <li> <p>Extract Reusable Components: Identify UI elements that appear in multiple places within your app and extract them into separate custom widgets. This promotes code reuse and reduces nesting.</p> </li> <li> <p>Use <code>Builder</code> Widgets: In cases where you need to conditionally build parts of your UI, consider using builder widgets like <code>Builder</code> or <code>FutureBuilder</code>. These widgets can help you avoid adding unnecessary nesting.</p> </li> <li> <p>Consider Separation of Concerns: Separate your UI code from your business logic. This separation often leads to cleaner and less deeply nested widget hierarchies. Use state management solutions like Provider or Riverpod to manage and distribute data across your UI components efficiently.</p> </li> <li> <p>Flutter DevTools: Use the Flutter DevTools suite, particularly the \"Widget Inspector,\" to visualize your widget hierarchy and identify areas where nesting can be reduced.</p> </li> </ol> <p>Here's an example of how you might refactor a deeply nested widget hierarchy:</p> <p>Before (Deep Nesting):</p> <pre><code>Column(\nchildren: [\nText('Hello'),\nContainer(\nchild: Column(\nchildren: [\nText('World'),\n// More nested widgets...\n],\n),\n),\n],\n)\n</code></pre> <p>After (Reduced Nesting):</p> <pre><code>Column(\nchildren: [\nText('Hello'),\nMyCustomWidget(), // Custom widget that handles nested content\n],\n)\n</code></pre> <p>In the refactored example, the deeply nested structure has been reduced to a more readable and maintainable layout by encapsulating the complex nesting within a custom widget (<code>MyCustomWidget</code>).</p> <p>By following these practices and striving to keep your widget hierarchy shallow and organized, you can create Flutter applications that are easier to understand, maintain, and perform efficiently.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Analyse%20and%20Format%20Your%20Code/","title":"Explain Analyse and Format Your Code","text":"<p>\"Analyse and Format Your Code\" are essential practices in software development, including Flutter development, that involve reviewing your code for potential issues, errors, and style inconsistencies while also ensuring that your code adheres to a consistent and readable format. These practices help maintain code quality, readability, and consistency throughout your projects. Here's a breakdown of these two practices:</p> <p>Analyze Your Code:</p> <p>Analyzing your code involves using static analysis tools and linters to identify potential issues, bugs, and code smells without executing the code. In the context of Flutter development, you can use the Dart analysis tool, which is integrated into Flutter's development environment. Here's how to analyze your code:</p> <ol> <li> <p>Use the Dart Analyzer: The Dart Analyzer is a static analysis tool that comes bundled with the Dart SDK and Flutter. It checks your code for a wide range of issues, including syntax errors, type mismatches, unused variables, and more.</p> </li> <li> <p>IDE Integration: Most popular Flutter IDEs, such as Visual Studio Code and Android Studio, integrate the Dart Analyzer seamlessly. You'll see error and warning highlights in your code editor as you write code.</p> </li> <li> <p>Command-Line Usage: You can run the Dart Analyzer from the command line using the <code>dart analyze</code> command. This is useful for batch analysis or integrating with continuous integration (CI) pipelines.</p> </li> <li> <p>Analyze and Address Issues: Regularly run the analyzer on your project and address any issues it identifies. The goal is to have a codebase with zero analysis issues.</p> </li> <li> <p>Custom Analysis Rules: You can define custom analysis rules for your codebase using packages like <code>lint</code>. This allows you to enforce project-specific coding standards and best practices.</p> </li> </ol> <p>Format Your Code:</p> <p>Formatting your code ensures that it follows a consistent style and layout, making it more readable and maintainable. For Dart and Flutter development, the recommended tool for code formatting is <code>dartfmt</code>. Here's how to format your code:</p> <ol> <li> <p>Use <code>dartfmt</code>: <code>dartfmt</code> is a command-line tool that automatically formats your Dart and Flutter code according to the official Dart style guide. It enforces consistent code formatting across your project.</p> </li> <li> <p>IDE Integration: Most Flutter IDEs integrate <code>dartfmt</code> and provide automatic code formatting when you save a file or through keyboard shortcuts (e.g., Shift + Alt + F in Visual Studio Code).</p> </li> <li> <p>Pre-commit Hooks: Set up pre-commit hooks in your version control system (e.g., Git hooks) to automatically format your code before each commit. This ensures that no unformatted code makes it into your repository.</p> </li> <li> <p>Continuous Integration: Incorporate code formatting into your CI/CD pipeline. Ensure that your codebase remains correctly formatted throughout development.</p> </li> <li> <p>Custom Style Configuration: If needed, you can configure your own code style rules by creating an <code>.analysis_options</code> file in your project and specifying your preferred formatting rules. You can also adjust line lengths, indentation, and other formatting options.</p> </li> <li> <p>EditorConfig: Consider using an <code>.editorconfig</code> file to define project-wide coding style preferences, including indentation, line endings, and other code formatting settings. Many IDEs support EditorConfig files.</p> </li> </ol> <p>By consistently analyzing and formatting your code, you improve code quality, readability, and maintainability. It also helps ensure that your codebase is consistent, which is especially valuable when working in a team or on open-source projects. Additionally, it reduces the likelihood of introducing subtle bugs and issues due to code formatting discrepancies.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Document%20Your%20Code/","title":"Explain Document Your Code","text":"<p>\"Document Your Code\" is a crucial best practice in software development, including Flutter development. It involves adding comments, annotations, and documentation to your code to make it more understandable and maintainable for both yourself and other developers who may work on the project. Proper documentation helps convey the purpose, usage, and implementation details of your code. Here's why documenting your code is important and how to do it effectively:</p> <p>Why Document Your Code:</p> <ol> <li> <p>Code Understanding: Well-documented code is easier to understand. Developers can quickly grasp the intent and functionality of code segments, even if they didn't write them.</p> </li> <li> <p>Collaboration: When working on a team project, documentation is crucial for effective collaboration. It helps team members understand each other's contributions and reduces the learning curve for new team members.</p> </li> <li> <p>Maintenance: Code evolves over time. Documentation serves as a guide for maintaining and updating the code. When you revisit your code after months or years, clear documentation can be a lifesaver.</p> </li> <li> <p>Debugging: Comments can provide insights into the logic and reasoning behind specific code decisions. This can be invaluable when debugging issues or addressing unexpected behavior.</p> </li> <li> <p>Onboarding: For open-source projects or when onboarding new team members, well-documented code makes it easier for newcomers to get up to speed quickly.</p> </li> </ol> <p>How to Document Your Code Effectively:</p> <ol> <li>Use Meaningful Variable and Function Names:</li> <li> <p>Choose descriptive and meaningful names for variables, functions, classes, and other code elements. Well-named entities reduce the need for excessive comments.</p> </li> <li> <p>Comments:</p> </li> <li>Add comments to explain complex algorithms, non-obvious logic, or any part of your code that may not be immediately clear to others. Use comments to provide context, not just describe what the code does.</li> <li>Follow a consistent commenting style, such as using double slashes (<code>//</code>) for single-line comments and <code>///</code> for documentation comments that generate documentation files.</li> </ol> <pre><code>// Calculate the sum of two numbers\nint add(int a, int b) {\nreturn a + b;\n}\n</code></pre> <ol> <li>Documentation Comments:</li> <li>Use documentation comments (also known as doc comments) for classes, functions, methods, and other public API elements. These comments can be automatically generated into documentation using tools like Dart's <code>dartdoc</code>.</li> <li>Include details about the purpose of the entity, its parameters, return values, and usage examples.</li> </ol> <pre><code>/// A class representing a person.\nclass Person {\n/// The person's name.\nString name;\n/// The person's age.\nint age;\n/// Creates a new [Person] instance with the given [name] and [age].\nPerson(this.name, this.age);\n}\n</code></pre> <ol> <li>Keep Comments Updated:</li> <li> <p>Maintain your comments alongside code changes. Outdated comments can mislead developers and lead to misunderstandings.</p> </li> <li> <p>Coding Standards and Style Guides:</p> </li> <li> <p>Adhere to coding standards and style guides, which often include recommendations for commenting and documentation practices. In the Dart and Flutter ecosystem, you can refer to the official Dart Effective Dart Style Guide for guidance.</p> </li> <li> <p>Javadoc/Docstring Conventions:</p> </li> <li> <p>Consider following Javadoc or docstring conventions for documentation comments. These conventions specify how to format comments to ensure consistency and generate useful documentation.</p> </li> <li> <p>Use Inline Comments Sparingly:</p> </li> <li> <p>Inline comments should be used judiciously and only for complex or non-intuitive code sections. The code itself should be self-explanatory with clear variable names and functions.</p> </li> <li> <p>Documentation Generators:</p> </li> <li> <p>Leverage documentation generators like <code>dartdoc</code> to automatically generate documentation from your code comments. This makes it easier to create and maintain documentation.</p> </li> <li> <p>README Files and Documentation Files:</p> </li> <li>Beyond inline comments and doc comments, maintain a <code>README.md</code> file in your project's root directory to provide a high-level overview of your project and its usage. Consider using Markdown for formatting.</li> </ol> <p>By following these practices, you can create well-documented Flutter projects that are more understandable, maintainable, and accessible to other developers. Clear and concise documentation is an investment in the long-term success and sustainability of your codebase.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Follow%20the%20Dart%20Style%20Guide/","title":"Explain Follow the Dart Style Guide","text":"<p>\"Follow the Dart Style Guide\" is a fundamental best practice in Dart and Flutter development. Dart, the programming language used for Flutter, has an official style guide that provides a set of conventions and guidelines for writing clean, consistent, and readable Dart code. Adhering to this style guide helps maintain code quality, improves codebase readability, and promotes consistency in coding practices across teams and projects.</p> <p>Here's a breakdown of why it's essential to follow the Dart Style Guide and how to do so effectively:</p> <p>Why Follow the Dart Style Guide:</p> <ol> <li> <p>Consistency: Consistency in code style makes your codebase more predictable and easier to understand for both you and other developers who may work on the project.</p> </li> <li> <p>Readability: A consistent style enhances code readability by ensuring that code elements like variable names, formatting, and indentation follow a common pattern.</p> </li> <li> <p>Collaboration: When working on team projects, following a style guide ensures that all team members write code in a consistent manner, reducing friction during code reviews and collaboration.</p> </li> <li> <p>Maintainability: A well-structured and consistently styled codebase is easier to maintain and refactor. It reduces the likelihood of introducing new bugs during code changes.</p> </li> <li> <p>Scalability: As your Flutter project grows, adhering to a style guide becomes increasingly important. It helps manage complexity and ensures that your code remains manageable and extendable.</p> </li> </ol> <p>How to Follow the Dart Style Guide Effectively:</p> <ol> <li>Read the Official Dart Style Guide:</li> <li>Familiarize yourself with the official Dart Style Guide, which is available at dart.dev/guides/style.</li> <li> <p>Pay attention to the conventions and recommendations for naming conventions, code formatting, and coding practices.</p> </li> <li> <p>Configure Your IDE/Editor:</p> </li> <li> <p>Configure your development environment, including IDEs like Visual Studio Code, IntelliJ IDEA, or Android Studio, to automatically apply Dart's formatting rules. The Dart SDK comes with a tool called <code>dartfmt</code> that helps format your code to comply with the style guide.</p> </li> <li> <p>Use Linters:</p> </li> <li>Consider using linters like <code>dartanalyzer</code> to perform static code analysis. Linters can identify code style violations and potential issues in your code.</li> <li> <p>Popular linters include <code>pedantic</code>, which enforces a strict adherence to the Dart Style Guide, and <code>lint</code>, which allows you to customize linting rules.</p> </li> <li> <p>Enable Lint Rules:</p> </li> <li> <p>If you choose to use linters, enable and configure lint rules in your Dart project. You can do this by adding a <code>.analysis_options</code> file to your project's root directory.</p> </li> <li> <p>Naming Conventions:</p> </li> <li> <p>Follow Dart's naming conventions for variables, functions, classes, and constants. Use <code>lowerCamelCase</code> for variable and function names, and <code>UpperCamelCase</code> for class names.</p> </li> <li> <p>Code Formatting:</p> </li> <li> <p>Ensure that your code is formatted consistently. Use <code>dartfmt</code> or the built-in formatting features of your IDE to automatically apply formatting rules.</p> </li> <li> <p>Documentation Comments:</p> </li> <li> <p>Include documentation comments (doc comments) for public APIs, classes, functions, and methods, following the conventions outlined in the style guide.</p> </li> <li> <p>Imports:</p> </li> <li> <p>Organize import statements alphabetically and separate them into three groups: Dart imports, package imports, and local imports.</p> </li> <li> <p>Whitespace and Indentation:</p> </li> <li> <p>Follow the recommended rules for whitespace, indentation, line lengths, and line breaks as outlined in the style guide.</p> </li> <li> <p>Code Structure:</p> <ul> <li>Organize your code logically, breaking it into functions, classes, and libraries that have clear responsibilities and interfaces.</li> </ul> </li> <li> <p>Avoidance of Deprecated Features:</p> <ul> <li>Stay updated with changes in Dart and Flutter and avoid using deprecated features and libraries. Refer to official documentation and release notes for updates.</li> </ul> </li> <li> <p>Regular Code Reviews:</p> <ul> <li>Conduct regular code reviews within your team to ensure that the style guide is consistently followed. Code reviews are an opportunity to catch and correct style violations early.</li> </ul> </li> </ol> <p>By following the Dart Style Guide, you contribute to writing clean, consistent, and maintainable Dart and Flutter code. This adherence to best practices enhances code quality, fosters collaboration, and facilitates long-term project sustainability.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Organize%20Code%20into%20Small%20Functions%20and%20Classes/","title":"Explain Organize Code into Small Functions and Classes","text":"<p>\"Organize Code into Small Functions and Classes\" is a best practice in software development, including Dart and Flutter development, that involves breaking down your code into smaller, more focused functions and classes. The goal is to improve code readability, maintainability, and reusability while reducing complexity. This practice aligns with the principles of modular programming and the Single Responsibility Principle (SRP) from SOLID design principles. Here's why it's important and how to implement it effectively:</p> <p>Why Organize Code into Small Functions and Classes:</p> <ol> <li> <p>Readability: Smaller functions and classes are easier to read and understand. They allow developers to focus on one specific task or responsibility at a time.</p> </li> <li> <p>Maintainability: Smaller code units are easier to maintain and debug. When a bug occurs or a change is needed, it's quicker and more straightforward to locate the relevant code.</p> </li> <li> <p>Reusability: Well-structured, small functions and classes can be reused in different parts of your application or in other projects, reducing duplication of code.</p> </li> <li> <p>Testing: Smaller functions are easier to test. You can write unit tests to verify the behavior of individual functions or methods, improving code reliability.</p> </li> <li> <p>Scalability: As your Flutter project grows, organizing code into smaller units helps manage complexity and prevents your codebase from becoming unwieldy and difficult to manage.</p> </li> </ol> <p>How to Organize Code into Small Functions and Classes Effectively:</p> <ol> <li>Single Responsibility Principle (SRP):</li> <li> <p>Follow the SRP, which states that a function or class should have only one reason to change. If a function or class has multiple responsibilities, consider breaking it into smaller, more focused units.</p> </li> <li> <p>Descriptive Names:</p> </li> <li> <p>Choose clear, descriptive names for functions and classes that reflect their purpose and responsibility. Names should make it evident what the function or class does.</p> </li> <li> <p>Short and Focused Functions:</p> </li> <li> <p>Aim for short, focused functions that perform a single task. If a function is longer than a screen's worth of code or has multiple levels of indentation, consider splitting it into smaller functions.</p> </li> <li> <p>Parameter Lists:</p> </li> <li> <p>Limit the number of parameters a function accepts. Functions with many parameters can be challenging to use and understand. If necessary, use data structures like objects or maps to group related parameters.</p> </li> <li> <p>Avoid Deep Nesting:</p> </li> <li> <p>Minimize nesting of functions and classes. Deeply nested code can become difficult to follow. Consider flattening your code structure when possible.</p> </li> <li> <p>Use Helper Functions and Methods:</p> </li> <li> <p>Encapsulate common or repetitive logic into helper functions or methods. This reduces duplication and promotes reusability.</p> </li> <li> <p>Separation of Concerns:</p> </li> <li> <p>Divide your code into separate functions or classes based on different concerns. For example, separate UI logic from business logic and data access.</p> </li> <li> <p>Class Encapsulation:</p> </li> <li> <p>In Flutter, use classes to encapsulate related UI elements and behavior. For example, create custom widgets for specific UI components to keep your widget tree organized.</p> </li> <li> <p>Documentation:</p> </li> <li> <p>Include comments or documentation that explain the purpose and usage of functions and classes. Well-documented code helps other developers understand your intentions.</p> </li> <li> <p>Code Reviews:</p> <ul> <li>Conduct code reviews within your team to ensure that the codebase adheres to the practice of organizing code into small, focused units. Code reviews provide opportunities to identify and address issues.</li> </ul> </li> </ol> <p>Here's an example of organizing code into small functions and classes in Dart:</p> <pre><code>class ShoppingCart {\nList&lt;CartItem&gt; items = [];\nvoid addItem(Product product, int quantity) {\nfinal item = CartItem(product, quantity);\nitems.add(item);\n}\ndouble calculateTotalPrice() {\ndouble totalPrice = 0;\nfor (var item in items) {\ntotalPrice += item.product.price * item.quantity;\n}\nreturn totalPrice;\n}\n}\nclass CartItem {\nfinal Product product;\nfinal int quantity;\nCartItem(this.product, this.quantity);\n}\n</code></pre> <p>In this example, the code is organized into two classes, <code>ShoppingCart</code> and <code>CartItem</code>, each with a single, well-defined responsibility. This makes the code more modular and easier to maintain.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Properly%20Comment%20Your%20Code/","title":"Explain Properly Comment Your Code","text":"<p>\"Properly Comment Your Code\" is a coding practice that involves adding comments and documentation to your source code to provide explanations, context, and additional information about the code's functionality. Well-placed comments make your code more understandable and maintainable, especially for yourself and other developers who may work on the codebase in the future. Here's why it's important and how to do it effectively:</p> <p>Why Properly Comment Your Code:</p> <ol> <li> <p>Code Understanding: Comments help developers understand the purpose and behavior of the code, making it easier to work with and modify.</p> </li> <li> <p>Documentation: Comments serve as documentation, providing insights into the code's intent, usage, and edge cases. They can also describe algorithms, data structures, and complex logic.</p> </li> <li> <p>Collaboration: When working in a team, comments enable effective collaboration. Team members can understand each other's code, even if they didn't write it.</p> </li> <li> <p>Maintenance: Code evolves over time. Comments provide guidance for maintaining and updating the codebase, preventing errors during modifications.</p> </li> <li> <p>Debugging: Comments can be valuable when debugging, as they provide context that can help you locate and fix issues more efficiently.</p> </li> <li> <p>Onboarding: For new team members or contributors, well-documented code with comments is easier to onboard to. It reduces the learning curve.</p> </li> </ol> <p>How to Properly Comment Your Code Effectively:</p> <ol> <li>Use Meaningful Comments:</li> <li> <p>Write comments that add value. Avoid obvious or redundant comments that merely restate the code. Focus on explaining why the code does something, not just what it does.</p> </li> <li> <p>Comment Header Blocks:</p> </li> <li> <p>Start files, functions, classes, and significant sections of code with header blocks. These blocks include information such as the file's purpose, author, date, and a brief description.</p> </li> <li> <p>Function and Method Comments:</p> </li> <li> <p>Document function and method behavior, parameters, return values, and any special considerations. Use comments to explain what the function accomplishes, not just how it does it.</p> </li> <li> <p>Inline Comments:</p> </li> <li> <p>Use inline comments sparingly and only when necessary. Inline comments should clarify complex or non-obvious code, not describe every line. Make sure to keep them up to date when code changes.</p> </li> <li> <p>Code Documentation Comments:</p> </li> <li>Use documentation comments for public APIs, classes, functions, and methods. In Dart, you can use <code>///</code> for documentation comments, which can be processed by documentation generators.</li> </ol> <pre><code>/// This function calculates the total price of items in a cart.\n/// The [items] parameter is a list of [CartItem] objects.\n/// Returns the total price as a double.\ndouble calculateTotalPrice(List&lt;CartItem&gt; items) {\ndouble totalPrice = 0;\nfor (var item in items) {\ntotalPrice += item.product.price * item.quantity;\n}\nreturn totalPrice;\n}\n</code></pre> <ol> <li>Update Comments During Maintenance:</li> <li> <p>As you make changes to your code, update the comments to reflect the current state and behavior of the code. Outdated comments can be misleading.</p> </li> <li> <p>Use a Consistent Comment Style:</p> </li> <li> <p>Establish a consistent comment style throughout your codebase, including header blocks, function comments, and inline comments. Consistency enhances readability.</p> </li> <li> <p>Avoid Excessive Comments:</p> </li> <li> <p>While comments are essential, avoid over-commenting your code. Code should be self-explanatory through meaningful variable names and clear structure. Comments should complement, not substitute for, readable code.</p> </li> <li> <p>Consider Using Markdown or Markup: </p> </li> <li> <p>In documentation comments, consider using Markdown or markup languages to format your comments for better readability and to generate documentation.</p> </li> <li> <p>Code Reviews:</p> <ul> <li>Include code review practices in your development process. Code reviews are an opportunity to ensure that comments are present, meaningful, and correctly reflect the code's behavior.</li> </ul> </li> </ol> <p>Remember that while comments are beneficial, they should not be a substitute for writing clean, self-explanatory code. Comments should complement your code and provide additional insights that enhance code understanding and maintainability.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Use%20Descriptive%20Variable%20and%20Function%20Names/","title":"Explain Use Descriptive Variable and Function Names","text":"<p>\"Use Descriptive Variable and Function Names\" is a fundamental coding practice that involves choosing meaningful and informative names for variables, functions, methods, classes, and other identifiers in your code. The goal is to make your code more readable, self-explanatory, and maintainable by conveying the purpose and functionality of these elements through their names. This practice is crucial in Dart and Flutter development, as it enhances code understanding and collaboration. Here's why it's important and how to implement it effectively:</p> <p>Why Use Descriptive Names:</p> <ol> <li> <p>Code Readability: Clear and descriptive names make your code more readable and easier to understand for both yourself and other developers who may work on the project.</p> </li> <li> <p>Intent Clarification: Well-chosen names help convey the intent and purpose of variables, functions, and classes, reducing the need for extensive comments.</p> </li> <li> <p>Documentation Reduction: Descriptive names can replace or reduce the need for comments, as they provide self-documentation by describing what the code does.</p> </li> <li> <p>Maintenance: During maintenance or debugging, descriptive names can significantly reduce the time it takes to understand the code and locate potential issues.</p> </li> <li> <p>Collaboration: When working on a team, using descriptive names ensures that team members can understand and collaborate on code more effectively.</p> </li> </ol> <p>How to Use Descriptive Names Effectively:</p> <ol> <li>Choose Meaningful Names:</li> <li>Select names that accurately represent the purpose and role of the variable, function, class, or method.</li> <li> <p>Avoid generic or cryptic names like <code>x</code>, <code>temp</code>, or <code>foo</code>.</p> </li> <li> <p>Use Clear and Understandable Language:</p> </li> <li>Use words from the problem domain or domain-specific terminology to name your identifiers.</li> <li> <p>Avoid abbreviations or acronyms that may not be universally understood unless they are well-known and widely accepted in your domain.</p> </li> <li> <p>Be Explicit:</p> </li> <li> <p>Be explicit in your naming. A longer, descriptive name is often better than a shorter, ambiguous one. For example, use <code>calculateTotalPrice</code> instead of <code>calc</code> or <code>total</code>.</p> </li> <li> <p>Follow Naming Conventions:</p> </li> <li> <p>Adhere to naming conventions and conventions outlined in the Dart Style Guide. For example, use <code>lowerCamelCase</code> for variables and function names, and <code>UpperCamelCase</code> for class names.</p> </li> <li> <p>Use Nouns for Variables and Constants:</p> </li> <li> <p>Variable names should generally be nouns or noun phrases that describe what the variable represents. For example, <code>userProfile</code>, <code>productList</code>, or <code>invoiceTotal</code>.</p> </li> <li> <p>Use Verbs for Functions and Methods:</p> </li> <li> <p>Function and method names should generally be verbs or verb phrases that describe what action they perform. For example, <code>calculateTotalPrice()</code>, <code>fetchUserData()</code>, or <code>validateEmail()</code>.</p> </li> <li> <p>Avoid Overly Generic Names:</p> </li> <li> <p>Avoid using names like <code>data</code>, <code>result</code>, or <code>value</code> without additional context. Provide context that clarifies what the data represents.</p> </li> <li> <p>Avoid Magic Numbers and Strings:</p> </li> <li> <p>Replace magic numbers and strings in your code with well-named constants. For instance, replace <code>const int maxAttempts = 3</code> instead of <code>const int 3</code> when limiting attempts.</p> </li> <li> <p>Refactor as Needed:</p> </li> <li> <p>If you find that an existing name is no longer descriptive due to code changes, don't hesitate to refactor and choose a more appropriate name.</p> </li> <li> <p>Use Consistent Naming Styles:</p> <ul> <li>Maintain consistency in naming styles throughout your codebase. Consistency makes the code more predictable and easier to navigate.</li> </ul> </li> <li> <p>Self-Explanatory Code:</p> <ul> <li>Strive to make your code as self-explanatory as possible through the use of descriptive names. Minimize the need for comments by making the code's intent evident in the names themselves.</li> </ul> </li> </ol> <p>Here's an example of code with descriptive names:</p> <pre><code>// Descriptive variable names\nString userName = 'john_doe';\nint itemCount = 10;\nbool isLoggedIn = false;\n// Descriptive function names\ndouble calculateTotalPrice(double itemPrice, int quantity) {\nreturn itemPrice * quantity;\n}\nList&lt;User&gt; fetchActiveUsers() {\n// Implementation\n}\n</code></pre> <p>By consistently using descriptive names, you enhance the clarity and maintainability of your Dart and Flutter code. This practice promotes better collaboration among developers, reduces the risk of misunderstandings, and ultimately leads to more robust and maintainable codebases.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Explain%20Use%20Version%20Control/","title":"Explain Use Version Control","text":"<p>\"Use Version Control\" is a fundamental best practice in software development, including Flutter development. Version control systems (VCS) are tools that help you manage and track changes to your codebase over time. Flutter developers often use Git, a distributed version control system, to implement this practice. Here's an explanation of why version control is crucial and how to use it effectively in your Flutter projects:</p> <p>Why Use Version Control:</p> <ol> <li> <p>History Tracking: Version control systems keep a detailed history of changes made to your codebase, allowing you to see who made a change, when it was made, and what exactly was changed. This historical information is invaluable for understanding how your code has evolved.</p> </li> <li> <p>Collaboration: When working on a team or collaborating with others, version control facilitates concurrent development. Multiple developers can work on different aspects of the project simultaneously, and version control helps merge their changes together.</p> </li> <li> <p>Backup and Recovery: Version control serves as a reliable backup system. If something goes wrong with your code, you can revert to a previous, known-good state. This backup and recovery capability can prevent data loss and code disasters.</p> </li> <li> <p>Branching and Experimentation: Version control systems enable you to create branches, which are isolated copies of your code. You can use branches for feature development, bug fixes, and experimentation without affecting the main codebase. This promotes safe and controlled development.</p> </li> <li> <p>Code Reviews: Version control facilitates code reviews by providing a structured way for team members to collaborate on code changes. Code reviews are essential for maintaining code quality and sharing knowledge.</p> </li> <li> <p>Release Management: You can use version control to manage releases and track which versions of your software have been deployed. This is crucial for tracking and deploying updates to production.</p> </li> </ol> <p>How to Use Version Control in Flutter Projects:</p> <ol> <li> <p>Install Git: If you haven't already, install Git on your development machine. You can download Git from the official website (https://git-scm.com/).</p> </li> <li> <p>Initialize a Git Repository: To start using version control in a Flutter project, navigate to the project's root directory in your terminal and run <code>git init</code>. This command initializes a new Git repository.</p> </li> <li> <p>Add and Commit Changes:</p> </li> <li>Use <code>git add</code> to stage changes you want to commit. You can specify individual files or directories.</li> <li> <p>Use <code>git commit</code> to commit your staged changes along with a descriptive commit message. A good commit message should explain what the change does.</p> </li> <li> <p>Create Branches:</p> </li> <li>Use <code>git branch</code> to list existing branches.</li> <li>Use <code>git checkout -b &lt;branch-name&gt;</code> to create and switch to a new branch.</li> <li> <p>Use <code>git checkout &lt;branch-name&gt;</code> to switch between branches.</p> </li> <li> <p>Merge Branches:</p> </li> <li> <p>Use <code>git merge &lt;branch-name&gt;</code> to merge changes from one branch into another. Merging is common when you've completed a feature or bug fix in a separate branch.</p> </li> <li> <p>Remote Repositories:</p> </li> <li> <p>To collaborate with others, you'll typically use remote repositories hosted on platforms like GitHub, GitLab, or Bitbucket. Connect your local repository to a remote using <code>git remote add origin &lt;repository-url&gt;</code> and push your changes using <code>git push origin &lt;branch-name&gt;</code>.</p> </li> <li> <p>Pull Changes: Before making changes to your codebase, it's a good practice to pull the latest changes from the remote repository to ensure you're working with the most up-to-date code.</p> </li> <li> <p>Resolve Conflicts: When merging branches or pulling changes, conflicts may arise if multiple people modify the same code. Use <code>git mergetool</code> or manually edit the conflicting files to resolve conflicts.</p> </li> <li> <p>Commit History: Use <code>git log</code> to view the commit history of your project. This helps you understand how your codebase has evolved.</p> </li> <li> <p>Tagging Releases: You can use Git tags to mark specific commits as releases. This helps you keep track of important versions of your software.</p> </li> </ol> <p>By following these steps and best practices, you can effectively use version control to manage your Flutter projects. Git is a powerful tool that enhances collaboration, code quality, and project management in software development.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Keep%20UI%20and%20Business%20Logic%20Separated/","title":"Keep UI and Business Logic Separated","text":"<p>\"Keep UI and Business Logic Separated\" is a fundamental principle in software development that advocates for a clear separation between the user interface (UI) and the business logic of an application. This separation, often referred to as the Model-View-Controller (MVC), Model-View-Presenter (MVP), or Model-View-ViewModel (MVVM) architecture, helps improve the maintainability, testability, and scalability of your Flutter application.</p> <p>Here's a breakdown of this principle and its benefits:</p> <ol> <li> <p>Separation of Concerns (SoC): Separating UI and business logic adheres to the SoC principle, which states that different aspects of software should be handled independently and not intermingle. In the context of Flutter, this means keeping code responsible for how the UI looks and behaves separate from code responsible for the underlying data, calculations, and application logic.</p> </li> <li> <p>Maintainability: When UI and business logic are separated, changes to one part of the application are less likely to affect the other. This makes it easier to maintain and update your codebase. UI designers and developers can work on the UI components without worrying about the intricacies of the application's logic.</p> </li> <li> <p>Testability: Separating UI and business logic allows for more effective testing. You can write unit tests for the business logic independently of the UI. This promotes a test-driven development (TDD) approach, where you write tests before implementing the functionality, ensuring that your code is robust and less error-prone.</p> </li> <li> <p>Reusability: Isolating the business logic from the UI makes it possible to reuse the same logic in different parts of the application or even across multiple applications. For instance, you can create a business logic layer that can be shared across different Flutter projects.</p> </li> </ol> <p>In the context of Flutter, here's how you can keep UI and business logic separated:</p> <ol> <li> <p>Use State Management: Employ state management solutions like Provider, Riverpod, or Bloc to manage application state. These libraries help you separate the UI (widgets) from the underlying data and logic by providing a clear mechanism for passing data and events between them.</p> </li> <li> <p>ViewModels and Models: Utilize ViewModels to encapsulate the UI-related logic and Models to represent the data structures. This approach follows the MVVM pattern and ensures a clean separation between UI components and the logic that drives them.</p> </li> <li> <p>Business Logic Classes: Implement business logic in separate Dart classes or packages. These classes should not have direct dependencies on Flutter widgets or context. They should be purely Dart code responsible for performing calculations, making API calls, and managing application data.</p> </li> <li> <p>UI Widgets: Design your UI using Flutter widgets. These widgets should focus solely on rendering and responding to user interactions. Minimize the amount of application logic embedded within widgets, and instead, delegate tasks to the appropriate business logic classes or ViewModels.</p> </li> <li> <p>Dependency Injection: Use dependency injection techniques to provide business logic classes or ViewModels to your UI widgets. This ensures that the UI components are decoupled from the specific implementations of the business logic, making it easier to switch or test different implementations.</p> </li> <li> <p>Testing: Write tests for your business logic independently of the UI. Flutter's testing framework allows you to write unit tests and widget tests to ensure that both UI and logic work as expected.</p> </li> </ol> <p>By adhering to the principle of keeping UI and business logic separated, you can create Flutter applications that are easier to maintain, test, and scale. This separation promotes code organization and modularity, making your codebase more robust and maintainable as your application grows and evolves.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Stay%20Up-to-Date/","title":"Stay Up to Date","text":"<p>\"Stay Up-to-Date\" is a critical best practice in software development, including Flutter development. It involves actively keeping yourself informed about the latest updates, changes, and trends in the Flutter framework, associated packages, and the broader software development ecosystem. Staying up-to-date ensures that you can make informed decisions, adopt best practices, and take advantage of new features and improvements. Here's why staying up-to-date is important and how to do it effectively:</p> <p>Why Stay Up-to-Date:</p> <ol> <li> <p>New Features and Enhancements: Flutter is an evolving framework, and updates often introduce new features, improvements, and performance optimizations. Staying up-to-date allows you to take advantage of these enhancements in your projects.</p> </li> <li> <p>Bug Fixes and Security Patches: Updates may include bug fixes and security patches. Applying these updates promptly helps keep your applications secure and free of known issues.</p> </li> <li> <p>Compatibility: As Flutter evolves, package dependencies and compatibility may change. Staying up-to-date ensures that your app remains compatible with the latest versions of Flutter and its dependencies.</p> </li> <li> <p>Performance: Newer versions of Flutter may include performance optimizations that can make your apps run faster and smoother. Staying current can help you deliver a better user experience.</p> </li> <li> <p>Community and Best Practices: Staying engaged with the Flutter community and following best practices is crucial for learning from others' experiences, sharing knowledge, and improving your own coding practices.</p> </li> <li> <p>Emerging Trends: Technology trends can impact your application's design and functionality. Keeping up-to-date allows you to evaluate and adopt emerging trends that may be relevant to your projects.</p> </li> </ol> <p>How to Stay Up-to-Date:</p> <ol> <li>Official Documentation and Announcements:</li> <li> <p>Regularly check the official Flutter documentation (https://flutter.dev/docs) and announcements on the Flutter website to learn about the latest features, updates, and best practices.</p> </li> <li> <p>Release Notes:</p> </li> <li> <p>Read the release notes for Flutter and related packages. These notes provide detailed information about what has changed in each new release.</p> </li> <li> <p>Flutter Blog and Newsletters:</p> </li> <li> <p>Subscribe to the Flutter blog (https://flutter.dev/blog) and newsletters to receive updates, tutorials, and insights directly from the Flutter team.</p> </li> <li> <p>Social Media and Forums:</p> </li> <li> <p>Follow Flutter-related accounts and communities on social media platforms like Twitter and Reddit. Engaging in discussions and forums like the Flutter subreddit (r/FlutterDev) can keep you informed and provide opportunities to ask questions and share knowledge.</p> </li> <li> <p>Conferences and Meetups:</p> </li> <li> <p>Attend Flutter conferences, webinars, and local meetups when possible. These events often feature presentations on the latest developments in Flutter.</p> </li> <li> <p>Online Courses and Tutorials:</p> </li> <li> <p>Enroll in online courses and tutorials related to Flutter. Platforms like Udemy, Coursera, and Pluralsight offer courses that cover Flutter development.</p> </li> <li> <p>GitHub Repositories:</p> </li> <li> <p>Monitor GitHub repositories for Flutter and popular Flutter packages. You can see the latest commits, issues, and discussions related to these projects.</p> </li> <li> <p>Version Management:</p> </li> <li> <p>Use a version management tool like <code>pub</code> (the Dart package manager) or <code>pubspec.yaml</code> to specify the Flutter and package versions in your projects. Regularly update these dependencies to stay current.</p> </li> <li> <p>Experiment and Build Projects:</p> </li> <li> <p>Experiment with new Flutter features and try building small projects to gain hands-on experience with the latest updates.</p> </li> <li> <p>Community Involvement:</p> <ul> <li>Actively participate in the Flutter community by sharing your knowledge, answering questions, and contributing to open-source projects. This can help you stay engaged and build a network of fellow Flutter developers.</li> </ul> </li> </ol> <p>Remember that staying up-to-date is an ongoing process, and the Flutter ecosystem evolves continuously. Regularly allocating time for learning and keeping current with Flutter developments is essential for maintaining the quality and competitiveness of your Flutter applications.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Test%20Your%20Code/","title":"Test Your Code","text":"<p>\"Test Your Code\" is a crucial practice in software development, including Flutter development, that involves systematically creating and running tests to verify that your code behaves as expected. Testing helps ensure that your application works correctly, remains reliable during updates, and can be maintained more easily. In the context of Flutter, there are different types of tests you can write:</p> <ol> <li>Unit Tests:</li> <li>What: Unit tests focus on testing individual units of code in isolation, typically functions, methods, or small classes.</li> <li>Why: They help verify that each part of your codebase works as intended independently of the rest of the application. This practice aids in catching bugs early in development.</li> <li> <p>How: In Flutter, you can write unit tests using the <code>test</code> package or Flutter's built-in test framework. Mocking dependencies or using test doubles (like <code>Mockito</code>) is common to isolate units for testing.</p> </li> <li> <p>Widget Tests:</p> </li> <li>What: Widget tests are used to test individual widgets or small groups of widgets. These tests interact with widgets and check their behavior.</li> <li>Why: Widget tests help ensure that UI components render correctly and respond to user interactions as expected.</li> <li> <p>How: You can write widget tests using Flutter's <code>flutter_test</code> package. These tests can simulate user interactions and verify widget properties and state changes.</p> </li> <li> <p>Integration Tests:</p> </li> <li>What: Integration tests involve testing how different parts of your application work together. These tests often cover complete user flows and interactions between various components.</li> <li>Why: Integration tests help ensure that the different parts of your app integrate correctly and that end-to-end functionality works as expected.</li> <li> <p>How: Flutter provides the <code>flutter_driver</code> package for writing integration tests. These tests run on a real or emulated device, interact with your app, and can validate the entire user journey.</p> </li> <li> <p>Golden Tests (Snapshot Tests):</p> </li> <li>What: Golden tests are a type of widget test that captures the rendered output of a widget and compares it to a \"golden\" image or snapshot.</li> <li>Why: They help detect unintended UI changes, ensuring that the UI remains consistent across different Flutter versions or device configurations.</li> <li>How: Use the <code>flutter_test</code> package's <code>goldenFileComparator</code> to create and validate golden tests.</li> </ol> <p>Here's how to integrate testing into your Flutter development workflow:</p> <ol> <li> <p>Write Tests Alongside Code: Practice test-driven development (TDD) by writing tests before implementing the corresponding code. This ensures that your code is designed with testability in mind and helps catch issues early.</p> </li> <li> <p>Use Test Frameworks and Libraries: Flutter provides built-in support for testing, including the <code>test</code> package for unit tests and the <code>flutter_test</code> package for widget and golden tests. You can also use additional packages like <code>Mockito</code> for mocking and <code>integration_test</code> for integration tests.</p> </li> <li> <p>Continuous Integration (CI): Set up CI pipelines to automatically run your tests whenever changes are pushed to your code repository. Popular CI/CD platforms like GitHub Actions, Travis CI, and CircleCI work well with Flutter testing.</p> </li> <li> <p>Test Coverage: Track test coverage to ensure that your tests cover a significant portion of your codebase. Tools like <code>lcov</code> or packages like <code>coverage</code> can help you measure and visualize code coverage.</p> </li> <li> <p>Refactoring and Maintenance: Whenever you refactor or modify your code, run your tests to ensure that the changes don't introduce regressions or break existing functionality.</p> </li> <li> <p>Parameterized Tests: Write parameterized tests to test multiple scenarios with different input data, making your test suite more comprehensive.</p> </li> <li> <p>Documentation: Maintain documentation for your tests. Describe what each test is validating and why it's important. Well-documented tests make it easier for other developers to understand your code.</p> </li> <li> <p>Test Isolation: Ensure that tests are isolated from external dependencies like databases or web services. Use mocking or dependency injection to replace these dependencies with test-specific versions.</p> </li> </ol> <p>By regularly testing your Flutter code using the appropriate types of tests, you can increase the reliability and maintainability of your application, catch and fix bugs early, and have confidence that your app behaves correctly across different scenarios and updates.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Use%20Flutter%20DevTools/","title":"Use Flutter DevTools","text":"<p>Flutter DevTools is a suite of developer tools and utilities provided by the Flutter team to assist developers in building, debugging, profiling, and analyzing Flutter applications. DevTools offers a range of features and capabilities that help streamline the development process and improve the overall quality of your Flutter apps. Here's an explanation of some of the key features and how to use them:</p> <ol> <li>Widget Inspector:</li> <li>The Widget Inspector allows you to interactively explore the widget tree of your Flutter application.</li> <li>You can select widgets in your running app and inspect their properties and state.</li> <li> <p>Use it to debug layout issues, identify widget rebuilds, and understand the widget hierarchy.</p> </li> <li> <p>Timeline:</p> </li> <li>The Timeline provides a detailed view of your app's performance over time.</li> <li>You can capture and analyze frames, track UI thread and GPU work, and find performance bottlenecks.</li> <li> <p>This is particularly useful for optimizing app performance and identifying rendering issues.</p> </li> <li> <p>Debugger:</p> </li> <li>Flutter DevTools includes a built-in debugger that lets you set breakpoints, step through code, and inspect variables.</li> <li> <p>You can also view the call stack and navigate through your app's code while it's running.</p> </li> <li> <p>Logging:</p> </li> <li>DevTools provides a log viewer that displays print and debug output from your app.</li> <li> <p>You can filter logs based on severity and search for specific messages.</p> </li> <li> <p>Memory Profiler:</p> </li> <li>The Memory Profiler allows you to track memory usage and identify memory leaks in your Flutter app.</li> <li> <p>You can take memory snapshots and analyze object allocation over time.</p> </li> <li> <p>Network Profiler:</p> </li> <li>The Network Profiler helps you monitor network requests made by your app.</li> <li> <p>You can view details of HTTP requests and responses, including headers and payload data.</p> </li> <li> <p>Performance Overlay:</p> </li> <li>The Performance Overlay is a widget that can be added to your app to visualize performance metrics directly on the screen.</li> <li> <p>It displays information like frames per second (FPS), GPU rendering, and CPU usage.</p> </li> <li> <p>Dart DevTools Integration:</p> </li> <li>DevTools is closely integrated with Dart DevTools, which offers additional features for Dart development.</li> <li>Dart DevTools includes features for analyzing and debugging Dart code, including observatory, heap snapshots, and more.</li> </ol> <p>Here's how to use Flutter DevTools in your development workflow:</p> <ol> <li>Install Flutter DevTools:</li> <li> <p>You can install DevTools as a Flutter package by running the following command in your terminal:      <pre><code>flutter pub global activate devtools\n</code></pre></p> </li> <li> <p>Launch DevTools:</p> </li> <li>Run your Flutter app, and then open DevTools by running this command:      <pre><code>flutter pub global run devtools\n</code></pre></li> <li> <p>DevTools will open in your default web browser, providing access to all its features.</p> </li> <li> <p>Connect to Your App:</p> </li> <li>DevTools will automatically detect your running Flutter app.</li> <li> <p>If you're using a different device or need to specify a custom URL, you can manually connect DevTools to your app.</p> </li> <li> <p>Explore and Use DevTools:</p> </li> <li>Navigate through the various tabs and panels in DevTools to access the features you need.</li> <li> <p>Use the Widget Inspector to examine the widget tree, the Timeline to analyze performance, and other tools as required for your development and debugging tasks.</p> </li> <li> <p>Customization and Preferences:</p> </li> <li>DevTools offers customization options and preferences that allow you to tailor its behavior to your specific needs.</li> </ol> <p>Flutter DevTools is an invaluable resource for Flutter developers, whether you're building new apps, optimizing performance, or debugging issues. By leveraging its features, you can streamline your development process and create high-quality Flutter applications.</p>"},{"location":"Flutter%20Notes/Code%20Practice/Explaination/Use%20Proper%20Widget%20Composition/","title":"Use Proper Widget Composition","text":"<p>\"Use Proper Widget Composition\" is a best practice in Flutter development that emphasizes the importance of structuring your user interface (UI) by composing widgets in a logical and organized manner. Proper widget composition leads to more maintainable, readable, and efficient Flutter code.</p> <p>Here's a more detailed explanation of this practice:</p> <ol> <li> <p>Widget Hierarchy: In Flutter, you build UIs by composing widgets. Each widget represents a piece of the UI, from simple elements like text or buttons to complex layouts and views. Proper widget composition involves creating a hierarchy of widgets to represent the structure of your UI.</p> </li> <li> <p>Reusability: Widgets are designed to be reusable. Instead of duplicating UI code, you should break it into smaller, reusable widgets. For example, if you have a complex UI element that appears in multiple places in your app, create a custom widget for it.</p> </li> <li> <p>Separation of Concerns: Follow the principle of separation of concerns. Keep UI-related code separate from business logic and data processing. This separation makes your code easier to maintain and test.</p> </li> <li> <p>Widget Organization: Organize your widgets hierarchically based on their relationships and responsibilities. For example, use <code>Column</code> and <code>Row</code> widgets to lay out other widgets vertically and horizontally, respectively. Use containers like <code>ListView</code> or <code>GridView</code> when dealing with lists of items.</p> </li> <li> <p>Composition Over Inheritance: In Flutter, you compose widgets to build complex UIs rather than relying on inheritance. Instead of extending existing widgets, create custom widgets that use existing ones as building blocks. This approach is more flexible and encourages code reusability.</p> </li> <li> <p>Keep Widgets Focused: Each widget should have a single responsibility. If a widget becomes too complex or handles too many tasks, consider breaking it into smaller, more focused widgets. This simplifies debugging and maintenance.</p> </li> <li> <p>Pass Data Using Constructor Parameters: When you create custom widgets, pass data and configuration to them using constructor parameters. This makes it explicit how a widget should behave and reduces reliance on global state.</p> </li> </ol> <p>Here's an example to illustrate proper widget composition:</p> <p>Suppose you're building a weather app with a forecast screen that displays a list of daily weather forecasts. Instead of creating a single monolithic widget for the entire forecast screen, you can break it down into smaller widgets:</p> <ul> <li>Create a custom <code>WeatherForecast</code> widget that takes a list of forecast data as a parameter.</li> <li>Inside the <code>WeatherForecast</code> widget, use a <code>ListView</code> to display a list of <code>DailyForecast</code> widgets, each representing a single day's forecast.</li> <li>The <code>DailyForecast</code> widget takes a single forecast item as a parameter and displays the weather information for that day.</li> </ul> <p>By composing these widgets, you create a clear and organized hierarchy, improve code reusability, and make it easier to manage and update the forecast screen in your app.</p> <p>In summary, using proper widget composition in Flutter involves breaking down your UI into smaller, reusable widgets, organizing them hierarchically, and keeping them focused on specific responsibilities. This approach leads to cleaner, more maintainable code and a more efficient development process.</p>"},{"location":"Notes/Apps/","title":"Apps","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Apps/#apps","title":"Apps","text":"<ul> <li>As a DevOps engineer, you should be aware of the basics of programming and the software development lifecycle.</li> </ul>"},{"location":"Notes/Apps/#compilers","title":"Compilers","text":"<ul> <li>Older compilers used to compile directly into machine code which was specific to a processor set. So, it could not be run on other machines.</li> <li>Modern compilers convert to an intermediate code called as Byte Code which is then run by the interpreter (in case of interpreted languages)</li> </ul>"},{"location":"Notes/Apps/#languages","title":"Languages","text":""},{"location":"Notes/Apps/#java","title":"Java","text":"<ul> <li>JDK (Java Development Kit) - tool to develop, build and run Java applications<ul> <li>![[attachments/Pasted image 20220922161712.png]]</li> <li>javac - used for compiling</li> <li>javadoc - used for documentation</li> <li>jdb - used for debugging</li> </ul> </li> <li>Java code compilation and execution process<ul> <li>![[attachments/Pasted image 20220922162328.png]]</li> <li>Source code is compiled into byte-code</li> </ul> </li> <li>Java Archive (JAR) is a packaging format to package multiple java <code>.class</code> files and dependent libraries into a single distributable package in <code>.jar</code> format.<ul> <li><code>jar cf MyApp.jar MyClass.class Service1.class Service2.class ...</code> - to create a jar file</li> <li><code>java -jar MyApp.jar</code> - run a jar file</li> </ul> </li> <li>Web Archive (WAR) is a packaging format to package all the contents of <code>.jar</code> along with images and static contents required to build a web application. </li> <li>JavaDoc is a tool to automatically generate documentation of the code <ul> <li><code>javadoc -d doc MyClass.java</code> - generate documentation for the given java file</li> </ul> </li> <li>Java Application Build Process<ul> <li>![[attachments/Pasted image 20220922163630.png]]</li> </ul> </li> <li>Application build process can be automated using Build Tools like Maven, Gradle &amp; ANT. They use configuration files where we can specify build steps<ul> <li>![[attachments/Pasted image 20220922164033.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Apps/#javascript-nodejs","title":"JavaScript / NodeJS","text":"<ul> <li>JavaScript was originally developed as a client side scripting language</li> <li>NodeJS brought JS to the server side</li> <li>NodeJS can handle a large number of concurrent connections by implementing a non-blocking IO model</li> <li>Node Package Manager (NPM) is a package manager and repository of publicly available NodeJS packages<ul> <li><code>npm -v</code> - get the NPM version</li> <li><code>npm install serverless</code> - install a package</li> <li><code>npm install -g serverless</code> - install a package globally</li> <li><code>npm uninstall serverless</code> - uninstall a package</li> <li><code>npm search serverless</code> - search a package</li> </ul> </li> <li>When importing a package in the code, node looks at the local <code>node_modules</code> folder under the application folder. If not found, it looks at the global <code>node_modules</code> directory. To see the list of paths node scans, run <code>node -e \"console.log(module.paths)\"</code></li> <li>Application dependencies are stored as configuration in <code>package.json</code> file at the root of the application directory.</li> </ul>"},{"location":"Notes/Apps/#python","title":"Python","text":"<ul> <li>Server-side scripting language heavily used in Analytics</li> <li>Both Python2 and Python3 can be installed on the system simulatneously</li> <li>Pip Install Package (PIP) - package manager for Python<ul> <li><code>pip2 install numpy</code> - python2</li> <li><code>pip3 install numpy</code> - python3</li> <li><code>pip -V</code> - check which python pip is pointing to</li> <li><code>pip show numpy</code> - show where a package is installed</li> <li><code>pip install numpy --upgrade</code> - version upgrade a package</li> <li><code>pip uninstall numpy</code> - uninstall a package</li> </ul> </li> <li>Application dependencies should be written in <code>requirements.txt</code> and installed by running    <code>pip install -r requirements.txt</code></li> </ul>"},{"location":"Notes/Architecture/","title":"Architecture","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Architecture/#architecture","title":"Architecture","text":""},{"location":"Notes/Architecture/#worker-nodes","title":"Worker Nodes","text":"<ul> <li>These nodes do the actual work (need more resources)</li> <li>Each worker node has multiple pods running on it. </li> <li>3 processes must be installed on every worker node<ul> <li>Container Runtime (eg. docker)</li> <li>Kubelet <ul> <li>process of Kubernetes</li> <li>starts pods and runs containers inside them</li> <li>allocates resources from the node to the container</li> </ul> </li> <li>Kubeproxy<ul> <li>process of Kubernetes</li> <li>forwards the requests to pods intelligently</li> <li>Image<ul> <li>Kubeproxy forwards requests to the DB pod running on the same node to minimize network overhead.</li> <li>![[attachments/Pasted image 20220828123915.png]]</li> <li></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Architecture/#master-nodes","title":"Master Nodes","text":"<ul> <li>Control the cluster state &amp; manage worker nodes</li> <li>Need less resources as they don't do the actual work</li> <li>Usually multi-master for fault tolerance</li> <li>4 processes run on every master node<ul> <li>API Server<ul> <li>User interacts with the cluster via the API server using a client (Kubernetes Dashboard, CLI, or Kubernetes API)</li> <li>Cluster gateway (acts as the entry point into the cluster)</li> <li>Can be used for authentication</li> <li>Image<ul> <li>![[attachments/Pasted image 20220828124843.png]]</li> </ul> </li> </ul> </li> <li>Scheduler<ul> <li>Decides the node where the new pod should be scheduled and sends a request to the Kubelet to start a pod.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220828125230.png]]</li> </ul> </li> </ul> </li> <li>Controller Manager<ul> <li>Detects state changes like crashing of pods</li> <li>If a pod dies, it requests scheduler to schedule starting up of a new pod</li> <li>Image<ul> <li>![[attachments/Pasted image 20220828125455.png]]</li> </ul> </li> </ul> </li> <li>etcd<ul> <li>Key-value store of the cluster state (also known as cluster brain)</li> <li>Cluster changes get stored in the etcd</li> <li>In multi-master configuration, etcd is a distributed key-value store</li> <li>Application data is not stored in the etcd</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Architecture/#cluster","title":"Cluster","text":"<ul> <li>In a K8 cluster, master or worker nodes can be added dynamically</li> </ul>"},{"location":"Notes/Architecture/#layers-of-abstraction","title":"Layers of Abstraction","text":"<ul> <li>Deployment (manages replicasets)</li> <li>Replicaset (how many replicas of pods to create)</li> <li>Pod</li> <li>Container</li> </ul> <p>[!important] Everything below the Deployment layer is managed by Kubernetes</p>"},{"location":"Notes/Components/","title":"Components","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Components/#components","title":"Components","text":""},{"location":"Notes/Components/#node","title":"Node","text":"<ul> <li>A physical or virtual machine running pods</li> <li>Worker servers in K8 cluster</li> </ul>"},{"location":"Notes/Components/#pod","title":"Pod","text":"<ul> <li>Smallest unit of [[Kubernetes]]</li> <li>Abstraction over a container </li> <li>Creates a running environment over the container so that we only interact with the Kubernetes layer. This allows us to replace the container technology like Docker.</li> <li>Usually, only 1 application is run on a pod</li> <li>Each pod gets a unique internal IP address for communicating with each other (virtual network created by K8)</li> <li>If a pod is restarted (maybe after the application running on it crashed), its IP address changes</li> <li>Image<ul> <li>![[attachments/Pasted image 20220828102630.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Components/#service","title":"Service","text":"<ul> <li>Static IP address that can be attached to pods</li> <li>Pods communicate with each other using services</li> <li>Lifecycle of pod and service are not connected. So even if a pod dies, we can restart it and attach the original service to have the same IP.</li> <li>Service could be:<ul> <li>ClusterIP<ul> <li>Enables access to the service from within the K8s cluster (internal)</li> <li>Every service created in Kubernetes has a cluster IP</li> </ul> </li> <li>NodePort<ul> <li>Every node in the cluster has a public IP</li> <li>Assigns a port to the service (external requests going to any node at that port will be routed to the service)</li> <li>Limitations <ul> <li>If a node goes down it's public IP might change when restarted</li> <li>One port per service</li> </ul> </li> <li>Image<ul> <li>![[attachments/Pasted image 20220912100744.png]]</li> </ul> </li> </ul> </li> <li>LoadBalancer<ul> <li>Depends on how the cloud provider is providing their Kubernetes service</li> <li>Spins up a [[AWS Solutions Architect Associate/Elastic Load Balancer (ELB)#Network Load Balancer NLB| Network Load Balancer]] for each service. Requests going to the NLB's IP will be routed to the service.</li> <li>Limitations:<ul> <li>One NLB IP per service</li> <li>If the number of services increase, need to add a lot of NLBs (expensive)</li> </ul> </li> </ul> </li> </ul> </li> <li>Multiple pods could be connected to a service. In this case, the service acts as a load balancer.</li> </ul>"},{"location":"Notes/Components/#ingress","title":"Ingress","text":"<ul> <li>DNS for K8</li> <li>Used to route traffic into the K8 cluster</li> <li>Allows us to use domain names instead of IPs</li> <li>The request first goes to ingress and is then forwarded to service</li> <li>Image<ul> <li>![[attachments/Pasted image 20220904222518.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Components/#configmap","title":"ConfigMap","text":"<ul> <li>External configuration to the application</li> <li>Used to store parameters like DB_URL</li> <li>Not for storing secrets (DB_USER, DB_PASS)</li> <li>Can be connected to the pod to get parameters (configs)</li> </ul>"},{"location":"Notes/Components/#secret","title":"Secret","text":"<ul> <li>Used to store secrets (DB_USER, DB_PASS)</li> <li>Base 64 encoded</li> <li>Built-in security mechanism is not enabled by default</li> <li>Can be connected to the pod to get secrets</li> <li>Image<ul> <li>![[attachments/Pasted image 20220828110943.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Components/#volume","title":"Volume","text":"<ul> <li>If the database container or pod gets restarted, the stored data would be lost</li> <li>A volume (persistent storage) could be attached to the database pod</li> <li>Volume could be:<ul> <li>local (on the same node as the DB pod)</li> <li>remote (outside the K8 cluster, could be a cloud storage)</li> </ul> </li> <li>Image<ul> <li>![[attachments/Pasted image 20220828111746.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Components/#deployment","title":"Deployment","text":"<ul> <li>Blueprint for stateless pods (application layer)</li> <li>Deployment specifies how many replicas of a pod will be running</li> <li>Databases can't be replicated using deployments (because they are stateful)</li> </ul>"},{"location":"Notes/Components/#statefulset","title":"StatefulSet","text":"<ul> <li>Blueprint for stateful pods (database layer)</li> <li>Database pods should be created using StatefulSets (not Deployments)</li> <li>Would take care of replicating the DB pods or scaling them while ensuring database consistency</li> <li>Working with StatefulSets is tedious. So, it is a common practice to host databases outside the K8 cluster and use K8 for hosting stateless applications.</li> </ul>"},{"location":"Notes/Data%20Storage/","title":"Data Storage","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Data%20Storage/#data-storage","title":"Data Storage","text":"<ul> <li>Any local or remote storage must be thought of as a hard drive plugged into the K8 cluster. K8 does not manage any data persistence. So, the system admin is responsible for backing up the data storage and managing it.</li> <li>Working with stateful pods is difficult. So, it is a common practice to host databases outside the K8 cluster and use K8 only for stateless applications.</li> </ul>"},{"location":"Notes/Database/","title":"Database","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Database/#database","title":"Database","text":"<ul> <li>DevOps engineers deploy and manage databases</li> <li>SQL databases represent data in tabular format whereas noSQL databases represent data in documents.</li> </ul>"},{"location":"Notes/Database/#mysql","title":"MySQL","text":"<ul> <li>Open-source SQL database</li> <li>Used by FaceBook and YouTube</li> <li><code>yum install mysql-server</code> - install MySQL on CentOS</li> <li><code>systemctl start mysqld</code> - start the MySQL service</li> <li>Logs are available at <code>/var/log/mysqld.log</code><ul> <li>When <code>mysqld</code> service is started, the logs contain a temporary password required to log into the MySQL database.</li> </ul> </li> <li><code>mysql -u root -p</code> - login to the DB as root user (you will be prompted to enter password)</li> <li>Commands<ul> <li><code>ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPassword'</code> - change the password for root user</li> <li><code>SHOW DATABASES</code></li> <li><code>CREATE DATABASE Students</code></li> <li><code>USE Students</code> - Switch to Students database (can only work on 1 database at a time)</li> <li>Once a DB is selected, you can create tables, insert values in them and so on...</li> </ul> </li> </ul>"},{"location":"Notes/Database/#user-management","title":"User Management","text":"<ul> <li>Root users have admin access to the DB</li> <li>Create additional users - <code>CREATE USER 'john'@'localhost' IDENTIFIED BY 'password'</code></li> <li><code>'john'@'localhost'</code> means that the user <code>john</code> can connect from the local host, he will be denied access if he tries to connect from another host. </li> <li><code>'john'@'192.168.1.10'</code> means that the user  <code>john</code> can connect from the host with IP <code>192.168.1.10</code></li> <li><code>'john'@'%'</code> means that the user <code>john</code> can connect from any host</li> </ul> <p>[!tip]+ Assigning privileges to the users ![[attachments/Pasted image 20220923222223.png]]</p>"},{"location":"Notes/Database/#mongodb","title":"MongoDB","text":"<ul> <li>Open-source NoSQL database</li> <li>Data is organized into documents </li> <li>Multiple documents form a collection</li> <li>Multiple collections form a database</li> <li>We can have multiple databases within a MongoDB server</li> <li>MongoDB is available as a cloud offering or as on-premises server</li> <li><code>yum install mongodb-org</code></li> <li><code>systemctl start mongod</code> - start the MongoDB service</li> <li>Logs are stored at <code>/var/log/mongodb/mongod.1og</code></li> <li>Configuration file is available at <code>/etc/mongod.conf</code></li> <li>Commands<ul> <li><code>mongo</code> - activate mongo shell</li> <li><code>show dbs</code> - show databases</li> <li><code>use school</code> - switch to school DB (if not present, create new and switch)</li> </ul> </li> </ul>"},{"location":"Notes/DevOps/","title":"DevOps","text":"<p>[[Notes]]</p>"},{"location":"Notes/DevOps/#devops","title":"DevOps","text":"<p>[[Kubernetes]]</p> <p>[[Pre-Requisites]]</p>"},{"location":"Notes/Helm%20Package%20Manager/","title":"Helm Package Manager","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Helm%20Package%20Manager/#helm-package-manager","title":"Helm Package Manager","text":"<ul> <li>It serves 2 purposes<ul> <li>Package manager for K8s<ul> <li>Used to package YAML files and distribute them in public &amp; private repositories</li> </ul> </li> <li>Templating Engine<ul> <li>Used to define a template YAML with placeholders for dynamic values</li> <li>Useful in CI/CD</li> <li>![[attachments/Pasted image 20220904233635.png]]</li> </ul> </li> </ul> </li> <li>Uses<ul> <li>Deploy same application across multiple clusters<ul> <li>Package the application as a Helm chart and deploy it across multiple clusters</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Helm%20Package%20Manager/#helm-charts","title":"Helm Charts","text":"<ul> <li>Bundle of YAML files</li> <li>Example: If you need to implement elastic search in your application, instead of creating the YAML files for all the components, you can use an existing helm chart from Helm Repository.</li> <li>You can also create and push your own helm charts for others to use.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220904232830.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Helm%20Package%20Manager/#helm-chart-directory-structure","title":"Helm Chart Directory Structure","text":"<ul> <li><code>values.yaml</code> contains default values for the placeholders ![[attachments/Pasted image 20220904234111.png]]</li> </ul>"},{"location":"Notes/Helm%20Package%20Manager/#helm-commands","title":"Helm Commands","text":"<p>Install a packaged app from the Helm Chart <pre><code>helm install --values=my-values.yaml &lt;chartname&gt;\n</code></pre></p> <p>![[attachments/Pasted image 20220904234614.png]]</p>"},{"location":"Notes/Ingress/","title":"Ingress","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Ingress/#ingress","title":"Ingress","text":""},{"location":"Notes/Ingress/#intro","title":"Intro","text":"<ul> <li>Provides rule based routing</li> <li>Requests can be received at a single IP address (NLB's IP) and forwarded to the right service based on the path or subdomain.<ul> <li>![[attachments/Pasted image 20220912101826.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Ingress/#sample-yaml","title":"Sample YAML","text":"<p>![[attachments/Pasted image 20220904223108.png]] - Here, we define routing rules for the hostname - Backend is the target where the incoming requests for that hostname will be redirected</p>"},{"location":"Notes/Ingress/#ingress-internal-service-setup","title":"Ingress &amp; Internal Service Setup","text":"<p>![[attachments/Pasted image 20220904223440.png]] - ServiceName - name of the internal service - ServicePort - port on which the internal service is available</p>"},{"location":"Notes/Ingress/#ingress-controller","title":"Ingress Controller","text":"<ul> <li>A pod or a set of pods that act as the entry point to the cluster and evaluate ingress rules &amp; manage redirections</li> <li>Ingress component cannot do anything by itself, it needs an ingress controller which is an implementation for ingress component.</li> <li>Example:<ul> <li>K8s Nginx Ingress Controller</li> <li>3rd party</li> </ul> </li> <li>Image<ul> <li>![[attachments/Pasted image 20220904224433.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Ingress/#setup-configuration","title":"Setup &amp; Configuration","text":"<ul> <li>On Cloud<ul> <li>Cloud provider's load balancer acts as the entry point and forwards request to ingress controller (managed solution, minimal effort)</li> <li>Image<ul> <li>![[attachments/Pasted image 20220904224738.png]]</li> </ul> </li> </ul> </li> <li>On Premise<ul> <li>Need to configure entry point ourselves</li> <li>External proxy server (hardware or software) acting as the entry point (has external IP address)</li> <li>None of the nodes in the cluster have external IP addresses (for security)</li> <li>Image<ul> <li>![[attachments/Pasted image 20220904225310.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Ingress/#default-backend-in-ingress","title":"Default Backend in Ingress","text":"<ul> <li>If a request does not match any rule, the ingress controller redirects it to the    <code>default-http-backend</code> service. If it doesn't exist, 404 error is returned. </li> <li>This is useful to display a custom \"Page doesn't exist\" error message.</li> <li>Default backend can be created by creating a pod with the same name as the default http backend and on the same port.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220904230006.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Ingress/#routing-to-multiple-paths","title":"Routing to multiple paths","text":"<p>![[attachments/Pasted image 20220904230551.png]]</p>"},{"location":"Notes/Ingress/#routing-to-multiple-subdomains","title":"Routing to multiple subdomains","text":"<p>![[attachments/Pasted image 20220904230715.png]]</p>"},{"location":"Notes/Ingress/#configuring-tls-certificate","title":"Configuring TLS Certificate","text":"<ul> <li>TLS Certificate has to be created as a secret in the same namespace as the ingress</li> <li>Ingress uses the TLS certificate as a reference from the secret ![[attachments/Pasted image 20220904230853.png]]</li> </ul>"},{"location":"Notes/Ingress/#resources","title":"Resources","text":"<p>(30) Kubernetes Ingress Explained Completely For Beginners - Updated - YouTube</p>"},{"location":"Notes/Intro/","title":"Intro","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Intro/#intro","title":"Intro","text":"<ul> <li>[[Kubernetes]] is an open-source container orchestration tool developed by Google</li> <li>It helps us manage containerized applications in different deployment environments</li> </ul>"},{"location":"Notes/Intro/#container-orchestration","title":"Container Orchestration","text":"<ul> <li>Containers act as perfect hosts for micro-services (running independently)</li> <li>The rise of micro-services architecture led to applications using 1000s of containers that need to be smartly managed.</li> <li>Container orchestration offers:<ul> <li>High Availability (no downtime)</li> <li>Scalability</li> <li>Disaster Recovery </li> </ul> </li> <li>Example<ul> <li>In the image below, a replica of both Application [[Components#Pod|Pod]] and the DB Pod are maintained on a separate node with load balancing. So, even if one of the nodes fail, the application will be accessible.</li> <li>![[attachments/Pasted image 20220828121643.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Kubectl/","title":"Kubectl","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Kubectl/#kubectl","title":"Kubectl","text":"<ul> <li>Command line tool for K8s cluster</li> <li>It communicates via the API server running on the master node or [[Local Cluster Setup#Minikube|Minikube]]</li> <li>This is the most powerful way to control the K8s cluster (compared to Kubernetes Dashboard and API) </li> </ul>"},{"location":"Notes/Kubectl/#commands","title":"Commands","text":"<ul> <li>List all components - <code>kubectl get all</code></li> <li>List nodes - <code>kubectl get nodes</code></li> <li>List pods - <code>kubectl get pods</code></li> <li>List services - <code>kubectl get services</code></li> <li>List deployments - <code>kubectl get deployments</code></li> <li>Describe a pod - <code>kubectl describe pod &lt;pod-name&gt;</code></li> <li>Create deployment - <code>kubectl create deployment &lt;deployment-name&gt; --image=nginx</code><ul> <li>This will download the nginx image from dockerhub and create a deployment with the provided name and the nginx image</li> <li>Here deployment name and the image are the parameters we are passing, the rest will be taken as default to generate the K8s config file.</li> </ul> </li> <li>Get container logs of a pod - <code>kubectl logs &lt;pod-name&gt;</code><ul> <li>The container must be running inside the pod for this command to work.</li> </ul> </li> <li>Open interactive terminal for a pod - <code>kubectl exec -it &lt;pod-name&gt; -- bin/bash</code></li> <li>Delete deployment - <code>kubectl delete deployment &lt;deployment-name&gt;</code><ul> <li>Replicasets, Pods and Containers under the deployment will be deleted automatically.</li> </ul> </li> <li>Create or update a deployment using a [[Kubernetes Config File|K8s config file]] - <code>kubectl apply -f config.yaml</code></li> <li>Delete a deployment using a [[Kubernetes Config File|K8s config file]] - <code>kubectl delete -f config.yaml</code></li> <li>Get IP address of the pods - <code>kubectl get pods -o wide</code></li> <li>Get the result of deployment - <code>kubectl get deployment nginx-deployment -o yaml</code><ul> <li>Can also save it to a file - <code>kubectl get deployment nginx-deployment -o yaml &gt; nginx-deployment-result.yaml</code></li> </ul> </li> <li>Create namespace - <code>kubectl create namespace &lt;namespace-name&gt;</code></li> <li>Get namespaces - <code>kubectl get namespaces</code></li> <li>List non-namespaced resources - <code>kubectl api-resources --namespaced=false</code></li> <li></li> </ul>"},{"location":"Notes/Kubernetes%20Config%20File/","title":"Kubernetes Config File","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Kubernetes%20Config%20File/#kubernetes-config-file","title":"Kubernetes Config File","text":"<ul> <li>YAML config file to create and maintain components in a Kubernetes cluster</li> <li>It has 3 parts<ul> <li>Metadata</li> <li>Specification</li> <li>Status<ul> <li>Stores the actual state of the component</li> <li>Automatically generated and added by K8s from the etcd which stores the status info of every component in the K8s cluster</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes%20Config%20File/#sample-config-file","title":"Sample Config File","text":"<p>nginx-deployment.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:            # label the deployment\napp: nginx\nspec:                # specifications for the deployment\nreplicas: 2\nselector:          # connect all pods with the given label\nmatchLabels:\napp: nginx\ntemplate:          # configuration for the pods (config inside config)\nmetadata:\nlabels:\napp: nginx   # label all the pods\nspec:            # specifications for the pods\ncontainers:\n- name: nginx\nimage: nginx:1.16\nports:\n- containerPort: 8080  # port at which the container is running\n</code></pre></p> <p>nginx-service.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: nginx-service\nspec:\nselector:          # connect the service with the labelled deployment\napp: nginx\nports:\n- protocol: TCP\nport: 80          # port at which the service could be contacted\ntargetPort: 8080  # port at which pods can be contacted\n</code></pre></p>"},{"location":"Notes/Kubernetes%20Config%20File/#connecting-components","title":"Connecting Components","text":"<ul> <li>Metadata contain labels which give key-value pairs to components</li> <li>Specifications contain selectors which select and connect components based on the labels</li> <li>Ports connect Services with Pods. <ul> <li>![[attachments/Pasted image 20220830222705.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Kubernetes%20Config%20File/#checking-connection","title":"Checking connection","text":"<p>Describe the service The endpoints give the IP address and port of the pods that the service will forward the requests to. <pre><code>\u276f kubectl describe service nginx-service\nName:              nginx-service\nNamespace:         default\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.39.212\nIPs:               10.105.39.212\nPort:              &lt;unset&gt;  80/TCP\nTargetPort:        8080/TCP\nEndpoints:         172.17.0.3:8080,172.17.0.4:8080\nSession Affinity:  None\nEvents:            &lt;none&gt;\n</code></pre></p>"},{"location":"Notes/Kubernetes%20Config%20File/#get-the-result-of-deployment","title":"Get the result of deployment","text":"<p><pre><code>kubectl get deployment nginx-deployment -o yaml\n</code></pre> <pre><code>kubectl get deployment nginx-deployment -o yaml &gt; nginx-deployment-result.yaml\n</code></pre> Check the status of the config file. <pre><code>status:\n  availableReplicas: 2\n  conditions:\n  - lastTransitionTime: \"2022-08-31T03:12:52Z\"\n    lastUpdateTime: \"2022-08-31T03:12:52Z\"\n    message: Deployment has minimum availability.\n    reason: MinimumReplicasAvailable\n    status: \"True\"\n    type: Available\n  - lastTransitionTime: \"2022-08-31T03:12:51Z\"\n    lastUpdateTime: \"2022-08-31T03:12:52Z\"\n    message: ReplicaSet \"nginx-deployment-6986574f5f\" has successfully progressed.\n    reason: NewReplicaSetAvailable\n    status: \"True\"\n    type: Progressing\n  observedGeneration: 1\n  readyReplicas: 2\n  replicas: 2\n  updatedReplicas: 2\n</code></pre></p>"},{"location":"Notes/Kubernetes/","title":"Kubernetes","text":"<p>[[Notes]] [[DevOps]]</p> <p>[[Intro]]</p> <p>[[Components]]</p> <p>[[Architecture]]</p> <p>[[Local Cluster Setup]]</p> <p>[[Data Storage]]</p> <p>[[Kubectl]]</p> <p>[[Kubernetes Config File]]</p> <p>[[MongoDB &amp; Mongo Express Project]]</p> <p>[[Namespaces]]</p> <p>[[Ingress]]</p> <p>[[Helm Package Manager]]</p>"},{"location":"Notes/Linux/","title":"Linux","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Linux/#linux","title":"Linux","text":"<ul> <li>All the cloud and DevOps tools are made to run on Linux </li> <li>Ubuntu is based on the Debian architecture</li> <li>CentOS is based on the Red Hat Enterprise Linux (RHEL) architecture</li> </ul>"},{"location":"Notes/Linux/#user-accounts","title":"User Accounts","text":"<ul> <li>Root user has unlimited access to the system</li> <li>Root user needs to grant a normal user root previlege by making an entry in the <code>/etc/sudoers</code> file. The normal user can then escalate their previlege by prefixing their command with <code>sudo</code> </li> </ul>"},{"location":"Notes/Linux/#important-commands","title":"Important Commands","text":"<ul> <li><code>echo $SHELL</code> - get shell type</li> <li><code>mkdir -p user/abdur/add</code> - make directory hierarchy</li> <li><code>cat &gt; file.txt</code> - add contents to a file</li> <li><code>whoami</code> - get user info</li> <li><code>id</code> - get user's id and group</li> <li><code>su &lt;username&gt;</code> - switch user</li> <li><code>sudo su -</code> - switch to root user</li> <li><code>ssh &lt;username&gt;@192.168.1.1</code> - ssh as a user into a system</li> <li><code>wget &lt;file-url&gt; -O file.txt</code> - download a file</li> </ul>"},{"location":"Notes/Linux/#package-managers","title":"Package Managers","text":"<ul> <li>Package managers download the necessary dependencies when installing packages in Linux</li> <li>CentOS uses Yum, an RPM (RedHat Package Manager) based package manager<ul> <li>![[attachments/Pasted image 20220915104247.png]]</li> </ul> </li> <li>Commands<ul> <li><code>yum install ansible</code> - install a package</li> <li><code>yum repolist</code> - list the repositories</li> <li><code>yum list</code> - list installed packages</li> <li><code>yum list &lt;package-name&gt;</code> - search for a package in the list of installed packages</li> <li><code>yum remove &lt;package-name&gt;</code> - remove a package</li> <li><code>yum --show-duplicates list &lt;package-name&gt;</code> - list all the installed versions of a package</li> <li><code>yum install ansible-2.4.2.0</code> - install a specific version of a package</li> </ul> </li> </ul>"},{"location":"Notes/Linux/#services","title":"Services","text":"<ul> <li>Services in Linux help us configure software to run in the background and ensure that they run automatically even if the servers are rebooted. Eg. Docker</li> <li>If there are multiple interdependent services, they should follow the order of startup.</li> <li><code>systemctl</code> command is used to manage <code>systemd</code> services</li> <li>Commands - might need to use sudo<ul> <li><code>systemctl start httpd</code> - start an service (HTTPD)</li> <li><code>systemctl stop httpd</code> - stop a running service</li> <li><code>systemctl status httpd</code> - check the status of a service</li> <li><code>systemctl enable httpd</code> - configure the service to start at startup</li> <li><code>systemctl disable httpd</code> - configure the service to not start at startup</li> <li><code>systemctl daemon-reload</code> - reload the systemd</li> </ul> </li> </ul>"},{"location":"Notes/Linux/#configure-an-application-as-a-service","title":"Configure an application as a Service","text":"<ul> <li>Let's assume a Python server app that runs at port 5000 and prints \"Hello World\"</li> <li>We want to configure this app as a service</li> <li>Create a new file <code>my_app.service</code> in the <code>/etc/systemd/system</code> directory and paste the following in it <pre><code>[Service]\nExecStart=&lt;command to start your app&gt;\n</code></pre></li> <li><code>systemctl daemon-reload</code>  - reload the systemd to let it know that a new service has been configured </li> <li><code>systemctl start my_app</code> - start your app as a service</li> <li>To configure the service to run when the system boots up, add the following to the service file <pre><code>[Install]\nWantedBy=multi-user.target\n</code></pre></li> <li>To add description <pre><code>[Unit]\nDescription=My best Python app\n</code></pre></li> <li>To restart the application if it crashes <pre><code>[Service]\nRestart=always\n</code></pre></li> <li>To run scripts before and after running the app <pre><code>[Service]\nExecStartPre=/opt/code/configure.sh\nExecStartPost=/opt/code/result.sh\n</code></pre></li> </ul>"},{"location":"Notes/Linux/#docker-as-a-service","title":"Docker as a Service","text":"<ul> <li>Once Docker is installed, the Docker daemon runs as a background process that listens for docker commands</li> <li>Docker installation creates a service file with the following contents<ul> <li>![[attachments/Pasted image 20220921225448.png]]</li> </ul> </li> </ul> <p>a</p>"},{"location":"Notes/Local%20Cluster%20Setup/","title":"Local Cluster Setup","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Local%20Cluster%20Setup/#local-cluster-setup","title":"Local Cluster Setup","text":""},{"location":"Notes/Local%20Cluster%20Setup/#minikube","title":"Minikube","text":"<ul> <li>To setup a local K8s cluster, use Minikube which is a single node cluster with both master and worker processes installed and a docker runtime. </li> <li>Minikube creates a virtual box on your local machine. The node runs inside that virtual box.</li> <li>This can be used for testing.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220829175912.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Local%20Cluster%20Setup/#installation-mac","title":"Installation (Mac)","text":"<p>This will install [[Kubectl]] as well <pre><code>brew install minikube\n</code></pre></p>"},{"location":"Notes/Local%20Cluster%20Setup/#spin-up-a-minikube","title":"Spin up a minikube","text":"<pre><code>minikube start\n</code></pre>"},{"location":"Notes/Local%20Cluster%20Setup/#check-status","title":"Check status","text":"<pre><code>minikube status\n</code></pre>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/","title":"MongoDB & Mongo Express Project","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#mongodb-mongo-express-project","title":"MongoDB &amp; Mongo Express Project","text":""},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#components-required","title":"Components Required","text":"<ul> <li>A MongoDB pod</li> <li>A Mongo Express pod</li> <li>An external service to allow users to interact with Mongo Express</li> <li>An internal service to interface with MongoDB</li> <li>ConfigMap for DB URL</li> <li>Secret for DB Username and Password ![[attachments/Pasted image 20220830233738.png]]</li> </ul>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#request-flow","title":"Request Flow","text":"<p>![[attachments/Pasted image 20220830233841.png]]</p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#creating-components","title":"Creating Components","text":""},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#secrets","title":"Secrets","text":"<ul> <li>DB username and secrets should be stored in secrets.</li> <li>Values in secrets must be base64 encoded - <code>echo -n \"&lt;string-value&gt;\" | base64</code></li> </ul> <p>mongo-secret.yaml <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: mongodb-secret\ntype: Opaque\ndata:\nmongo-root-username: dXNlcm5hbWU=\nmongo-root-password: cGFzc3dvcmQ=\n</code></pre></p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#mongodb-pod","title":"MongoDB Pod","text":"<ul> <li>DB container is accessed on port 27017</li> <li>DB username and password should not be stored in the code. It should be referenced from the secrets. </li> <li>Since the DB username and password are being referenced from the secret, it must be created before the MongoDB Pod.</li> </ul> <p>mongo.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mongodb-deployment\nlabels:\napp: mongodb\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mongodb\ntemplate:\nmetadata:\nlabels:\napp: mongodb\nspec:\ncontainers:\n- name: mongodb\nimage: mongo\nports:\n- containerPort: 27017\nenv:\n- name: MONGO_INITDB_ROOT_USERNAME\nvalueFrom:\nsecretKeyRef:\nname: mongodb-secret\nkey: mongo-root-username\n- name: MONGO_INITDB_ROOT_PASSWORD\nvalueFrom: secretKeyRef:\nname: mongodb-secret\nkey: mongo-root-password\n</code></pre></p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#internal-service","title":"Internal Service","text":"<ul> <li>Required by other pods to communicate with the MongoDB pod</li> <li>We can write multiple config in the same YAML file by separating them with <code>---</code></li> <li>Target port is the port on which DB pod can be connected (27017)</li> <li>Service port is the port on which the service is listening (27017)</li> </ul> <p>mongo.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mongodb-deployment\nlabels:\napp: mongodb\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mongodb\ntemplate:\nmetadata:\nlabels:\napp: mongodb\nspec:\ncontainers:\n- name: mongodb\nimage: mongo\nports:\n- containerPort: 27017\nenv:\n- name: MONGO_INITDB_ROOT_USERNAME\nvalueFrom:\nsecretKeyRef:\nname: mongodb-secret\nkey: mongo-root-username\n- name: MONGO_INITDB_ROOT_PASSWORD\nvalueFrom: secretKeyRef:\nname: mongodb-secret\nkey: mongo-root-password\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: mongodb-service\nspec:\nselector:\napp: mongodb\nports:\n- protocol: TCP\nport: 27017\ntargetPort: 27017\n</code></pre></p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#configmap","title":"ConfigMap","text":"<ul> <li>To store MongoDB URL</li> </ul> <p>mongo-configmap.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: mongodb-configmap\ndata:\ndatabase_url: mongodb-service\n</code></pre></p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#mongo-express-pod","title":"Mongo Express Pod","text":"<ul> <li>Since the secret and configMap are being referenced, they must be deployed beforehand.</li> </ul> <p>mongo-express.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mongo-express\nlabels:\napp: mongo-express\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mongo-express\ntemplate:\nmetadata:\nlabels:\napp: mongo-express\nspec:\ncontainers:\n- name: mongo-express\nimage: mongo-express\nports:\n- containerPort: 8081\nenv:\n- name: ME_CONFIG_MONGODB_ADMINUSERNAME\nvalueFrom:\nsecretKeyRef:\nname: mongodb-secret\nkey: mongo-root-username\n- name: ME_CONFIG_MONGODB_ADMINPASSWORD\nvalueFrom: secretKeyRef:\nname: mongodb-secret\nkey: mongo-root-password\n- name: ME_CONFIG_MONGODB_SERVER\nvalueFrom: configMapKeyRef:\nname: mongodb-configmap\nkey: database_url\n</code></pre></p>"},{"location":"Notes/MongoDB%20%26%20Mongo%20Express%20Project/#external-service","title":"External Service","text":"<ul> <li>To allow users to connect with Mongo Express</li> <li>Append the service config to <code>mongo-express.yaml</code></li> <li><code>type: LoadBalancer</code> tells the K8s that the service is external and so it is assigned an external IP address<ul> <li>By default <code>type: ClusterIP</code>, which means the IP is internal (internal service)</li> </ul> </li> <li><code>nodePort</code> is the port on which the external IP will be open (port on which the application can be accessed externally)<ul> <li>Must be in the range <code>30000</code> - <code>32767</code></li> </ul> </li> </ul> <p>mongo-express.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: mongo-express\nlabels:\napp: mongo-express\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: mongo-express\ntemplate:\nmetadata:\nlabels:\napp: mongo-express\nspec:\ncontainers:\n- name: mongo-express\nimage: mongo-express\nports:\n- containerPort: 8081\nenv:\n- name: ME_CONFIG_MONGODB_ADMINUSERNAME\nvalueFrom:\nsecretKeyRef:\nname: mongodb-secret\nkey: mongo-root-username\n- name: ME_CONFIG_MONGODB_ADMINPASSWORD\nvalueFrom: secretKeyRef:\nname: mongodb-secret\nkey: mongo-root-password\n- name: ME_CONFIG_MONGODB_SERVER\nvalueFrom: configMapKeyRef:\nname: mongodb-configmap\nkey: database_url\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: mongo-express-service\nspec:\nselector:\napp: mongo-express\ntype: LoadBalancer  ports:\n- protocol: TCP\nport: 8081\ntargetPort: 8081\nnodePort: 30000\n</code></pre></p>"},{"location":"Notes/Namespaces/","title":"Namespaces","text":"<p>[[Kubernetes]]</p>"},{"location":"Notes/Namespaces/#namespaces","title":"Namespaces","text":"<ul> <li>Used to organize resources</li> <li>Virtual cluster inside of a Kubernetes cluster</li> <li>Most resources cannot be shared across namespaces<ul> <li>ConfigMap</li> <li>Secrets</li> <li>Pods</li> </ul> </li> <li>[[Components#Service|Services]] can be shared across namespaces<ul> <li>![[attachments/Pasted image 20220903230027.png]]</li> </ul> </li> <li>Components that don't belong to a namespace. They live globally inside the cluster.<ul> <li>Volumes</li> <li>Nodes </li> </ul> </li> </ul>"},{"location":"Notes/Namespaces/#default-namespaces","title":"Default Namespaces","text":"<ul> <li>By default, a K8s cluster has 4 namespaces<ul> <li>kube-system<ul> <li>Contains system, master &amp; kubectl processes</li> <li>Do not create or modify anything in kube-system namespace</li> </ul> </li> <li>kube-public<ul> <li>Contains a configmap, which stores publicly accessible data</li> </ul> </li> <li>kube-node-lease<ul> <li>Contains information about the heartbeats of nodes</li> <li>Each node has an associated lease object which stores information about the availability of that node</li> </ul> </li> <li>default<ul> <li>Default namespace where you will be creating resources</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Namespaces/#custom-namespaces","title":"Custom Namespaces","text":"<ul> <li>You can create a custom namespace using<ul> <li>Kubectl - <code>kubectl create namespace &lt;namespace-name&gt;</code></li> <li>KubeConfig file - specify namespace for each component (recommended)<ul> <li>![[attachments/Pasted image 20220904003437.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Namespaces/#use-cases","title":"Use Cases","text":"<ul> <li>Grouping resources<ul> <li>![[attachments/Pasted image 20220903190603.png]]</li> </ul> </li> <li>Multiple teams<ul> <li>Example: two or more teams working on the same app where each team works in their own namespace without disrupting the other</li> <li>![[attachments/Pasted image 20220903190751.png]]</li> </ul> </li> <li>Resource sharing<ul> <li>Example: reuse Elastic Stack in both development and staging environments or between blue/green deployment versions</li> <li>![[attachments/Pasted image 20220903191054.png]]</li> <li>![[attachments/Pasted image 20220903191308.png]]</li> </ul> </li> <li>Access limiting <ul> <li>Example: limiting access to dev namespace for support team</li> <li>![[attachments/Pasted image 20220903225546.png]]</li> </ul> </li> <li>Resource allocation <ul> <li>Allocate resources (CPU, RAM, etc.) to different namespaces</li> </ul> </li> </ul>"},{"location":"Notes/Networking/","title":"Networking","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Networking/#networking","title":"Networking","text":""},{"location":"Notes/Networking/#switch","title":"Switch","text":"<p>![[attachments/Pasted image 20220921231550.png]] - Switch is a computer hardware connecting computers on a network - Switch can facilitate communication only in the same network (subnet) - Switch doesn't have DHCP so it doesn't auto-assign local IPs to connected devices - We can assign an IP address to the network interface on each computer. Then the computers can communicate with each other.</p>"},{"location":"Notes/Networking/#router","title":"Router","text":"<p>![[attachments/Pasted image 20220922000630.png]] - Router is a computer hardware that connects multiple networks (subnets) together so that devices on the two networks can communicate with each other.  - Router is assigned different IPs in the different networks - To view the route table of the system run the <code>route</code> command - Router acts as a gateway between two networks.  - Router is not required to connect devices on the same network. The entry for the same subnet in the route table has gateway as 0.0.0.0 (not required)     - ![[attachments/Pasted image 20220922003020.png]]     - - Commands (changes made are only valid till restart)     - <code>ip link</code> - list and modify interfaces on the linux host     - <code>ip addr</code> - view IP addresses assigned to the interfaces on the host     - <code>ip addr add 192.168.2.0/24 dev eth0</code> - set IP addresses to an interface     - <code>ip route add 192.168.2.0/24 via 192.168.1.1</code> - add routes to the routing table</p> <p>[!question]- How will device B communicate with device C? We need to add a route in the routing table of device B to direct all the traffic going to subnet 192.168.2.0/24 to go via 192.168.1.1 which is the router.  <code>ip route add 192.168.2.0/24 via 192.168.1.1</code></p> <p>Similarly, if device C wants to communicate with device A, it will have to update its routing table to route the request through the router. So, routing table needs to be configured on all the devices to ensure proper communication.</p> <p>Instead, we can add a default route on all the devices to route all requests going outside the current network to pass through the router (the router IP address will depend on the network the device is connect to) <code>ip route add default via 192.168.1.1</code> or <code>ip route add 0.0.0.0 via 192.168.1.1</code></p>"},{"location":"Notes/Networking/#connecting-to-the-internet","title":"Connecting to the Internet","text":"<p>![[attachments/Pasted image 20220922003539.png]] - Connect router to the internet and create a default gateway on all the devices to route external requests through the router.</p>"},{"location":"Notes/Networking/#using-multiple-routers","title":"Using Multiple Routers","text":"<p>![[attachments/Pasted image 20220922003811.png]] - One for internal network, one for internet - Need to create two routes      - ![[attachments/Pasted image 20220922003916.png]]</p>"},{"location":"Notes/Networking/#linux-host-as-router","title":"Linux host as Router","text":"<p>![[attachments/Pasted image 20220922004320.png]] - A and C are on different networks and B is on both.  - If we ping C from A, we get \"Network unreachable\" error - Add a route in A to route to C's network via B to allow the request from A to pass through B. - Add a route in C to route to A's network via B to allow the response from C to pass through B. - Now, if we ping C from A, we don't get \"Network unreachable\" error which means that the route tables are configured correctly.  - But, we don't get any response back because by default in Linux, packets are not forwarded from one interface to another. This means IP Forwarding is disabled by default. (eg. packets received on eth0 are not forwarded to eth1 on B). This is for security reasons. - We can enable IP Forwarding between interfaces on a Linux system by setting the value in the file <code>/proc/sys/net/ipv4/ip_forward</code> to 1     - <code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward</code>     - To persist this change between reboots, modify <code>/etc/sysctl.conf</code> and set <code>net.ipv4.ip_forward = 1</code> - Now, if we ping C from A, we get response back</p>"},{"location":"Notes/Networking/#dns","title":"DNS","text":""},{"location":"Notes/Networking/#dns-for-private-network","title":"DNS for Private Network","text":"<p>![[attachments/Pasted image 20220922115149.png]] - Two devices A and B on the same network, they can ping each other using IP addresses - Add IP of B in A's hosts table     - ![[attachments/Pasted image 20220922115941.png]] - Now A can ping B using the hostname <code>ping db</code> - Translating hostname to IP is known as Name Resolution</p> <p>We can even fool A into believing that B is <code>google.com</code> by naming it www.google.com in A's <code>/etc/hosts</code></p> <ul> <li>In a small network, name resolution can be easily managed by using <code>/etc/hosts</code> file. Every system will store the names of every other system in the network. <ul> <li>![[attachments/Pasted image 20220922120721.png]]</li> </ul> </li> <li>If the network grows, managing name resolution becomes difficult. If the IP of one of the system changes, the <code>/etc/hosts</code> file of all the other systems need to be updated. So, instead of each system managing its own hosts, we can shift the entries of the <code>/etc/hosts</code> file into a dedicated server to manage centrally. Then, we can point all the systems to this server to query the IP from the hostname. This is our DNS server.</li> <li>To point any device to the DNS server, edit the <code>/etc/resolv.conf</code> file to add the IP address of the DNS server. Repeat this for all the devices on the network.<ul> <li>![[attachments/Pasted image 20220922122941.png]]</li> </ul> </li> <li>If the IP address of any device on the network changes, simply update its entry on the DNS server. No need to update the <code>/etc/hosts</code> file on the other devices.</li> <li>In this setup, if you need to setup a test server, you can add it's hostname in your <code>/etc/hosts</code> file so that others are not able to resolve it's IP.</li> </ul> <p>The system first tries to resolve the hostname from the <code>/etc/hosts</code> file available locally. If it could not be resolved, then it queries the remote DNS.</p>"},{"location":"Notes/Networking/#dns-for-internet-connectivity","title":"DNS for Internet Connectivity","text":"<ul> <li>Edit the <code>/etc/resolv.conf</code> file on any system to add Google's name server <code>8.8.8.8</code> which will allow the system to ping unknown hostnames such as <code>www.facebook.com</code>. <ul> <li>![[attachments/Pasted image 20220922143525.png]]</li> </ul> </li> <li>If we want internet connectivity on every system in the local network, we need to set this up on every system on the network. Instead, we can configure the DNS server to forward all unknown requests to <code>8.8.8.8</code>. Now, all the devices on the network can ping external sites.</li> </ul> <p>To resolve <code>maps</code> as <code>maps.google.com</code>, add <code>search google.com</code> to <code>/etc/resolv.conf</code> file</p>"},{"location":"Notes/Networking/#dns-commands","title":"DNS Commands","text":"<ul> <li><code>nslookup www.google.com</code><ul> <li>Command line tool to query a hostname from DNS server</li> <li>Does not look into <code>/etc/hosts</code>, only queries the DNS</li> </ul> </li> <li><code>dig www.google.com</code></li> </ul>"},{"location":"Notes/Notes/","title":"Notes","text":"<p>[[../index|Home]]</p>"},{"location":"Notes/Notes/#notes","title":"Notes","text":"<p>[[AWS Solutions Architect Associate/AWS Solutions Architect Associate (SAA-C02)|AWS Solutions Architect Associate (SAA-C02)]]</p> <p>[[Static Site Generators/Static Site Generators|Static Site Generators]]</p> <p>[[Programming/Programming|Programming]]</p> <p>[[DevOps]]</p>"},{"location":"Notes/Pre-Requisites/","title":"Pre Requisites","text":"<p>[[DevOps]]</p>"},{"location":"Notes/Pre-Requisites/#pre-requisites","title":"Pre-Requisites","text":"<p>[[Linux]] [[Networking]] [[Apps]] [[Web Server]] [[Database]] [[Security]]</p>"},{"location":"Notes/Security/","title":"Security","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Security/#security","title":"Security","text":""},{"location":"Notes/Security/#asymmetric-keys","title":"Asymmetric Keys","text":"<ul> <li>Data can be encrypted by either one of them and will be decrypted by the other</li> <li>If you encrypt your data with your private key, anyone with your public key can decrypt your data.</li> <li>Private keys have the word <code>key</code> in their name<ul> <li>![[attachments/Pasted image 20220926132925.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Security/#ssl-tls-certificates","title":"SSL &amp; TLS Certificates","text":"<ul> <li>A certificate is used to guarantee trust between two parties during a transaction</li> <li>TLS (Transport Layer Security) certificate ensure that the communication between the user and the server is encrypted and the server is who it says it is.</li> <li>TLS or HTTPS relies on symmetric encryption which is computationally light. But, if the key is sniffed while being sent from the server to the user, it could be used to decrypt the communication between the user and the server. So, we use asymmetric encryption to encrypt &amp; transfer the symmetric key from the user to the server and then use symmetric encryption for future communications.</li> </ul>"},{"location":"Notes/Security/#ssh","title":"SSH","text":"<ul> <li>The user wants to securely connect to the server</li> <li>The user generates a key pair (public and private key)<ul> <li><code>ssh-keygen</code> - this will generate a private key (<code>id_rsa</code>) and a public key (<code>id_rsa.pub</code>)</li> </ul> </li> <li>Add the public key to the list of authorized keys for SSH (<code>~/.ssh/authorized_keys</code>)</li> <li>When doing an SSH into the server, the user specifies the private key in the SSH command<ul> <li><code>ssh -i id_rsa user@server</code></li> </ul> </li> </ul> <ul> <li>We can use the same key pair to SSH into multiple servers by adding the public key to their list of authorized keys for SSH.</li> <li>We can authorize other users to SSH into our server by adding their public keys into <code>~/.ssh/authorized_keys</code></li> </ul>"},{"location":"Notes/Security/#how-tls-works","title":"How TLS works","text":""},{"location":"Notes/Security/#encrypting-communication","title":"Encrypting Communication","text":"<ul> <li>The server generates a private and public key pair<ul> <li>![[attachments/Pasted image 20220926122851.png]]</li> </ul> </li> <li>The server sends the public key to the user (could be sniffed)</li> <li>The user's browser generates a symmetric key and encrypts it using the public key which was sent by the server.</li> <li>The encrypted symmetric key is then sent to the server (could be sniffed)</li> <li>The server decrypts the symmetric key using the private key</li> <li>Now, both client and server have the same symmetric key and can secure all future communication between them.</li> </ul>"},{"location":"Notes/Security/#verifying-server-identity","title":"Verifying Server Identity","text":"<ul> <li>When the server sends the public key, it sends the key inside a digital certificate. The certificate contains information about the identity of the web server. </li> <li>Self-signed certificates are not trusted as anyone can generate a certificate and sign it themselves. So, the identities of servers sending self-signed certificates are not verified.</li> <li>Modern web browsers validate the certificate sent by the server during the SSL handshake. If the certificate is self signed, they will warn you.</li> <li>Certificates signed by Certificate Authorities (CA) are trusted. Some common CAs are Symantec, GlobalSign, Comodo, DigiCert etc.</li> <li>To get your certificate signed by a CA, you need to generate a Certificate Signing Request (CSR) using the public key and the domain name of your website. Once they verify your server's information, they sign your certificate and send it to you.<ul> <li>![[attachments/Pasted image 20220926130250.png]]</li> <li>The CSR file needs to be sent to the CA for signing</li> </ul> </li> </ul> <p>This whole process of generating and maintaining certificates is called Public Key Infrastructure (PKI)</p> <p>[!tip]+ How do browsers verify CA's identity? - Every CA has a key pair. They use the private key to sign the certificate. - The public keys of all the CAs are saved in the browser. The browser verifies the certificate's sign using the public key of the associated  certificate authority.</p> <p>[!important]+ Verifying sites hosted locally We can deploy a CA locally for our private network (eg. organization's network). Most of the CA companies offer a private version of their service. Once a CA server is set up in your internal network, you can then install the public key of the CA on all of the employees' browsers to establish secure connectivity within your organization.</p>"},{"location":"Notes/Web%20Server/","title":"Web Server","text":"<p>[[Pre-Requisites]]</p>"},{"location":"Notes/Web%20Server/#web-server","title":"Web Server","text":"<ul> <li>Web Servers serve the static client-side content (HTML, CSS, JS)</li> <li>Application Servers serve dynamic content like the backend (responsible for the business logic)</li> <li>Web Frameworks like Express, Spring and Flask make it easy to develop application to run on a web server. This application code can then be hosted on the web server using Apache Tomcat or Nginx web server. These tools run processes that listen for requests from clients on a particular port.<ul> <li>![[attachments/Pasted image 20220923145355.png]]</li> </ul> </li> <li>Web Servers can also host multiple applications and redirect the request based on the URL or hostname or path to the right application.<ul> <li>![[attachments/Pasted image 20220923145825.png]]</li> </ul> </li> </ul> <p>[!tip]- Static vs Dynamic Websites - Static websites host static content (HTML, CSS and images that don't change). After the site is served, there is no interaction with the web server.   - Apache HTTP and Nginx are examples of static web server - Dynamic websites host content that changes (eg. products on Amazon). They have a backend to process payments, shipping information etc.   - Apache Tomcat and uWSGI are examples of dynamic web servers or application servers</p>"},{"location":"Notes/Web%20Server/#apache-web-server","title":"Apache Web Server","text":"<ul> <li>Open-source HTTP web server, developed and maintained by the Apache Foundation</li> <li>Install Apache in CentOS - <code>yum install httpd</code>)<ul> <li>If you have firewall on your system, add a rule to allow HTTP traffic   <code>firewall-cmd --permanent --add-service=http</code></li> </ul> </li> <li>View logs<ul> <li><code>cat /var/log/httpd/access_log</code> - access logs</li> <li><code>cat /var/log/httpd/error_log</code> - error logs</li> </ul> </li> <li>Config file - <code>/etc/httpd/conf/httpd.conf</code><ul> <li><code>Listen</code> - Port or IP:Port to listen on</li> <li><code>DocumentRoot</code> - Directory where to place the static files</li> <li><code>ServerName</code> - Domain name of the website (this will require a DNS to work)</li> </ul> </li> </ul> <p>[!tip]- Host multiple webpages on the same web server    We need to create two virtual hosts, each hosting a website with a ServerName and    DocumentRoot. Based on the domain name in the request, the Apache server will automatically    forward the request to the right application.    ![[attachments/Pasted image 20220923151916.png]]</p> <p>If the number of webpages grow, we can separate the config files for each website and include them in the main config file.     ![[attachments/Pasted image 20220923152612.png]]</p>"},{"location":"Notes/Web%20Server/#apache-tomcat","title":"Apache Tomcat","text":"<ul> <li>Provides a web server environment where we can host Java based web applications</li> <li>Installing Apache Tomcat on CentOS<ul> <li>![[attachments/Pasted image 20220923170352.png]]</li> </ul> </li> <li>Inside the <code>apache-tomcat-&lt;version&gt;</code> directory, there are multiple directories<ul> <li><code>bin</code> - contains scripts to control the web server</li> <li><code>conf</code> - contains configuration files for the web server</li> <li><code>logs</code> - contains log files</li> <li><code>webapps</code> - store web application files hosted by tomcat server<ul> <li>Package the web application into a <code>.war</code> file and and move it to <code>webapps</code> folder to host. If tomcat server is running, it will automatically detect the new app and host it.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Web%20Server/#deployments","title":"Deployments","text":""},{"location":"Notes/Web%20Server/#flask-app","title":"Flask App","text":"<ul> <li>Install dependencies - <code>pip install -r requirements.txt</code></li> <li>Run the app<ul> <li><code>python main.py</code> - good for running in development server (not in production)</li> <li>For production, we need to run it in a production grade web server for Flask like Gunicorn, uWSGI, Gevent or Twisted Web.</li> <li><code>gunicorn &lt;filename&gt;:&lt;flask-app-component&gt;</code> eg. <code>gunicorn main:app</code></li> <li><code>gunicorn main:app -w 3</code> - spawn 3 workers to host the flask app</li> </ul> </li> </ul>"},{"location":"Notes/Web%20Server/#nodejs-app","title":"NodeJS App","text":"<ul> <li>Install dependencies - <code>npm install</code></li> <li>Run the app<ul> <li><code>node app.js</code> - if scripts <code>package.json</code> are emtpy</li> <li><code>npm run start</code> - if start script is present</li> <li>For production, we need to run the app in a production grade web server for NodeJS like SupervisorD, Forever and Process Manager 2 (PM2).<ul> <li>PM2 is a production grade process manager for NodeJS with built in load balancer</li> <li><code>pm2 start app.js</code></li> <li><code>pm2 start app.js -i 3</code> - run 3 processes (workers)</li> <li><code>pm2 delete app.js</code> - terminate all the running processes of the app</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/Web%20Server/#ip-addresses-and-ports","title":"IP Addresses and Ports","text":"<ul> <li>Computer systems like laptops or servers have multiple wired and wireless network interfaces for connectivity. </li> <li>Once a system connects to network via an interface, the interface is assigned an IP address.</li> <li>Every network interface is divided into multiple logical components called ports (max 65535 ports per IP address). Each port is a communication endpoint. </li> <li>Programs running on the system can listen on one of the ports for requests.</li> <li>A system can connect to the same network through two different interfaces and hence take up two IP addresses (one for each interface).</li> <li>When running an app, we can specify the IP on which the port should be opened for the app to listen on.<ul> <li>![[attachments/Pasted image 20220923184027.png]]</li> </ul> </li> <li>We can open the port on all the available IP addresses by specifying <code>host = 0.0.0.0</code><ul> <li>![[attachments/Pasted image 20220923184423.png]]</li> </ul> </li> <li>If we don't want to open a port on an IP address on the network (eg, to test an app in our laptop's browser), we can omit the <code>host</code> field. In this case, the system opens the specified port on <code>127.0.0.1</code> (loopback address)<ul> <li>![[attachments/Pasted image 20220923184838.png]]</li> </ul> </li> </ul> <p>Every host as a built in virtual interface called Loopback Address which always equals <code>127.0.0.1</code>. Opening a port at this IP means opening it locally on the system (not the network). The loopback address it also referred to as <code>localhost</code></p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/","title":"API Gateway","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#api-gateway","title":"API Gateway","text":"<ul> <li>Serverless REST APIs</li> <li>Invoke Lambda functions using REST APIs (API gateway will proxy the request to lambda)</li> <li>Supports WebSocket (stateful)</li> <li>Rate Limiting (throttle requests) - returns 429 Too Many Requests</li> <li>Cache API responses</li> <li>Transform and validate requests and responses</li> <li>Can be integrated with any HTTP endpoint in the backend or any AWS API</li> </ul> <ul> <li>We can use an API Gateway REST API to directly access a DynamoDB table by creating a proxy for the DynamoDB query API.</li> <li>API cache is not enabled for a method, it is enabled for a stage</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#endpoint-types","title":"Endpoint Types","text":"<ul> <li>Edge-Optimized (default)<ul> <li>For global clients</li> <li>Requests are routed through the [[CloudFront]] edge locations (improves latency)</li> <li>The API Gateway lives in only one region but it is accessible efficiently through edge locations</li> </ul> </li> <li>Regional<ul> <li>For clients within the same region</li> <li>Could manually combine with your own CloudFront distribution for global deployment (this way you will have more control over the caching strategies and the distribution)</li> </ul> </li> <li>Private<ul> <li>Can only be accessed within your VPC using an Interface VPC endpoint (ENI)</li> <li>Use resource policy to define access</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#access-management","title":"Access Management","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#iam-policy","title":"IAM Policy","text":"<ul> <li>Create an IAM policy and attach to User or Role to allow it to call an API</li> <li>Good to provide access within your own AWS account</li> <li>Leverages Sig v4 where lAM credential are in the request headers</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220510203344.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#lambda-authorizer","title":"Lambda Authorizer","text":"<ul> <li>Uses a Lambda function to validate the token being passed in the header and return an lAM policy to determine if the user should be allowed to access the resource.</li> <li>Option to cache result of authentication</li> <li>For OAuth / SAML / 3rd party type of authentication</li> <li>Good to provide access outside your AWS account if you have an existing IDP</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220510203327.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#cognito-user-pools-cup","title":"Cognito User Pools (CUP)","text":"<ul> <li>Seamless integration with CUP (no custom lambda implementation required)</li> <li>Only supports authentication (authorization must be implemented in the backend)</li> <li>The client (user) first authenticates with Cognito and gets the access token which it passes in the header to API gateway. API gateway validates the token using Cognito and then hits the backend if the token is valid.</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220510203756.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/API%20Gateway/#serverless-crud-application","title":"Serverless CRUD Application","text":"<p>![[attachments/Pasted image 20220510201558.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Backup/","title":"AWS Backup","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Backup/#aws-backup","title":"AWS Backup","text":"<ul> <li>Centrally manage and automate backups of AWS services across regions and accounts</li> <li>On-Demand and Scheduled backups</li> <li>Supported services:<ul> <li>EC2 / EBS</li> <li>S3</li> <li>RDS / Aurora / DynamoDB</li> <li>DocumentDB / Neptune</li> <li>EFS / FSx (Lustre &amp; Windows)</li> <li>Storage Gateway (Volume Gateway)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Backup/#backup-vault","title":"Backup Vault","text":"<ul> <li>WORM (Write Once Read Many) model for backups</li> <li>Even the root user cannot delete backups</li> <li>Additional layer of defense to protect your backups against:<ul> <li>Inadvertent or malicious delete operations</li> <li>Updates that shorten or alter retention periods</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Directory%20Services/","title":"AWS Directory Services","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Directory%20Services/#aws-directory-services","title":"AWS Directory Services","text":"<ul> <li>Used to extend the AD network by involving services like EC2 to be a part of the AD to share login credentials.</li> </ul> <p>[!tip]- Exam tip  Use AWS Managed Microsoft AD unless the problem specifically asks for properties of AD Connector or Simple AD</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Directory%20Services/#aws-managed-microsoft-ad","title":"AWS Managed Microsoft AD","text":"<ul> <li>Login credentials are shared between on-premise and AWS managed AD</li> <li>Manage users on both AD (on-premise and on AWS managed AD)</li> <li>Supports MFA</li> <li>Establish trust connections with your on premise AD</li> <li>Supports directory-aware workloads on AWS</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Directory%20Services/#ad-connector","title":"AD Connector","text":"<ul> <li>AD connector will proxy all the requests to the on-premise AD</li> <li>Users are managed on the on-premise AD only</li> <li>Does not support directory-aware workloads on AWS</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Directory%20Services/#simple-ad","title":"Simple AD","text":"<ul> <li>AD-compatible managed directory on AWS (cannot be joined with on-premise AD)</li> <li>Users are managed on the AWS AD only</li> <li>Use when you don\u2019t have an on-premise AD</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Organizations/","title":"AWS Organizations","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Organizations/#aws-organizations","title":"AWS Organizations","text":"<ul> <li>Global service</li> <li>Manage multiple AWS accounts under an organization<ul> <li>1 master account</li> <li>member accounts</li> </ul> </li> <li>An AWS account can only be part of one organization</li> <li>Consolidated Billing across all accounts (lower cost)</li> <li>Pricing benefits from aggregated usage of AWS resources</li> <li>API to automate AWS account creation (on demand account creation)</li> <li>Establish Cross Account Roles for Admin purposes where the master account can assume an admin role in any of the children accounts</li> </ul> <p>Organization API can only create member accounts. They cannot configure anything within those accounts (use [[CloudFormation]] for that).</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Organizations/#organizational-units-ou","title":"Organizational Units (OU)","text":"<ul> <li>Folders for grouping AWS accounts of an organization</li> <li>Can be nested<ul> <li>![[attachments/Pasted image 20220511200502.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Organizations/#service-control-policies-scp","title":"Service Control Policies (SCP)","text":"<ul> <li>Whitelist or blacklist IAM actions at the OU or Account level</li> <li>Does not apply to the Master Account</li> <li>Applies to all the Users and Roles of the member accounts, including the root user. So, if something is restricted for that account, even the root user of that account won\u2019t be able to do it.</li> <li>Must have an explicit allow (does not allow anything by default)</li> <li>Does not apply to service-linked roles</li> <li>Explicit Deny has the highest precedence</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Organizations/#migrating-accounts-between-organizations","title":"Migrating Accounts between Organizations","text":"<ul> <li>To migrate member accounts from one organization to another<ol> <li>Remove the member account from the old organization</li> <li>Send an invite to the member account from the new organization</li> <li>Accept the invite from the member account</li> </ol> </li> <li>To migrate the master account<ol> <li>Remove the member accounts from the organizations using procedure above</li> <li>Delete the old organization</li> <li>Repeat the process above to invite the old master account to the new org</li> </ol> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Shield/","title":"AWS Shield","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Shield/#aws-shield","title":"AWS Shield","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Shield/#shield-standard","title":"Shield Standard","text":"<ul> <li>Free service that is activated for every AWS customer</li> <li>Provides protection from SYN/UDP Floods, Reflection attacks and other layer 3 &amp; layer 4 attacks</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Shield/#shield-advanced","title":"Shield Advanced","text":"<ul> <li>DDoS mitigation service ($3,000 per month per organization)</li> <li>Protect against more sophisticated attacks on<ul> <li>EC2 instances</li> <li>Elastic Load Balancing (ELB)</li> <li>CloudFront</li> <li>Global Accelerator</li> <li>Route 53</li> </ul> </li> <li>24/7 access to AWS DDoS Response (DRP) team</li> <li>Get reimbursed for usage spikes due to DDoS</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AWS%20Solutions%20Architect%20Associate%20%28SAA-C02%29/","title":"flutter","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Amazon%20MQ/","title":"Amazon MQ","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Amazon%20MQ/#amazon-mq","title":"Amazon MQ","text":"<ul> <li>If you have some traditional applications running from on-premise, they may use open protocols such as MQTT, AMQP, STOMP, Openwire, WSS, etc. When migrating to the cloud, instead of re-engineering the application to use SQS and SNS (AWS proprietary), we can use Amazon MQ (managed Apache ActiveMQ) for communication.</li> <li>Doesn\u2019t \u201cscale\u201d as much as SQS or SNS because it is provisioned</li> <li>Runs on a dedicated machine (can run in HA with failover)</li> <li>Has both queue feature (SQS) and topic features (SNS)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Amazon%20MQ/#high-availability","title":"High Availability","text":"<ul> <li>[[Concepts#High Availability|High Availability]] in Amazon MQ works by leveraging MQ broker in multi AZ (active and standby). </li> <li>EFS (NFS that can be mounted to multi AZ) is used to keep the files safe in case the main AZ is down.  ![[attachments/Pasted image 20220509223723.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Amazon%20WorkSpaces/","title":"Amazon WorkSpaces","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Amazon%20WorkSpaces/#amazon-workspaces","title":"Amazon Workspaces","text":"<ul> <li>Persistent desktop virtualization service that enables users to access data, applications, and resources from any supported device</li> <li>Provision either Windows or Linux desktops</li> <li>Managed &amp; Secure Cloud Desktop</li> <li>Great to eliminate management of on-premise VDI (Virtual Desktop Infrastructure)</li> <li>Pay per usage (no provisioning)</li> <li>Integrated with Microsoft Active Directory</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AppSync/","title":"AppSync","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/AppSync/#appsync","title":"AppSync","text":"<ul> <li>Store and sync data across mobile and web apps in real-time</li> <li>Makes use of GraphQL (mobile technology from Facebook)</li> <li>Offline data synchronization (replaces Cognito Sync)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Athena/","title":"Athena","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Athena/#athena","title":"Athena","text":"<ul> <li>Serverless query service to perform analytics on [[Simple Storage Service (S3)|S3]]</li> <li>Uses SQL language to query the files</li> <li>Runs directly on S3 (no copying needed)</li> <li>Output stored in S3</li> <li>Built on Presto engine</li> <li>Supports CSV, JSON, ORC, AVI and Parquet file formats</li> </ul> <p>Use compressed or columnar data for cost-savings (due to less scan)</p> <p>Can be used along with AWS Transcribe (automatic speech recognition service that converts audio to text) to analyze audio for sentiment analysis.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/","title":"Aurora","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#aurora","title":"Aurora","text":"<ul> <li>Regional Service (supports global databases)</li> <li>Supports Multi AZ</li> <li>AWS managed Relational DB cluster</li> <li>Preferred over [[Relational Database Service (RDS)]]</li> <li>Auto-scaling (max 128TB)</li> <li>Up to 15 read replicas</li> <li>Asynchronous Replication (milliseconds) </li> <li>Supports only MySQL &amp; PostgreSQL</li> <li>Cloud-optimized (5x performance improvement over MySQL on RDS, over 3x the performance of PostgreSQL on RDS)</li> <li>Backtrack: restore data at any point of time without taking backups</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#endpoints","title":"Endpoints","text":"<ul> <li>Writer Endpoint (Cluster Endpoint)<ul> <li>Always points to the master (can be used for read/write)</li> <li>Each Aurora DB cluster has one cluster endpoint</li> </ul> </li> <li>Reader Endpoint<ul> <li>Provides load-balancing for read replicas only (used to read only)</li> <li>If the cluster has no read replica, it points to master (can be used to read/write)</li> <li>Each Aurora DB cluster has one reader endpoint</li> </ul> </li> <li>Custom Endpoint:<ul> <li>Used to point to a subset of replicas</li> <li>Provides load-balanced based on criteria other than the read-only or read-write capability of the DB instances like instance class (ex, direct internal users to low-capacity instances and direct production traffic to high-capacity instances)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#high-availability-read-scaling","title":"High Availability &amp; Read Scaling","text":"<ul> <li>Self healing (if some data is corrupted, it will be automatically healed)</li> <li>Storage is striped across 100s of volumes (more resilient)</li> <li>Automated failover<ul> <li>A read replica is promoted as the new master in less than 30 seconds</li> <li>Aurora flips the CNAME record for your DB Instance to point at the healthy replica</li> <li>In case no replica is available, Aurora will attempt to create a new DB Instance in the same AZ as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed.</li> </ul> </li> <li>Support for Cross Region Replication</li> <li>Aurora maintains 6 copies of your data across 3 AZ:<ul> <li>4 copies out of 6 needed for writes (can still write if 1 AZ completely fails)</li> <li>3 copies out of 6 need for reads</li> </ul> </li> </ul> <p>Each Read Replica is associated with a priority tier (0-15). In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority (lowest tier). If two or more Aurora Replicas share the same tier, then Aurora promotes the replica that is largest in size. If two or more Aurora Replicas share the same priority and size, then Aurora promotes an arbitrary replica in the same promotion tier.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#encryption-network-security","title":"Encryption &amp; Network Security","text":"<ul> <li>Encryption at rest using KMS (same as RDS)</li> <li>Encryption in flight using SSL (same as RDS)</li> <li>You can\u2019t SSH into Aurora instances (same as RDS)</li> <li>Network Security is managed using Security Groups (same as RDS)</li> <li>EC2 instances should access the DB using [[Relational Database Service (RDS)#Access Management|IAM DB Authentication]] but they can also do it using credentials fetched from the [[SSM Parameter Store]] (same as RDS) </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#aurora-serverless","title":"Aurora Serverless","text":"<ul> <li>Optional</li> <li>Automated database instantiation and auto scaling based on usage</li> <li>Good for unpredictable workloads</li> <li>No capacity planning needed</li> <li>Pay per second</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#aurora-multi-master","title":"Aurora Multi-Master","text":"<ul> <li>Optional</li> <li>Every node (replica) in the cluster can read and write</li> <li>Used for immediate failover for write node (high availability in terms of write). If disabled and the master node fails, need to promote a Read Replica as the new master (will take some time).</li> <li>Client needs to have multiple DB connections for failover</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#aurora-global-database","title":"Aurora Global Database","text":"<ul> <li>Entire database is replicated across regions to recover from region failure</li> <li>Designed for globally distributed applications with low latency local reads in each region</li> <li>1 Primary Region (read / write)</li> <li>Up to 5 secondary (read-only) regions (replication lag &lt; 1 second)</li> <li>Up to 16 Read Replicas per secondary region</li> <li>Helps for decreasing latency for clients in other geographical locations</li> <li>RTO of less than 1 minute (to promote another region as primary)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Aurora/#aurora-events","title":"Aurora Events","text":"<ul> <li>Invoke a Lambda function from an Aurora MySQL-compatible DB cluster with a native function or a stored procedure (same as [[Relational Database Service (RDS)#RDS Events|RDS]])</li> <li>Used to capture data changes whenever a row is modified</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/","title":"Auto Scaling Group (ASG)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#auto-scaling-group-asg","title":"Auto Scaling Group (ASG)","text":"<ul> <li>Regional Service</li> <li>Supports Multi AZ</li> <li>Automatically add or remove instances (scale horizontally) based on the load</li> <li>Free (pay for the underlying resources)</li> <li>IAM roles attached to an ASG will get assigned to the launched EC2 instances</li> <li>ASG can terminate instances marked as unhealthy by an ELB (and hence replace them)</li> </ul> <p>Even if an ASG is deployed across 3 AZs, minimum number of instances to remain highly available is still 2</p> <p>If you have an ASG with running instances and you delete the ASG, the instances will be terminated and the ASG will be deleted.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#scaling-policies","title":"Scaling Policies","text":"<ul> <li>Scheduled Scaling<ul> <li>Scale based on a schedule</li> <li>Used when the load pattern is predictable</li> </ul> </li> <li>Simple Scaling<ul> <li>Scale to certain size on a CloudWatch alarm</li> <li>Ex. when CPU &gt; 90%, then scale to 10 instances</li> </ul> </li> <li>Step Scaling<ul> <li>Scale incrementally in steps using CloudWatch alarms</li> <li>Ex. when CPU &gt; 70%, then add 2 units and when CPU &lt; 30%, then remove 1 unit</li> <li>Specify the instance warmup time to scale faster</li> </ul> </li> <li>Target Tracking Scaling<ul> <li>ASG maintains a CloudWatch metric and scale accordingly (automatically creates CW alarms)</li> <li>Ex. maintain CPU usage at 40%</li> </ul> </li> <li>Predictive Scaling<ul> <li>Historical data is used to predict the load pattern using ML and scale automatically</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#launch-configuration-launch-template","title":"Launch Configuration &amp; Launch Template","text":"<ul> <li>Defines the following info for ASG<ul> <li>AMI (Instance Type)</li> <li>EC2 User Data</li> <li>EBS Volumes</li> <li>Security Groups</li> <li>SSH Key Pair</li> <li>Min / Max / Desired Capacity</li> <li>Subnets (where the instances will be created)</li> <li>Load Balancer (specify which ELB to attach instances)</li> <li>Scaling Policy</li> </ul> </li> <li>Launch Configuration (legacy)<ul> <li>Cannot be updated (must be re-created)</li> <li>Does not support Spot Instances</li> </ul> </li> <li>Launch Template (newer)<ul> <li>Versioned</li> <li>Can be updated</li> <li>Supports both On-Demand and Spot Instances</li> <li>Recommended by AWS</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#scaling-cooldown","title":"Scaling Cooldown","text":"<ul> <li>After a scaling activity happens, the ASG goes into cooldown period (default 300 seconds) during which it does not launch or terminate additional instances (ignores scaling requests) to allow the metrics to stabilize.</li> <li>Use a ready-to-use AMI to launch instances faster to be able to reduce the cooldown period</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#health-checks","title":"Health Checks","text":"<ul> <li>By default, ASG uses the EC2 status check (not the ELB health check). This could explain why some instances that are labelled as unhealthy by an ELB are still not terminated by the ASG.</li> <li>To prevent ASG from replacing unhealthy instances, suspend the ReplaceUnhealthy process type</li> </ul> <p>ASG creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#termination-policy","title":"Termination Policy","text":"<ul> <li>Select the AZ with the most number of instances</li> <li>Delete the instance with the oldest launch configuration</li> <li>Delete the instance which is closest to the next billing hour</li> <li>Flow diagram<ul> <li>![[attachments/Pasted image 20220514202214.png]]</li> </ul> </li> </ul> <ul> <li>ASG does not immediately terminate instances with an Impaired status, it waits a few minutes for the instance to recover.</li> <li>ASG doesn't terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#rebalancing-azs","title":"Rebalancing AZs","text":"<ul> <li>ASG ensures that the group never goes below the minimum scale. Actions such as changing the AZ for the group or explicitly terminating or detaching instances can lead to the ASG becoming unbalanced between AZs. In such cases, ASG compensates by rebalancing the AZs by launching new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of the application.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#lifecycle-hooks","title":"Lifecycle Hooks","text":"<ul> <li>Used to perform extra steps before creating or terminating an instance. Example: <ul> <li>Install some extra software or do some checks (during pending state) before declaring the instance as \"in service\"</li> <li>Before the instance is terminated (terminating state), extract the log files</li> </ul> </li> <li>Without lifecycle hooks, pending and terminating states are avoided</li> <li>Flow diagram<ul> <li>![[attachments/Pasted image 20220506232117.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Auto%20Scaling%20Group%20%28ASG%29/#attach-running-instances-to-an-existing-asg","title":"Attach running instances to an existing ASG","text":"<ul> <li>The running instance must meet the following criteria:<ul> <li>The AMI used to launch the instance still exists</li> <li>The instance is not a member of another ASG</li> <li>The instance is launched into one of the AZ defined in your ASG</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Blue-Green%20Deployment/","title":"Blue Green Deployment","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Blue-Green%20Deployment/#blue-green-deployment","title":"Blue-Green Deployment","text":"<ul> <li>Blue-green deployment is a technique to test features in the new environment without impacting the currently running version of your application<ul> <li>Blue - current version</li> <li>Green - new version</li> </ul> </li> <li>When you\u2019re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment.</li> <li>Can mitigate common risks associated with deploying software, such as downtime and rollback capability.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFormation/","title":"CloudFormation","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFormation/#cloudformation","title":"CloudFormation","text":"<ul> <li>Infrastructure as Code (IaC) allows us to write our infrastructure as a config file which can be easily replicated &amp; versioned using Git</li> <li>Declarative way of outlining your AWS Infrastructure (no need to specify ordering and orchestration)</li> <li>Resources within a stack is tagged with an identifier (easy to track cost of each stack)</li> <li>Ability to destroy and re-create infrastructure on the fly</li> <li>Deleting a stack deletes every single artifact that was created by CloudFormation (clean way of deleting resources)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFormation/#cloudformation-templates","title":"CloudFormation Templates","text":"<ul> <li>YAML file that defines a CloudFormation Stack</li> <li>Templates have to be uploaded in S3 and then referenced in CloudFormation</li> <li>To update a template, upload a new version of the template</li> <li>Template components:<ul> <li>Resources: AWS resources declared in the template (mandatory)</li> <li>Parameters: Dynamic inputs for your template</li> <li>Mappings: Static variables for your template</li> <li>Outputs: References to what has been created (will be returned upon stack creation)</li> <li>Conditionals: List of conditions to perform resource creation</li> <li>Metadata</li> </ul> </li> <li>Templates helpers:<ul> <li>References</li> <li>Functions</li> </ul> </li> </ul> <ul> <li>You can associate the\u00a0<code>CreationPolicy</code>\u00a0attribute with a resource to prevent its status from reaching create complete until CloudFormation receives a specified number of <code>cfn-signal</code> or the timeout period is exceeded.</li> <li>Use CloudFormation with securely configured templates to ensure that applications are deployed in secure configurations</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFormation/#stack-sets","title":"Stack Sets","text":"<ul> <li>Create, update, or delete stacks across multiple accounts and regions with a single operation</li> <li>When you update a stack set, all associated stack instances are updated throughout all accounts and regions.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFormation/#updating-stacks","title":"Updating Stacks","text":"<ul> <li>CloudFormation provides two methods for updating stacks: <ul> <li>Direct update<ul> <li>CloudFormation immediately deploys the submitted changes. You cannot preview the changes.</li> </ul> </li> <li>Change Sets<ul> <li>You can preview the changes CloudFormation will make to your stack, and then decide whether to apply those changes.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/","title":"CloudFront","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#cloudfront","title":"CloudFront","text":"<ul> <li>Global service</li> <li>Global Content Delivery Network (CDN)</li> <li>Edge Locations are present outside the VPC</li> <li>Supports HTTP/RTMP protocol (does not support UDP protocol)</li> <li>Caches content at edge locations, reducing load at the origin</li> <li>Geo Restriction feature</li> <li>Improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery)</li> <li>To block a specific IP at the CloudFront level, deploy a [[Web Application Firewall (WAF)|WAF]] on CloudFront</li> <li>Supports Server Name Indication (SNI) to allow SSL traffic to multiple domains</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#origin","title":"Origin","text":"<ul> <li>[[Simple Storage Service (S3)|S3]] Bucket<ul> <li>For distributing static files</li> <li>Origin Access Identity (OAl) allows the S3 bucket to only be accessed by CloudFront</li> <li>Can be used as ingress to upload files to S3</li> </ul> </li> <li>Custom Origin (for HTTP) - need to be publicly accessible on HTTP by public IPs of edge locations<ul> <li>EC2 Instance</li> <li>ELB</li> <li>S3 Website (may contain client-side script)</li> <li>On-premise backend</li> </ul> </li> </ul> <p>To restrict access to ELB directly when it is being used as the origin in a CloudFront distribution, create a VPC Security Group for the ELB and use AWS Lambda to automatically update the CloudFront internal service IP addresses when they change.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#signed-url-cookies","title":"Signed URL / Cookies","text":"<ul> <li>Used to make a CloudFront distribution private (distribute to a subset of users)</li> <li>Signed URL \u21d2 access to individual files</li> <li>Signed Cookies \u21d2 access to multiple files</li> <li>Whenever we create a signed URL / cookie, we attach a policy specifying:<ul> <li>URL / Cookie Expiration (TTL)</li> <li>IP ranges allowed to access the data</li> <li>Trusted signers (which AWS accounts can create signed URLs)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#pricing","title":"Pricing","text":"<ul> <li>Price Class All: all regions (best performance)</li> <li>Price Class 200: most regions (excludes the most expensive regions)</li> <li>Price Class 100: only the least expensive regions</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#multiple-origin","title":"Multiple Origin","text":"<ul> <li>Route to different origins based on the path in the request<ul> <li>![[attachments/Pasted image 20220508161137.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#origin-groups","title":"Origin Groups","text":"<ul> <li>Consists of a primary and a secondary origin (can be in different regions)</li> <li>Automatic failover to secondary<ul> <li>![[attachments/Pasted image 20220508161659.png]]</li> </ul> </li> <li>Provides region-level [[Concepts#High Availability|High Availability]]</li> <li>Use when getting 504 (gateway timeout) Error</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudFront/#field-level-encryption","title":"Field-level Encryption","text":"<ul> <li>Sensitive information sent by the user is encrypted at the edge close to user which can only be decrypted by the web server (intermediate services can't see the encrypted fields)</li> <li>Asymmetric Encryption (public &amp; private key)</li> <li>Max 10 encrypted field</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudHSM/","title":"CloudHSM","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudHSM/#cloudhsm","title":"CloudHSM","text":"<ul> <li>AWS provisions dedicated encryption hardware (Hardware Security Module)</li> <li>Use when you want to manage encryption keys completely</li> <li>HSM device is stored in AWS (tamper resistant, FIPS 140-2 Level 3 compliance)</li> <li>Supports both symmetric and asymmetric encryption</li> <li>Good option to use with SSE-C encryption</li> <li>CloudHSM clusters are spread across Multi AZ (high availability)</li> <li>Redshift supports CloudHSM for database encryption and key management</li> <li>IAM permissions are required to perform CRUD operations on HSM cluster</li> <li>CloudHSM Software is used to manage the keys and users (in KMS, everything is managed using IAM)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/","title":"CloudTrail","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#cloudtrail","title":"CloudTrail","text":"<ul> <li>Global Service (a single trail can be applied to multiple regions)</li> <li>Provides governance, compliance and audit for the AWS Account</li> <li>Enabled by default</li> <li>Records the API calls made within the AWS account</li> <li>Event retention: 90 days</li> <li>Export CloudTrail logs into<ul> <li>[[CloudWatch#Logs|CloudWatch Logs]]</li> <li>[[Simple Storage Service (S3)|S3]] (encrypted by default using SSE-S3)</li> </ul> </li> </ul> <p>Modifications to log files can be detected by enabling Log File Validation on the logging bucket</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#event-types","title":"Event Types","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#management-events","title":"Management Events","text":"<ul> <li>Events of operations that modify AWS resources. Ex:<ul> <li>Creating a new IAM user</li> <li>Deleting a subnet</li> </ul> </li> <li>Enabled by default</li> <li>Can separate Read Events (that don\u2019t modify resources) from Write Events (that may modify resources)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#data-events","title":"Data Events","text":"<ul> <li>Events of operations that modify data<ul> <li>S3 object-level activity</li> <li>Lambda function execution</li> </ul> </li> <li>Disabled by default (due to high volume of data events)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#insight-events","title":"Insight Events","text":"<ul> <li>Enable CloudTrail Insights to detect unusual activity in your account<ul> <li>inaccurate resource provisioning</li> <li>hitting service limits</li> <li>bursts of AWS IAM actions</li> <li>gaps in periodic maintenance activity</li> </ul> </li> <li>CloudTrail Insights analyzes normal management events to create a baseline and then continuously analyzes write events to detect unusual patterns. If that happens, CloudTrail generates insight events that<ul> <li>show anomalies in the Cloud Trail console</li> <li>can can be logged to S3</li> <li>can trigger an EventBridge event for automation</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudTrail/#encryption","title":"Encryption","text":"<p>CloudTrail logs are encrypted by default using SSE-S3</p> <p>A single KMS key can be used to encrypt log files for trails applied to all regions</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/","title":"CloudWatch","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#cloudwatch","title":"CloudWatch","text":"<ul> <li>Serverless performance monitoring service</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#metrics","title":"Metrics","text":"<ul> <li>Variables to monitor in CloudWatch</li> <li>Dimension is an attribute of a metric (instance id, environment, etc.)</li> <li>Up to 10 dimensions per metric</li> <li>Segregated by namespaces (which AWS service they monitor)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#custom-metrics","title":"Custom Metrics","text":"<ul> <li>Define and send your own custom metrics to CloudWatch using PutMetricData API</li> <li>Metric resolution (StorageResolution API) - frequency of sending metric data</li> <li>Standard: 60 seconds</li> <li>High Resolution: 1/5/10/30 seconds (higher cost)</li> <li>Accepts metric data points two weeks in the past and two hours in the future</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#ec2-monitoring","title":"EC2 Monitoring","text":"<ul> <li>Must run a CloudWatch agent on instance to push system metrics and logs to CloudWatch.</li> <li>EC2 instances have metrics every 5 minutes</li> <li>With detailed monitoring (for a cost), you get metrics every 1 minute</li> <li>Use detailed monitoring if you want to react faster to changes (eg. scale faster for your ASG)</li> <li>Available metrics in CloudWatch:<ul> <li>CPU Utilization</li> <li>Network Utilization</li> <li>Disk Performance</li> <li>Disk Reads/Writes</li> </ul> </li> <li>Custom metrics:<ul> <li>Memory utilization (memory usage)</li> <li>Disk swap utilization</li> <li>Disk space utilization</li> <li>Page file utilization</li> </ul> </li> <li>lAM permissions must allow the instance to push logs to CloudWatch</li> <li>CloudWatch agent can be used for logging on premises servers too</li> <li>Can send logs &amp; additional system-level metrics<ul> <li>CPU (active, guest, idle, system, user, steal)</li> <li>Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)</li> <li>RAM (free, inactive, used, total, cached)</li> <li>Netstat (number of TCP and UDP connections, net packets, bytes)</li> <li>Processes (total, dead, bloqued, idle, running, sleep)</li> <li>Swap Space (free, used, used %)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#dashboards","title":"Dashboards","text":"<ul> <li>Setup custom dashboards for quick access to key metrics and alarms</li> <li>Dashboards are global (allows to monitor services across accounts &amp; regions)</li> <li>Dashboards can be shared with people who don\u2019t have an AWS account (public, email address, 3rd party SSO provider through Cognito)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#logs","title":"Logs","text":"<ul> <li>Used to store application logs</li> <li>Logs Expiration: never expire, 30 days, etc.</li> <li>Logs can be sent to:<ul> <li>S3 buckets (exports)</li> <li>Kinesis Data Streams</li> <li>Kinesis Data Firehose</li> <li>Lambda functions</li> <li>ElasticSearch</li> </ul> </li> <li>Metric Filters can be used to filter expressions and use the count to trigger CloudWatch alarms. Example filters:<ul> <li>find a specific IP in the logs</li> <li>count occurrences of \u201cERROR\u201d in the logs</li> </ul> </li> <li>Cloud Watch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards</li> <li>Logs can take up to 12 hours to become available for exporting to S3 (not real-time)</li> <li>To stream logs in real-time, apply a Subscription Filter on logs</li> <li>Logs from multiple accounts and regions can be aggregated using subscription filters<ul> <li>![[attachments/Pasted image 20220510222924.png]]</li> </ul> </li> </ul> <p>Metric Filters are a part of CloudWatch Logs (not CloudWatch Metrics)</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#alarms","title":"Alarms","text":"<ul> <li>Alarms are used to trigger notifications for CW metrics based on Metric Filters</li> <li>Various options to trigger alarm (sampling, %, max, min, etc.)</li> <li>Alarm States:<ul> <li>OK</li> <li>INSUFFICIENT_DATA</li> <li>ALARM</li> </ul> </li> <li>Period:<ul> <li>Length of time in seconds to evaluate the metric before triggering the alarm</li> <li>High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec</li> </ul> </li> <li>Targets:<ul> <li>Stop, Terminate, Reboot, or Recover an EC2 Instance</li> <li>Trigger Auto Scaling Action (ASG)</li> <li>Send notification to SNS</li> </ul> </li> </ul> <p>[!info]- EC2 Instance Recovery - CloudWatch alarm to automatically recover an EC2 instance if it becomes impaired - Terminated instances cannot be recovered - After the recovery, the following are retained   - Placement Group   - Public IP   - Private IP   - Elastic IP   - Instance ID   - Instance metadata - After the recovery, RAM contents are lost</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CloudWatch/#events","title":"Events","text":"<ul> <li>Schedule or Cron to create events on a schedule</li> <li>Uses default event bus (custom &amp; partner event buses are not supported)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cognito/","title":"Cognito","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cognito/#cognito","title":"Cognito","text":"<p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cognito/#cognito-user-pools-cup","title":"Cognito User Pools (CUP)","text":"<ul> <li>Serverless identity provider (provides sign in functionality for app users)</li> <li>Sends back a JSON Web Token (used to verify the identity of the user)</li> <li>MFA support</li> <li>Supports Federated Identities allowing users to authenticate via third party identity provider like Facebook, Google, SAML, etc.</li> <li>Seamless integration with API Gateway &amp; ALB for authentication</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cognito/#cognito-identity-pools-cip","title":"Cognito Identity Pools (CIP)","text":"<ul> <li>Provides temporary credentials (using STS) to users so they can access AWS resources</li> <li>Integrates with CUP as an identity provider</li> <li>Example use case: provide temporary access to write to an S3 bucket after authenticating the user via FaceBook (using CUP identity federation)<ul> <li>Can't use S3 pre-signed URL as we need to provide access to a bucket location and not an single object</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/","title":"Concepts","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#concepts","title":"Concepts","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#scalability","title":"Scalability","text":"<ul> <li>Ability to handle greater loads by adapting</li> <li>Vertical Scalability (scaling up / down)<ul> <li>increasing the size (performance) of the instance</li> <li>used in non-distributed systems, such as a database.</li> <li>limited by hardware</li> </ul> </li> <li>Horizontal Scalability (elasticity) (scaling out / in)<ul> <li>increasing the number of instances of the application</li> <li>used in distributed systems</li> <li>easier to achieve than vertical scalability</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#high-availability","title":"High Availability","text":"<ul> <li>Ability to survive a hardware or AZ failure</li> <li>Achieved by running at least 2 instances of the application in different AZs</li> <li>Cost effective way of implementing HA<ul> <li>Stateless<ul> <li>Create a system where only 1 EC2 instance stays active at a time. If the instance goes down, ASG will start a new one. Also, the EC2 instance will issue an API call to attach the Elastic IP based on tag. ![[attachments/Pasted image 20220513222823.png]]</li> </ul> </li> <li>Statefull<ul> <li>The EC2 instance maintains state in an EBS volume (attached to an AZ). If the instance goes down, create a snapshot of the EBS volume which will be triggered on ASG Terminate lifecycle hook. Similarly, when a new instance is spun up, create a new EBS volume in the appropriate instance using the ASG Launch lifecycle hook. ![[attachments/Pasted image 20220513222928.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#dns","title":"DNS","text":"<ul> <li>Translates the human friendly hostnames into the machine IP addresses</li> <li>Terminologies<ul> <li>Domain Registrar: registers domain names (Amazon Route 53, GoDaddy, etc.)</li> <li>DNS Records: A, AAAA, CNAME, NS, etc.</li> <li>Hosted Zone: contains DNS records (used to match hostnames to IP addresses)</li> <li>Name Server (NS): resolves DNS queries (Authoritative or Non-Authoritative)</li> <li>Top Level Domain (TLD)</li> <li>Second Level Domain (SLD)</li> </ul> </li> <li>Domain Name Structure<ul> <li>![[attachments/Pasted image 20220507120353.png]]</li> </ul> </li> <li>How DNS works<ul> <li>Your web browser wants to access the domain example.com which is being served by a server at IP 9.10.11.12. Your browser will first query the local DNS server which if it has that domain cached, it will return it right away. Otherwise, it will ask the same question to the Root DNS server. The root DNS server will extract the TLD (.com) from the domain and direct the local DNS to the TLD DNS Server that can serve .com TLD. The query to the TLD DNS server will be the same. The TLD DNS server returns the IP of the SLD DNS server which can store the IP of web server hosting example.com. Once again the same query is made to the SLD DNS Server which returns the IP 9.10.11.12 instead of NS (named server).</li> <li>![[attachments/Pasted image 20220507120627.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#cross-origin-resource-sharing-cors","title":"Cross-Origin Resource Sharing (CORS)","text":"<ul> <li> <p>An origin is a combination of scheme (protocol), host (domain) and port.    Eg: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)   Same origin: http://example.com/google &amp; http://example.com/yahoo   Different origins: http://google.example.com &amp; http://yahoo.example.com</p> </li> <li> <p>CORS is a web browser based security to allow requests to other origins while visiting the main origin only if the other origin allows for the requests from the main origin, using CORS Headers (<code>Access-Control-Allow-Origin</code> &amp; <code>Access-Control-Allow-Methods</code>)</p> </li> <li> <p>In the diagram below, web browser is on www.example.com and the server wants to redirect it to www.other.com In this case, the web browser will first send a preflight request to www.other.com via OPTIONS method, which requests permitted communication options for a given URL or server (www.example.com). The cross origin server responds with the methods that www.example.com is allowed to perform.</p> <ul> <li>![[attachments/Pasted image 20220507175132.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#database","title":"Database","text":"<ul> <li>Durability defines what are the chances of DB losing some stored data</li> <li>Availability defines how readily available data stored in the DB is</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#unicast-vs-anycast-ip","title":"Unicast vs Anycast IP","text":"<ul> <li>Unicast IP: one server holds one IP address</li> <li>Anycast IP: all servers hold the same IP address and the client is routed to the nearest one</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#hybrid-cloud","title":"Hybrid Cloud","text":"<ul> <li>Part of your infrastructure is on the cloud and the rest is on-premises</li> <li>This can be due to<ul> <li>Long cloud migrations</li> <li>Security requirements</li> <li>Compliance requirements</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#application-communication","title":"Application Communication","text":"<ul> <li>Synchronous (application \u2192 application)<ul> <li>Can be problematic if there are sudden spikes of traffic and one of the services gets overwhelmed</li> </ul> </li> <li>Asynchronous / Event-based (application \u2192 queue \u2192 application) <ul> <li>Allows applications to scale independently to handle spikes in traffic</li> </ul> </li> </ul> <p>![[attachments/Pasted image 20220509194411.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#kubernetes","title":"Kubernetes","text":"<ul> <li>Kubernetes is an open-source system for automatic deployment, scaling and management of containerized (usually Docker) applications</li> <li>Kubernetes is cloud-agnostic (can be used in any cloud provider). So, it is much more standardized.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#serverless","title":"Serverless","text":"<ul> <li>Serverless is a new paradigm in which the developers don\u2019t have to provision or manage servers. They just deploy code.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#micro-services-architecture","title":"Micro-Services Architecture","text":"<ul> <li>Many services interact with each other directly using a REST API</li> <li>Allows us to have a leaner development lifecycle for each service</li> <li>Services can scale independently of each other</li> <li>Each service has a separate code repository (easy for development)</li> <li>Communication between services:<ul> <li>Synchronous patterns: API Gateway, Load Balancers</li> <li>Asynchronous patterns: SQS, Kinesis, SNS</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#microsoft-active-directory-ad","title":"Microsoft Active Directory (AD)","text":"<ul> <li>It is a way to share login credentials of the users with all the machines within the network.</li> <li>Objects are organized in trees. A group of trees is a forest</li> <li>There is a domain controller. We will create an account there. Since each windows machine on the network is connected to the domain controller, this user can be logged in from any machine on the network.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#ephemeral-ports","title":"Ephemeral Ports","text":"<p>When a client sends an HTTP request to a server, it does so on a fixed IP and port of the server. In the request, the client also sends a temporary port for the server to respond back. The server when sending the response uses this port which is only lived for the duration of this HTTP connection. ![[attachments/Pasted image 20220512210852.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Concepts/#ipv6","title":"IPv6","text":"<ul> <li>IPv4 designed to provide 4.3 Billion addresses (they\u2019ll be exhausted soon)</li> <li>IPV6 is designed to provide 3.4 x 10^38 unique IP addresses</li> <li>Every IPv6 address is public and Internet-routable (no private range)</li> <li>Format x.x.x.x.x.x.x.x (x is hexadecimal, range can be from 0000 to ffff)</li> <li>Examples:<ul> <li>2001:db8:3333:4444:5555:6666:7777:8888</li> <li>2001:db8:3333:4444:cccc:dddd:eeee:ffff</li> <li>:: \u21d2 all 8 segments are zero</li> <li>2001:db8:: \u21d2 the last 6 segments are zero</li> <li>:5678 \u21d2 the first 6 segments are zero</li> <li>2001:db8:5678 \u21d2 the middle 4 segments are zero</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Config/","title":"Config","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Config/#config","title":"Config","text":"<ul> <li>Regional service</li> <li>Can be aggregated across regions and accounts</li> <li>Record configurations changes over time</li> <li>Evaluate compliance of resources using config rules</li> <li>Does not prevent non-compliant actions from happening (no deny)</li> <li>Evaluate config rules<ul> <li>for each config change (ex. configuration of EBS volume is changed)</li> <li>at regular time intervals (ex. every 2 hours)</li> </ul> </li> <li>Can make custom config rules (must be defined in Lambda functions) such as:<ul> <li>Check if each EBS disk is of type gp2</li> <li>Check if each EC2 instance is t2.micro</li> </ul> </li> <li>Can be used along with CloudTrail to get a timeline of changes in configuration and compliance overtime.</li> <li>Integrates with EventBridge or SNS to trigger notifications when AWS resources are non-compliant</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Config/#remediation","title":"Remediation","text":"<ul> <li>Automate remediation of non-compliant resources using SSM Automation Documents<ul> <li>AWS-Managed Automation Documents</li> <li>Custom Automation Documents<ul> <li>to invoke a Lambda function for automation</li> </ul> </li> </ul> </li> <li>You can set Remediation Retries if the resource is still non-compliant after auto remediation</li> <li>Ex. if IAM access key expires (non-compliant), trigger an auto-remediation action to revoke unused IAM user credentials.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Continuous%20Integration%20Continuous%20Delivery%20%28CICD%29/","title":"Continuous Integration Continuous Delivery (CICD)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Continuous%20Integration%20Continuous%20Delivery%20%28CICD%29/#continuous-integration-continuous-delivery-cicd","title":"Continuous Integration Continuous Delivery (CICD)","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Continuous%20Integration%20Continuous%20Delivery%20%28CICD%29/#continuous-integration","title":"Continuous Integration","text":"<ul> <li>Developers push the code to a code repository often (GitHub / CodeCommit / Bitbucket)</li> <li>A testing / build server checks the code as soon as it\u2019s pushed (CodeBuild / Jenkins CI)</li> <li>The developer gets feedback about the tests and checks that have passed / failed</li> <li>Deliver faster as the code is tested automatically</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Continuous%20Integration%20Continuous%20Delivery%20%28CICD%29/#continuous-delivery","title":"Continuous Delivery","text":"<ul> <li>Ensure that the software can be released reliably whenever needed</li> <li>Ensures deployments happen often and are quick</li> <li>Automated deployment using CodeDeploy, Jenkins CD, etc.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Continuous%20Integration%20Continuous%20Delivery%20%28CICD%29/#technology-stack-for-cicd","title":"Technology Stack for CICD","text":"<ul> <li>AWS CodeCommit is a fully managed code repository.</li> <li>AWS CodeBuild is a fully managed continuous integration (CI) service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is an alternative to Jenkins.</li> <li>AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of computing services such as EC2, Fargate, Lambda, and your on-premises servers. You can define the strategy you want to execute such as in-place or blue/green deployments.<ul> <li>Used to deploy application, not infrastructure (use [[CloudFormation]] for that)</li> </ul> </li> <li>AWS CodePipeline is a fully managed continuous delivery (CD) service that helps you automate your release pipeline for fast and reliable application and infrastructure updates. It automates the build, test, and deploy phases of your release process every time there is a code change. It has direct integration with Elastic Beanstalk. ![[attachments/Pasted image 20220513225544.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cost%20Effective%20Highly%20Available%20Monolithic%20Architecture/","title":"Cost Effective Highly Available Monolithic Architecture","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cost%20Effective%20Highly%20Available%20Monolithic%20Architecture/#cost-effective-highly-available-monolithic-architecture","title":"Cost Effective Highly Available Monolithic Architecture","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cost%20Effective%20Highly%20Available%20Monolithic%20Architecture/#stateless","title":"Stateless","text":"<ul> <li>Only 1 EC2 instance stays active at a time and can be accessed using an Elastic IP (static)</li> <li>If the instance goes down, ASG will start a new one in another AZ. </li> <li>The newly launched EC2 instance will issue an API call to attach the Elastic IP based on tag using the EC2 user data. </li> <li>Should not use ALB as the traffic only needs to be routed to a single instance (cost effective) ![[attachments/Pasted image 20220518222221.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Cost%20Effective%20Highly%20Available%20Monolithic%20Architecture/#stateful","title":"Stateful","text":"<ul> <li>The EC2 instance maintains state in an EBS volume (attached to an AZ). If the instance goes down, create a snapshot of the EBS volume which will be triggered on ASG Terminate lifecycle hook. Similarly, when a new instance is spun up, create a new EBS volume in the appropriate instance using the ASG Launch lifecycle hook. ![[attachments/Pasted image 20220518222310.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CostExplorer/","title":"CostExplorer","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/CostExplorer/#costexplorer","title":"CostExplorer","text":"<ul> <li>Visualize and manage your costs and service usage over time</li> <li>Create custom reports that analyze cost and usage data</li> <li>Recommendations to choose an optimal savings plan</li> <li>View usage for the last 12 months &amp; forecast up to 12 months</li> </ul> <p>AWS Cost Explorer helps you identify under-utilized EC2 instancess</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DDoS%20Protection/","title":"DDoS Protection","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DDoS%20Protection/#ddos-protection","title":"DDoS Protection","text":"<p>Shield will protect against DDoS attack and WAF will control the kind of requests that can pass through. ![[attachments/Pasted image 20220511231801.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DataSync/","title":"DataSync","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DataSync/#datasync","title":"DataSync","text":"<ul> <li>Move large amounts of data from your on-premises NAS or file system via NFS or SMB protocol to AWS over the public internet using TLS</li> <li>Can synchronize to: <ul> <li>S3 (all storage classes)</li> <li>EFS</li> <li>FSx for Windows</li> </ul> </li> <li>Scheduled Replication (not continuous)</li> <li>Need to install AWS DataSync Agent on premises</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220513214950.png]]</li> </ul> </li> <li>Can also be used to transfer between two EFS in different regions<ul> <li>![[attachments/Pasted image 20220513215131.png]]</li> </ul> </li> <li>Suitable in automating and accelerating online data transfers to a variety of AWS storage services (over [[Storage Gateway]] which only works with S3)</li> </ul> <p>Perfect to move large amounts of historical data from on-premises to S3 Glacier Deep Archive (directly).</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/","title":"Database Migration Service (DMS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/#database-migration-service-dms","title":"Database Migration Service (DMS)","text":"<ul> <li>Migrate entire databases from on-premises to AWS cloud</li> <li>The source database remains available during migration</li> <li>Continuous Data Replication using CDC (change data capture)</li> <li>Requires EC2 instance running the DMS software to perform the replication tasks. If the amount of data is large, use a large instance. If multi-AZ is enabled, need an instance in each AZ.</li> </ul> <p>Most resource-efficient way with least development and no management, to continuously replicate data to [[Redshift]] for analytics (first moves data to an S3 bucket and then to Redshift)</p> <p>DMS supports S3 as the source and Kinesis as the target, so data stored in an S3 bucket can be streamed to Kinesis. This also supports CDC to stream new data that is put in the S3 bucket.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/#types-of-migration","title":"Types of Migration","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/#homogeneous-migration","title":"Homogeneous Migration","text":"<ul> <li>When the source and target DB engines are the same (eg. Oracle to Oracle)</li> <li>One step process:<ul> <li>Use the Database Migration Service (DMS) to migrate data from the source database to the target database</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/#heterogeneous-migration","title":"Heterogeneous Migration","text":"<ul> <li>When the source and target DB engines are different (eg. Microsoft SQL Server to Aurora)</li> <li>Two step process:<ol> <li>Use the Schema Conversion Tool (SCT) to convert the source schema and code to match that of the target database</li> <li>Use the Database Migration Service (DMS) to migrate data from the source database to the target database</li> </ol> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Database%20Migration%20Service%20%28DMS%29/#migrating-using-snow-family","title":"Migrating using [[Snow Family]]","text":"<ol> <li>Use the Schema Conversion Tool (SCT) to extract the data locally and move it to the Edge device</li> <li>Ship the Edge device or devices back to AWS</li> <li>After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket.</li> <li>AWS DMS takes the files and migrates the data to the target data store (eg. DynamoDB)</li> </ol>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/","title":"Direct Connect (DX)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/#direct-connect-dx","title":"Direct Connect (DX)","text":"<ul> <li>Dedicated private connection from an on-premise data center to a VPC<ul> <li>![[attachments/Pasted image 20220512233459.png]]</li> </ul> </li> <li>Data in transit is not-encrypted but the connection is private (secure)</li> <li>More stable and secure than Site-to-Site VPN</li> <li>Access public &amp; private resources on the same connection using Public &amp; Private Virtual Interface (VIF) respectively</li> <li>Connection to a data center is made from a Direct Connect Location</li> <li>Connects to a Virtual Private Gateway (VGW) on the VPC end</li> <li>Supports both IPv4 and IPv6</li> <li>Supports Hybrid Environments (on premises + cloud)</li> <li>Lead time &gt; 1 month</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/#connection-types","title":"Connection Types","text":"<ul> <li>Dedicated Connection<ul> <li>1 Gbps and 10 Gbps (fixed capacity)</li> <li>Physical ethernet port dedicated to a customer</li> </ul> </li> <li>Hosted Connection<ul> <li>50Mbps, 500 Mbps, up to 10 Gbps</li> <li>On-demand capacity scaling (more flexible than dedicated connection)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/#encryption","title":"Encryption","text":"<ul> <li>For encryption in flight, use AWS Direct Connect + VPN which provides an IPsec-encrypted private connection<ul> <li>![[attachments/Pasted image 20220512234026.png]]</li> </ul> </li> <li>Good for an extra level of security</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/#resiliency","title":"Resiliency","text":"<ul> <li>Best way (redundant direct connect connections)<ul> <li>![[attachments/Pasted image 20220512234246.png]]</li> </ul> </li> <li>Cost-effective way (VPN connection as a backup)<ul> <li>Implement an IPSec VPN connection and use the same BGP prefix. Both the Direct Connect connection and IPSec VPN are active and being advertised using the Border Gateway Protocol (BGP). The Direct Connect link will always be preferred unless it is unavailable.</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Direct%20Connect%20%28DX%29/#direct-connect-gateway","title":"Direct Connect Gateway","text":"<ul> <li>Used to setup a Direct Connect to multiple VPCs, possibly in different regions but same account</li> <li>Using DX, we will create a Private VIF to the Direct Connect Gateway which will extend the VIF to Virtual Private Gateways in multiple VPCs (possibly across regions).<ul> <li>![[attachments/Pasted image 20220512234818.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/","title":"Disaster Recovery","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>Any event that has a negative impact on a company\u2019s business continuity or finances is a disaster</li> <li>Recovery Point Objective (RPO): how often you backup your data (determines how much data are you willing to lose in case of a disaster)</li> <li>Recovery Time Objective (RTO): how long it takes to recover from the disaster (down time)</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220513102636.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#strategies","title":"Strategies","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#backup-restore","title":"Backup &amp; Restore","text":"<ul> <li>High RPO (hours)</li> <li>Need to spin up instances and restore volumes from snapshots in case of disaster =&gt; High RTO</li> <li>Cheapest &amp; easiest to manage</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#pilot-light","title":"Pilot Light","text":"<ul> <li>Critical parts of the app are always running in the cloud (eg. continuous replication of data to another region)</li> <li>Low RPO (minutes)</li> <li>Critical systems are already up =&gt; Low RTO</li> <li>Ideal when RPO should be in minutes and the solution should be inexpensive</li> <li>DB is critical so it is replicated continuously but EC2 instance is spin up only when a disaster strikes</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#warm-standby","title":"Warm Standby","text":"<ul> <li>A complete backup system is up and running at the minimum capacity. This system is quickly scaled to production capacity in case of a disaster.</li> <li>Very low RPO &amp; RTO (minutes)</li> <li>Expensive</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Disaster%20Recovery/#multi-site-or-hot-site-approach","title":"Multi-Site or Hot Site Approach","text":"<ul> <li>A backup system is running at full production capacity and the request can be routed to either the main or the backup system.</li> <li>Multi-data center approach</li> <li>Lowest RPO &amp; RTO (minutes or seconds)</li> <li>Very Expensive</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/","title":"DynamoDB","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/#dynamodb","title":"DynamoDB","text":"<ul> <li>Serverless NoSQL DB with multi-AZ</li> <li>Distributed Database</li> <li>Not an in-memory database (uses storage devices)</li> <li>Storage auto-scaling</li> <li>Single digit millisecond response time at any scale</li> <li>Maximum size of an item: 400 KB</li> <li>Primary key (must be decided at creation) can be a single field or a pair of fields (partition key and sort key)</li> <li>Indexes allow us to query on attributes other than the Primary Key</li> <li>Supports TTL (automatically delete an item after an expiry timestamp)</li> <li>Supports Transactions (either write to multiple tables or write to none)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/#capacity","title":"Capacity","text":"<ul> <li>Provisioned Mode (default)<ul> <li>Provision read &amp; write capacity</li> <li>Pay for the provisioned capacity</li> <li>Auto-scaling option (eg. set RCU and WCU to 80% and the capacities will be scaled automatically based on the workload)</li> </ul> </li> <li>On-demand Mode<ul> <li>Capacity auto-scaling based on the workload</li> <li>Pay for what you use (more expensive)</li> <li>Great for unpredictable workloads</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/#dynamodb-accelerator-dax","title":"DynamoDB Accelerator (DAX)","text":"<ul> <li>Caches the queries and scans of DynamoDB items</li> <li>Solves read congestion (<code>ProvisionedThroughputExceededException</code>)</li> <li>Microseconds latency for cached data</li> <li>Doesn\u2019t require application code changes</li> <li>5 minutes TTL for cache (default)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/#dynamodb-streams","title":"DynamoDB Streams","text":"<ul> <li>Ordered stream of notifications of item-level modifications (create/update/delete) in a table</li> <li>Destination can be<ul> <li>Kinesis Data Streams</li> <li>AWS Lambda</li> <li>Kinesis Client Library applications</li> </ul> </li> <li>Data Retention for up to 24 hours</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/DynamoDB/#global-tables","title":"Global Tables","text":"<ul> <li>For low latency access in multiple-regions</li> <li>Applications can READ and WRITE to the table in any region and the change will automatically be replicated to other tables (active-active cross-region replication)</li> <li>Must enable DynamoDB Streams as a pre-requisite</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElastiCache/","title":"ElastiCache","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElastiCache/#elasticache","title":"ElastiCache","text":"<ul> <li>Regional Service</li> <li>AWS managed caching service</li> <li>In-memory key-value store with sub-millisecond latency</li> <li>Need to provision an underlying EC2 instance</li> <li>Makes the application stateless because it doesn\u2019t have to cache locally</li> <li>Using ElastiCache requires heavy application code changes (setup the application to query the cache before and after querying the database)</li> <li>Usage:<ul> <li>DB Cache (lazy loading): cache read operations on a database (reduced latency)</li> <li>Session Store: store user's session data like cart info (allows the application to remain stateless)</li> <li>Global Data Store: store intermediate computation results</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElastiCache/#redis-vs-memcached","title":"Redis vs Memcached","text":"Redis Memcached In-memory data store Distributed memory object cache Read Replicas (for scaling reads &amp; HA) No replication Backup &amp; restore No backup &amp; restore Single-threaded Multi-threaded HIPAA compliant Not HIPAA compliant Data is stored in an in-memory DB which is replicated Data is partitioned across multiple nodes (sharding) Redis Sorted Sets are used in realtime Gaming Leaderboards Good for auto-completion Multi-AZ support with automatic failover (disaster recovery)"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElastiCache/#security-access-management","title":"Security &amp; Access Management","text":"<ul> <li>Network security is managed using Security Groups (only allow EC2 security group for incoming requests)</li> <li>At rest encryption using KMS</li> <li>In-flight encryption using SSL</li> <li>Use Redis Auth to authenticate to ElastiCache for Redis</li> <li>Memcached supports SASL-based authentication</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Beanstalk/","title":"Elastic Beanstalk","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Beanstalk/#elastic-beanstalk","title":"Elastic Beanstalk","text":"<ul> <li>Used to deploy applications on AWS infrastructure</li> <li>Platform as a Service (PaaS)</li> <li>Automatically handles capacity provisioning, load balancing, scaling, application health monitoring, instance configuration, etc. but we have full control over the configuration</li> <li>Free (pay for the underlying resources)</li> <li>Supports versioning of application code</li> <li>Can create multiple environment (dev, test, prod)</li> <li>Supports the deployment of web applications from Docker containers and automatically handles load balancing, auto-scaling, monitoring, and placing containers across the cluster.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Beanstalk/#web-worker-environments","title":"Web &amp; Worker Environments","text":"<ul> <li>Web Environment (Web Server Tier): clients requests are directly handled by EC2 instances through a load balancer.</li> <li>Worker Environment (Worker Tier): clients\u2019s requests are put in a SQS queue and the EC2 instances will pull the messages to process them. Scaling depends on the number of SQS messages in the queue. ![[attachments/Pasted image 20220507153757.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/","title":"Elastic Block Storage (EBS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]] [[Elastic Compute Cloud (EC2)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#elastic-block-storage-ebs","title":"Elastic Block Storage (EBS)","text":"<ul> <li>Volume Network Drive (provides low latency access to data)</li> <li>Can only be mounted to 1 instance at a time (except EBS multi-attach)</li> <li>Bound to an AZ</li> <li>Must provision capacity in advance (size in GB &amp; throughput in IOPS)</li> <li>By default, upon instance termination, the root EBS volume is deleted and any other attached EBS volume is not deleted (can be over-ridden using <code>DeleteOnTermination</code>\u00a0attribute)</li> <li>To replicate an EBS volume across AZ or region, need to copy its snapshot</li> <li>EBS Multi-attach allows the same EBS volume to attach to multiple EC2 instances in the same AZ</li> </ul> <p><code>DeleteOnTermination</code>\u00a0attribute can be updated for the root EBS volume only from the CLI</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#volume-types","title":"Volume Types","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#general-purpose-ssd","title":"General Purpose SSD","text":"<ul> <li>Good for system boot volumes, virtual desktops</li> <li>Storage: 1 GB - 16 TB</li> <li>gp3<ul> <li>3,000 lOPS baseline (max 16,000 - independent of size)</li> <li>125 MiB/s throughput (max 1000MiB/s - independent of size)</li> </ul> </li> <li>gp2<ul> <li>Burst IOPS up to 3,000</li> <li>3 IOPS per GB</li> <li>Max IOPS: 16,000 (at 5,334 GB)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#provisioned-iops-ssd","title":"Provisioned IOPS SSD**","text":"<ul> <li>Optimized for Transaction-intensive Applications with high frequency of small &amp; random IO operations. They are sensitive to increased I/O latency.</li> <li>Maintain high IOPS while keeping I/O latency down by maintaining a low queue length and a high number of IOPS available to the volume.</li> <li>Supports EBS Multi-attach (not supported by other types)</li> <li>io1 or io2<ul> <li>Storage: 4 GB - 16 TB</li> <li>Max IOPS: 64,000 for Nitro EC2 instances &amp; 32,000 for non-Nitro</li> <li>50 lOPS per GB (64,000 IOPS at 1,280 GB)</li> <li>io2 have more durability and more IOPS per GB (at the same price as io1)</li> </ul> </li> <li>io2 Block Express<ul> <li>Storage: 4 GiB - 64 TB</li> <li>Sub-millisecond latency</li> <li>Max IOPS: 256,000</li> <li>1000 lOPS per GB</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#hard-disk-drives-hdd","title":"Hard Disk Drives (HDD)","text":"<ul> <li>Optimized for Throughput-intensive Applications that require large &amp; sequential IO operations and are less sensitive to increased I/O latency (big data, data warehousing, log processing)</li> <li>Maintain high throughput to HDD-backed volumes by maintaining a high queue length when performing large, sequential I/O</li> <li>Cannot be used as boot volume for an EC2 instance</li> <li>Storage: 125 MB - 16 TB</li> <li>Throughput Optimized HDD (st1)<ul> <li>Optimized for large sequential reads and writes (Big Data, Data Warehouses, Log Processing)</li> <li>Max throughput: 500 MB/s</li> <li>Max IOPS: 500</li> </ul> </li> <li>Cold HDD (sc1)<ul> <li>For infrequently accessed data</li> <li>Cheapest</li> <li>Max throughput: 250 MB/s</li> <li>Max IOPS: 250</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#encryption","title":"Encryption","text":"<ul> <li>Optional</li> <li>For Encrypted EBS volumes<ul> <li>Data at rest is encrypted</li> <li>Data in-flight between the instance and the volume is encrypted</li> <li>All snapshots are encrypted</li> <li>All volumes created from the snapshot are encrypted</li> </ul> </li> <li>Encrypt an un-encrypted EBS volume<ul> <li>Create an EBS snapshot of the volume</li> <li>Copy the EBS snapshot and encrypt the new copy</li> <li>Create a new EBS volume from the encrypted snapshot (the volume will be automatically encrypted)</li> </ul> </li> </ul> <p>All EBS types and all instance\u00a0families\u00a0support encryption but not all instance\u00a0types\u00a0support encryption.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#snapshots","title":"Snapshots","text":"<ul> <li>Data Lifecycle Manager (DLM) can be used to automate the creation, retention, and deletion of snapshots of EBS volumes</li> <li>Snapshots are incremental</li> <li>Only the most recent snapshot is required to restore the volume</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Block%20Storage%20%28EBS%29/#raid","title":"RAID","text":"<ul> <li>RAID 0<ul> <li>Improve performance of a storage volume by distributing reads &amp; writes in a stripe across attached volumes</li> <li>If you add a storage volume, you get the straight addition of throughput and IOPS</li> <li>For high performance applications</li> </ul> </li> <li>RAID 1<ul> <li>Improve data availability by mirroring data in multiple volumes</li> <li>For critical applications</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/","title":"Elastic Compute Cloud (EC2)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#elastic-compute-cloud-ec2","title":"Elastic Compute Cloud (EC2)","text":"<ul> <li>Regional Service</li> <li>EC2 (Elastic Compute Cloud) is an Infrastructure as a Service (IaaS)</li> <li>Stopping &amp; Starting an instance may change its public IP but not its private IP</li> <li>AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads</li> <li>There is a vCPU-based On-Demand Instance soft limit per region</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#user-data","title":"User Data","text":"<ul> <li>Some commands that run when the instance is launched for the first time (doesn't execute for subsequent runs)</li> <li>Used to automate dynamic boot tasks (that cannot be done using AMIs)<ul> <li>Installing updates</li> <li>Installing software</li> <li>Downloading common files from the internet</li> </ul> </li> <li>Runs with the root user privilege</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#instance-classes","title":"Instance Classes","text":"<ul> <li>General Purpose<ul> <li>Great for a diversity of workloads such as web servers or code repositories</li> <li>Balance between compute, memory &amp; networking</li> </ul> </li> <li>Compute Optimized<ul> <li>Great for compute intensive tasks<ul> <li>Batch Processing</li> <li>Media Transcoding</li> <li>HPC</li> <li>Gaming Servers</li> </ul> </li> </ul> </li> <li>Memory Optimized<ul> <li>Great for in-memory databases or distributed web caches</li> </ul> </li> <li>Storage Optimized<ul> <li>Great for storage intensive tasks (accessing local databases)<ul> <li>OLTP systems</li> <li>Distributed File System (DFS)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#security-groups","title":"Security Groups","text":"<ul> <li>Only contain Allow rules</li> <li>External firewall for EC2 instances (if a request is blocked by SG, instance will never know)</li> <li>Security groups rules can reference a resource by IP or Security Group</li> <li>Default SG<ul> <li>inbound traffic from the same SG is allowed</li> <li>all outbound traffic is allowed</li> </ul> </li> <li>New SG<ul> <li>all inbound traffic is blocked</li> <li>all outbound traffic is allowed</li> </ul> </li> <li>A security group can be attached to multiple instances and vice versa</li> <li>Bound to a VPC (and hence to a region)</li> <li>Recommended to maintain a separate security group for SSH access</li> <li>Blocked requests will give a Time Out error</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#iam-roles-for-ec2-instances","title":"IAM Roles for EC2 instances","text":"<ul> <li>Never enter AWS credentials into the EC2 instance, instead attach [[Identity &amp; Access Management (IAM)#Roles|IAM Roles]] to the instances</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#purchasing-options","title":"Purchasing Options","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#on-demand-instances","title":"On-demand Instances","text":"<ul> <li>Pay per use (no upfront payment)</li> <li>Highest cost</li> <li>No long-term commitment </li> <li>Recommended for short-term, uninterrupted and unpredictable workloads</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#standard-reserved-instances","title":"Standard Reserved Instances","text":"<ul> <li>Reservation Period: 1 year or 3 years</li> <li>Recommended for steady-state applications (like database)</li> <li>Sell unused instances on the Reserved Instance Marketplace</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#convertible-reserved-instances","title":"Convertible Reserved Instances","text":"<ul> <li>Can change the instance type</li> <li>Lower discount</li> <li>Cannot sell unused instances on the Reserved Instance Marketplace</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#scheduled-reserved-instances","title":"Scheduled Reserved Instances","text":"<ul> <li>reserved for a time window (ex. everyday from 9AM to 5PM)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#spot-instances","title":"Spot Instances","text":"<ul> <li>Work on a bidding basis where you are willing to pay a specific max hourly rate for the instance. Your instance can terminate if the spot price increases.</li> <li>Spot blocks are designed not to be interrupted</li> <li>Good for workloads that are resilient to failure<ul> <li>Distributed jobs (resilient if some nodes go down)</li> <li>Batch jobs</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#dedicated-hosts","title":"Dedicated Hosts","text":"<ul> <li>Server hardware is allocated to a specific company (not shared with other companies)</li> <li>3 year reservation period</li> <li>Billed per host</li> <li>Useful for software that have BYOL (Bring Your Own License) or for companies that have strong regulatory or compliance needs</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#dedicated-instances","title":"Dedicated Instances","text":"<ul> <li>Dedicated hardware</li> <li>Billed per instance</li> <li>No control over instance placement</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#on-demand-capacity-reservations","title":"On-Demand Capacity Reservations","text":"<ul> <li>Ensure you have the available capacity in an AZ to launch EC2 instances when needed</li> <li>Can reserve for a recurring schedule (ex. everyday from 9AM to 5PM)</li> <li>No need for 1 or 3-year commitment (independent of billing discounts)</li> <li>Need to specify the following to create capacity reservation:         - AZ         - Number of instances         - Instance attributes</li> </ul> <p>[!important]- Reserved Capacity &amp; Instances Comparison ![[attachments/Pasted image 20220516114146.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#spot-instances_1","title":"Spot Instances","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#spot-requests","title":"Spot Requests","text":"<ul> <li>One-time: Request once opened, spins up the spot instances and the request closes.</li> <li>Persistent: <ul> <li>Request will stay disabled while the spot instances are up and running. </li> <li>It becomes active after the spot instance is interrupted. </li> <li>If you stop the spot instance, the request will become active only after you start the spot instance. </li> </ul> </li> <li>You can only cancel spot instance requests that are open, active, or disabled.</li> <li>Cancelling a Spot Request does not terminate instances. You must first cancel a Spot Request, and then terminate the associated Spot Instances.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#spot-fleets","title":"Spot Fleets","text":"<ul> <li>Combination of spot and on-demand instances (optional) that tries to optimize for cost or capacity</li> <li>Launch Templates must be used to have on-demand instances in the fleet</li> <li>Can consist of instances of different classes</li> <li>Strategies to allocate Spot Instances:<ul> <li>lowestPrice - from the pool with the lowest price (cost optimization, short workload)</li> <li>diversified - distributed across all pools (great for availability, long workloads)</li> <li>capacityOptimized - pool with the optimal capacity for the number of instances</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#elastic-ip","title":"Elastic IP","text":"<ul> <li>Static Public IP that you own as long as you don't delete it</li> <li>Can be attached to an EC2 instance (even when it is stopped)</li> <li>Soft limit of 5 elastic IPs per account</li> <li>Doesn\u2019t incur charges as long as the following conditions are met (EIP behaving like any other public IP randomly assigned to an EC2 instance):<ul> <li>The Elastic IP is associated with an Amazon EC2 instance</li> <li>The instance associated with the Elastic IP is running</li> <li>The instance has only one Elastic IP attached to it</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#placement-groups-placement-strategies","title":"Placement Groups (Placement Strategies)","text":"<ul> <li>Cluster Placement Group (optimize for network)<ul> <li>All the instances are placed on the same hardware (same rack)</li> <li>Pros: Great network (10 Gbps bandwidth between instances)</li> <li>Cons: If the rack fails, all instances will fail at the same time</li> <li>Used in HPC (minimize inter-node latency &amp; maximize throughput)</li> <li>Image<ul> <li>![[attachments/Pasted image 20220505231518.png]]</li> </ul> </li> </ul> </li> <li>Spread Placement Group (maximize availability)<ul> <li>Each instance is in a separate rack (physical hardware) inside an AZ</li> <li>Supports Multi AZ</li> <li>Up to 7 instances per AZ per placement group (ex. for 15 instances, need 3 AZ)</li> <li>Used for critical applications</li> <li>Image<ul> <li>![[attachments/Pasted image 20220505232110.png]]</li> </ul> </li> </ul> </li> <li>Partition Placement Group (balance of performance and availability)<ul> <li>Instances in a partition share rack with each other</li> <li>If the rack goes down, the entire partition goes down</li> <li>Up to 7 partitions per AZ</li> <li>Used in big data applications (Hadoop, HDFS, HBase, Cassandra, Kafka)</li> <li>Image<ul> <li>![[attachments/Pasted image 20220505232434.png]]</li> </ul> </li> </ul> </li> </ul> <p>If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Restarting the instances may migrate them to hardware that has capacity for all the requested instances.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#elastic-network-interface-eni","title":"Elastic Network Interface (ENI)","text":"<ul> <li>ENI is a virtual network card that gives a private IP to an EC2 instance</li> <li>A primary ENI is created and attached to the instance upon creation and will be deleted automatically upon instance termination.</li> <li>We can create additional ENIs and attach them to an EC2 instance to access it via multiple private IPs.</li> <li>We can detach &amp; attach ENIs across instances</li> <li>ENIs are tied to the subnet (and hence to the AZ)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#instance-states","title":"Instance States","text":"<ul> <li>Stop<ul> <li>EBS root volume is preserved</li> </ul> </li> <li>Terminate<ul> <li>EBS root volume gets destroyed</li> </ul> </li> <li>Hibernate<ul> <li>Hibernation saves the contents from the instance memory (RAM) to the EBS root volume</li> <li>EBS root volume is preserved</li> <li>The instance boots much faster as the OS is not stopped and restarted</li> <li>When you start your instance:<ul> <li>EBS root volume is restored to its previous state</li> <li>RAM contents are reloaded</li> <li>Processes that were previously running on the instance are resumed</li> <li>Previously attached data volumes are reattached and the instance retains its instance ID</li> </ul> </li> <li>Should be used for applications that take a long time to start</li> <li>Not supported for Spot Instances</li> <li>Max hibernation duration = 60 days</li> </ul> </li> <li>Standby<ul> <li>Instance remains attached to the [[Auto Scaling Group (ASG)|ASG]] but is temporarily put out of service (the ASG doesn't replace this instance)</li> <li>Used to install updates or troubleshoot a running instance</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#ec2-nitro","title":"EC2 Nitro","text":"<ul> <li>Newer virtualization technology for EC2 instances</li> <li>Better networking options (enhanced networking, HPC, IPv6)</li> <li>Higher Speed EBS (64,000 EBS IOPS max on Nitro instances whereas 32,000 on non-Nitro)</li> <li>Better underlying security</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#vcpu-threads","title":"vCPU &amp; Threads","text":"<ul> <li>vCPU is the total number of concurrent threads that can be run on an EC2 instance</li> <li>Usually 2 threads per CPU core (eg. 4 CPU cores \u21d2 8 vCPU)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#storage","title":"Storage","text":"<p>[[Instance Store]] [[Elastic Block Storage (EBS)]] [[Elastic File System (EFS)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#monitoring","title":"Monitoring","text":"<p>[[CloudWatch#EC2 Monitoring]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#amazon-machine-image-ami","title":"Amazon Machine Image (AMI)","text":"<ul> <li>AMIs are the image of the instance after installing all the necessary OS, software and configuring everything. </li> <li>It boots much faster because the whole thing is pre-packaged and doesn\u2019t have to be installed separately for each instance.</li> <li>Good for static configurations</li> <li>Bound to a region (can be copied across regions)</li> </ul> <p>When the new AMI is copied from region A into region B, it automatically creates a snapshot in region B because AMIs are based on the underlying snapshots.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#instance-metadata","title":"Instance Metadata","text":"<ul> <li>Url to fetch metadata about the instance (http://169.254.169.254/latest/meta-data)</li> <li>This URL is internal to AWS and can only be hit from the instance</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#ec2-classic-classiclink","title":"EC2 Classic &amp; ClassicLink","text":"<ul> <li>Instances run in single network shared with other customers (this is how AWS started)</li> <li>ClassicLink allows you to link EC2-Classic instances to a VPC in your account</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#billing","title":"Billing","text":"<ul> <li>Reserved instances will be billed regardless of their state (billed for a reserved period)</li> <li>On-demand instances in <code>stopping</code> state when preparing to hibernate will be billed</li> <li>If an instance is running, it will be billed</li> <li>In all the other cases, an instance will not be billed</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#run-command","title":"Run Command","text":"<ul> <li>Systems Manager Run Command\u00a0lets you remotely and securely manage the configuration of your managed instances. A\u00a0managed instance\u00a0is any EC2 instance that has been configured for Systems Manager. </li> <li>Run Command enables you to automate common administrative tasks and perform ad-hoc configuration changes at scale. </li> <li>You can use Run Command from the AWS Console, the AWS CLI, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#instance-tenancy","title":"Instance Tenancy","text":"<ul> <li>Default: Instance runs on shared hardware</li> <li>Dedicated: Instance runs on single-tenant hardware</li> <li>Host: Instance runs on dedicated host</li> </ul> <ul> <li>Tenancy of an instance can only be changed from host to dedicated or dedicated to host after the instance has been launched.</li> <li>Dedicated instance tenancy takes precedence over Default instance tenancy</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Compute%20Cloud%20%28EC2%29/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>The following are a few reasons why an instance might immediately terminate:<ul> <li>You\u2019ve reached your EBS volume limit.</li> <li>An EBS snapshot is corrupt.</li> <li>The root EBS volume is encrypted and you do not have permissions to access the KMS key for decryption.</li> <li>The instance store-backed AMI that you used to launch the instance is missing a required part.</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Registry%20%28ECR%29/","title":"Elastic Container Registry (ECR)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Registry%20%28ECR%29/#elastic-container-registry-ecr","title":"Elastic Container Registry (ECR)","text":"<ul> <li>AWS managed private Docker repository</li> <li>Pay for the storage you use to store docker images (no provisioning)</li> <li>Integrated with ECS &amp; IAM for security</li> <li>Storage backed by [[Simple Storage Service (S3)|S3]]</li> <li>Can upload Docker images on ECR manually or we can use a CICD service like CodeBuild</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/","title":"Elastic Container Service (ECS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#elastic-container-service-ecs","title":"Elastic Container Service (ECS)","text":"<ul> <li>Used to launch Docker containers on AWS</li> <li>Integrates with ALB for load balancing to ECS tasks</li> <li>[[Elastic File System (EFS)|EFS]] is used as persistent multi-AZ shared storage for ECS tasks</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#launch-types","title":"Launch Types","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#ec2-launch-type","title":"EC2 Launch Type","text":"<ul> <li>Not Serverless</li> <li>Containers run on underlying EC2 instances</li> <li>ECS takes care of launching &amp; stopping containers (ECS tasks)</li> <li>You must provision &amp; maintain EC2 instances (use ASG)</li> <li>Inside a VPC spanning multiple AZ, there is an ECS cluster spanning multiple AZ. Inside the ECS cluster, there will be an ASG responsible for launching container instances (EC2). On every EC2 instance, ECS agent will be running (happens automatically if you choose the AMI for ECS when launching the instance) which registers these instances to the ECS cluster. This will allow the ECS cluster to run Docker containers (ECS tasks) on these instances.<ul> <li>![[attachments/Pasted image 20220509230845.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#fargate-launch-type","title":"Fargate Launch Type","text":"<ul> <li>Serverless</li> <li>No need to provision infrastructure</li> <li>No need to worry about infrastructure scaling</li> <li>ECS launches the required containers based on the CPU / RAM needed (we won\u2019t know where these containers are running)</li> <li>VPC and ECS cluster are setup the same way as in EC2 launch type, but instead of using [[Auto Scaling Group (ASG)|ASG]] with EC2 instances, we have a Fargate cluster spanning multiple AZ. The Fargate cluster will run ECS tasks anywhere within the cluster and attach an ENI (private IP) to each task. So, if we have a lot of ECS tasks, we need sufficient free private IPs.<ul> <li>![[attachments/Pasted image 20220509231345.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#iam-roles-for-ecs-tasks","title":"IAM Roles for ECS Tasks","text":"<ul> <li>EC2 Instance Profile (IAM role for the EC2 instance)<ul> <li>Used by the ECS agent to:<ul> <li>Make API calls to ECS service</li> <li>Send container logs to Cloud Watch</li> <li>Pull Docker image from ECR</li> </ul> </li> </ul> </li> <li>Task Execution Role<ul> <li>Allows ECS tasks to access AWS resources</li> <li>Each task can have a separate role</li> <li>Use different roles for the different ECS Services</li> <li>Task Role is defined in the task definition</li> <li>Use <code>taskRoleArn</code> parameter to assign IAM policies to ECS Task Execution Role</li> <li>Ex. Reference sensitive data in Secrets Manager or SSM Parameter Store</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#ecs-services","title":"ECS Services","text":"<ul> <li>An ECS Service is a collection of ECS tasks that perform the same function</li> <li>We can use ALB to send requests to these tasks</li> <li>Service CPU Usage or the SQS queue length for a service are used for scaling</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509232531.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#load-balancing","title":"Load Balancing","text":"<ul> <li>EC2 Launch Type<ul> <li>Dynamic port is assigned randomly to ECS tasks</li> <li>Once the ALB is registered to a service in the ECS cluster, it will find the right port on the EC2 Instances</li> <li>You must allow on the EC2 instance\u2019s security group any port from the ALB security group because it may attach on any port</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509232914.png]]</li> </ul> </li> </ul> </li> <li>Fargate Launch Type<ul> <li>Each task has a unique IP but same port (80)</li> <li>You must allow on the ENI\u2019s security group the task port (80) from the ALB security group</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509233226.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#rolling-updates","title":"Rolling Updates","text":"<ul> <li>Minimum healthy percentage - determines how many tasks, running the current version, we can terminate while staying above the threshold</li> <li>Maximum percentage - determines how many new tasks, running the new version, we can launch while staying below the threshold</li> <li>Min: 50% and Max: 100% and starting number of tasks 4<ul> <li>![[attachments/Pasted image 20220509234001.png]]</li> </ul> </li> <li>Min: 100% and Max: 150% and starting number of tasks 4<ul> <li>![[attachments/Pasted image 20220509234016.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#secrets-in-ecs-tasks","title":"Secrets in ECS tasks","text":"<ul> <li>Store the secrets in [[Secrets Manager]] or [[SSM Parameter Store|Parameter Store]] and encrypt them using KMS</li> <li>Reference the secrets in container definition with the name of the environment variable</li> <li>Create an ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store/Secrets Manager.</li> <li>Supported for both EC2 and Fargate launch types</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#scaling-ecs-tasks-using-eventbridge","title":"Scaling ECS Tasks using EventBridge","text":"<ul> <li>You can use\u00a0EventBridge (CloudWatch Events)\u00a0to run Amazon ECS tasks when certain AWS events occur. </li> <li>Ex: set up a CloudWatch Events rule that runs an Amazon ECS task whenever a file is uploaded to an S3 bucket. You can also declare a reduced number of ECS tasks whenever a file is deleted from the S3 bucket.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Container%20Service%20%28ECS%29/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<ul> <li>Verify that the Docker daemon is running on the container instance.</li> <li>Verify that the Docker Container daemon is running on the container instance.</li> <li>Verify that the container agent is running on the container instance.</li> <li>Verify that the IAM instance profile has the necessary permissions.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/","title":"Elastic File System (EFS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]] [[Elastic Compute Cloud (EC2)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/#elastic-file-system-efs","title":"Elastic File System (EFS)","text":"<ul> <li>AWS managed Network File System (NFS)</li> <li>Can be mounted to multiple EC2 instances across AZs</li> <li>Pay per use (no capacity provisioning)</li> <li>Auto scaling (up to PBs)</li> <li>Compatible with Linux based AMIs (POSIX file system)</li> <li>Uses security group to control access to EFS</li> <li>Lifecycle management feature to move files to EFS-IA after N days</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/#performance-mode","title":"Performance Mode","text":"<ul> <li>General Purpose (default)<ul> <li>latency-sensitive use cases (web server, CMS, etc.)</li> </ul> </li> <li>Max I/O<ul> <li>higher latency &amp; throughput (big data, media processing)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/#throughput-mode","title":"Throughput Mode","text":"<ul> <li>Bursting (default)<ul> <li>Throughput: 50MB/s per TB</li> <li>Burst of up to 100MB/s.</li> </ul> </li> <li>Provisioned<ul> <li>Fixed throughput (provisioned)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/#storage-tiers","title":"Storage Tiers","text":"<ul> <li>Standard - for frequently accessed files</li> <li>Infrequent access (EFS-IA) - cost to retrieve files, lower price to store</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20File%20System%20%28EFS%29/#security","title":"Security","text":"<ul> <li>EFS Security Groups to control network traffic</li> <li>POSIX Permissions to control access from hosts by IAM User or Group</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Kubernetes%20Service%20%28EKS%29/","title":"Elastic Kubernetes Service (EKS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Kubernetes%20Service%20%28EKS%29/#elastic-kubernetes-service-eks","title":"Elastic Kubernetes Service (EKS)","text":"<ul> <li>Used to launch [[Concepts#Kubernetes|Kubernetes]] (open-source) clusters on AWS</li> <li>Supports both EC2 and Fargate launch types</li> <li>Inside the EKS cluster, we have EKS nodes (EC2 instances) and EKS pods (tasks) within them. We can use a private or public load balancer to access these EKS pods.![[attachments/Pasted image 20220509235639.png]]</li> </ul> <p>CloudWatch Container Insights can be configured to view metrics and logs for an EKS cluster in the CloudWatch console</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Kubernetes%20Service%20%28EKS%29/#use-case","title":"Use case","text":"<ul> <li>If your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/","title":"Elastic Load Balancer (ELB)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#elastic-load-balancer","title":"Elastic Load Balancer","text":"<ul> <li>Regional Service</li> <li>Supports Multi AZ</li> <li>Spread load across multiple EC2 instances</li> <li>Separate public traffic from private traffic</li> <li>Health checks allow ELB to know which instances are working properly (done on a port and a route, <code>/health</code> is common)</li> <li>Does not support weighted routing</li> </ul> <p>If no targets are associated with the target groups =&gt; 503 Service Unavailable</p> <p>Using ALB &amp; NLB, instances in peered VPCs can be used as targets using IP addresses.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#types","title":"Types","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#classic-load-balancer-clb-deprecated","title":"Classic Load Balancer (CLB) - deprecated","text":"<ul> <li>Load Balancing to a single application</li> <li>Supports HTTP, HTTPS (layer 7) &amp; TCP, SSL (layer 3)</li> <li>Health checks are HTTP or TCP based</li> <li>Provides a fixed hostname (xxx.region.elb.amazonaws.com)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#application-load-balancer-alb","title":"Application Load Balancer (ALB)","text":"<ul> <li>Load balancing to multiple applications (target groups)</li> <li>Operates at Layer 7 (HTTP, HTTPS and WebSocket)</li> <li>Provides a fixed hostname (xxx.region.elb.amazonaws.com)</li> <li>ALB terminates the upstream connection and creates a new downstream connection to the targets (connection termination)</li> <li>Security Groups can be attached to ALBs to filters requests</li> <li>Great for micro services &amp; container-based applications (Docker &amp; ECS)</li> <li>Client info is passed in the request headers<ul> <li>Client IP =&gt; <code>X-Forwarded-For</code></li> <li>Client Port =&gt; <code>X-Forwarded-Port</code></li> <li>Protocol =&gt; <code>X-Forwarded-Proto</code></li> </ul> </li> <li>Target Groups<ul> <li>Health checks are done at the target group level</li> <li>Target Groups could be<ul> <li>EC2 instances - HTTP</li> <li>ECS tasks - HTTP</li> <li>Lambda functions - HTTP request is translated into a JSON event</li> <li>Private IP Addresses</li> </ul> </li> </ul> </li> <li>Listener Rules can be configured to route traffic to different target groups based on:<ul> <li>Path (example.com/users &amp; example.com/posts)</li> <li>Hostname (one.example.com &amp; other.example.com)</li> <li>Query String (example.com/users?id=123&amp;order=false)</li> <li>Request Headers</li> <li>Source IP address</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#network-load-balancer-nlb","title":"Network Load Balancer (NLB)","text":"<ul> <li>Operates at Layer 4 (TCP, TLS, UDP)</li> <li>Can handle millions of request per seconds (extreme performance)</li> <li>Lower latency ~ 100 ms (vs 400 ms for ALB)</li> <li>1 static public IP per AZ (vs a static hostname for CLB &amp; ALB)</li> <li>Elastic IP can be assigned to NLB (helpful for whitelisting specific IP)</li> <li>Maintains the same connection from client all the way to the target</li> <li>No security groups can be attached to NLBs. They just forward the incoming traffic to the right target group as if those requests were directly coming from client. So, the attached instances must allow TCP traffic on port 80 from anywhere.</li> <li>Within a target group, NLB can send traffic to<ul> <li>EC2 instances<ul> <li>If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address</li> </ul> </li> <li>IP addresses<ul> <li>Used when you want to balance load for a physical server having a static IP.</li> </ul> </li> <li>Application Load Balancer (ALB)<ul> <li>Used when you want a static IP provided by an NLB but also want to use the features provided by ALB at the application layer.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#gateway-load-balancer-gwlb","title":"Gateway Load Balancer (GWLB)","text":"<ul> <li>Operates at layer 3 (Network layer) - IP Protocol</li> <li>Used to route requests to a fleet of 3rd party virtual appliances like Firewalls, Intrusion Detection and Prevention Systems (IDPS), etc.</li> <li>Performs two functions:<ul> <li>Transparent Network Gateway (single entry/exit for all traffic)</li> <li>Load Balancer (distributes traffic to virtual appliances)</li> </ul> </li> <li>Uses GENEVE protocol</li> <li>Target groups for GWLB could be<ul> <li>EC2 instances</li> <li>IP addresses</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#sticky-sessions-session-affinity","title":"Sticky Sessions (Session Affinity)","text":"<ul> <li>Requests coming from a client is always redirected to the same instance based on a cookie. After the cookie expires, the requests coming from the same user might be redirected to another instance.</li> <li>Only supported by CLB &amp; ALB</li> <li>Used to ensure the user doesn\u2019t lose his session data, like login or cart info, while navigating between web pages.</li> <li>Stickiness may cause load imbalance</li> <li>Cookies could be<ul> <li>Application-based (TTL defined by the application)</li> <li>Load Balancer generated (TTL defined by the load balancer)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#cross-zone-load-balancing","title":"Cross-zone Load Balancing","text":"<ul> <li>Allows ELBs in different AZ containing unbalanced number of instances to distribute the traffic evenly across all instances in all the AZ registered under a load balancer.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220506220851.png]]</li> </ul> </li> <li>Supported Load Balancers<ul> <li>Classic Load Balancer<ul> <li>Disabled by default</li> <li>No charges for inter AZ data</li> </ul> </li> <li>Application Load Balancer<ul> <li>Always on (can\u2019t be disabled)</li> <li>No charges for inter AZ data</li> </ul> </li> <li>Network Load Balancer<ul> <li>Disabled by default</li> <li>Charges for inter AZ data</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#in-flight-encryption","title":"In-flight Encryption","text":"<ul> <li>Use an NLB with a TCP listener &amp; terminate SSL on EC2 instances</li> <li>Use an ALB with an HTTPS listener, install SSL certificates on the ALB &amp; terminate SSL on the ALB</li> <li>Communication between ALB &amp; EC2 instances can happen over HTTP inside the VPC</li> <li>Server Name Indication (SNI)<ul> <li>SNI allows us to load multiple SSL certificates on one Load Balancer to serve multiple websites securely<ul> <li>![[attachments/Pasted image 20220506222424.png]]</li> </ul> </li> <li>Only works for ALB &amp; NLB (CLB only supports one SSL certificate)</li> <li>Newer protocol, not every client supports it yet</li> <li>Supported in CloudFront also</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#connection-draining-de-registration-delay","title":"Connection Draining (De-registration Delay)","text":"<ul> <li>When an instance is to be de-registered from the ELB, the in-flight requests being served by that instance are given some pre-defined time to complete before the ELB de-registers it.</li> <li>ELB stops sending new requests to the EC2 instance which is de-registering</li> <li>Set manually (0 to 3600 seconds) (default: 300 seconds)</li> </ul> <p>For instances behind an ELB and using ASG, increase the de-registration delay to ensure that the in-flight requests are completed before the ELB deregisters an instance which is to be terminated by the ASG.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#access-logs","title":"Access Logs","text":"<ul> <li>Captures detailed information about requests sent to the load balancer</li> <li>Used to analyze traffic patterns and troubleshoot issues</li> <li>Disabled by default</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Load%20Balancer%20%28ELB%29/#misc","title":"Misc","text":"<ul> <li>Security Groups for a public facing ELB<ul> <li>ELB will be publicly available on the internet, so it\u2019s security group should allow HTTP and HTTPS traffic from anywhere. EC2 should only allow traffic from the ELB, so the its security group should allow HTTP requests from ELB\u2019s security group.</li> <li>Image<ul> <li>![[attachments/Pasted image 20220506205310.png]] </li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Map%20Reduce%20%28EMR%29/","title":"Elastic Map Reduce (EMR)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Elastic%20Map%20Reduce%20%28EMR%29/#elastic-map-reduce-emr","title":"Elastic Map Reduce (EMR)","text":"<ul> <li>Used to create Big Data clusters to analyze and process vast amounts of data</li> <li>Uses Hadoop, an open-source framework, to distribute your data and processing across a cluster of 100s of EC2 instances.</li> <li>Supports open-source tools such as Apache Spark, HBase, Presto, Flink, etc.</li> <li>EMR takes care of all the provisioning and configuration</li> <li>Auto-scaling</li> <li>Integrated with Spot Instances</li> <li>Can be used to process large amounts of log files</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElasticSearch/","title":"ElasticSearch","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/ElasticSearch/#elasticsearch","title":"ElasticSearch","text":"<ul> <li>Used in combination with a database to perform search operations on the database</li> <li>Can search on any field, even supports partial matches</li> <li>Need to provision a cluster of instances (pay for provisioned instances)</li> <li>Supports Multi-AZ</li> <li>Used in Big Data</li> <li>Comes with Kibana (visualization) &amp; Logstash (log ingestion) - ELK stack</li> <li>Integrated with [[Cognito]] for access control</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/EventBridge/","title":"EventBridge","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/EventBridge/#eventbridge","title":"EventBridge","text":"<ul> <li>Extension of [[CloudWatch#Events]]</li> <li>Event buses types:<ul> <li>Default event bus: events from AWS services are sent to this</li> <li>Partner event bus: receive events from external SaaS applications</li> <li>Custom Event bus: for your own applications</li> </ul> </li> <li>Event Rules: how to process the events</li> <li>Event buses support cross-account access</li> <li>Cron Jobs: when creating an EB rule, we can select \u201cSchedule\u201d instead of event pattern to trigger an event based on a cron expression.</li> </ul> <p>EventBridge is recommended for decoupling applications that reacts to events from third-party SaaS applications.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/EventBridge/#schema-registry","title":"Schema Registry","text":"<ul> <li>Defines how the data is structured in the event bus</li> <li>Schema can be versioned</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Extras/","title":"Extras","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Extras/#extras","title":"Extras","text":"<ul> <li>If you got your certificate from a third-party certificate authority (CA), import the certificate into AWS Certificate Manager or upload it to the IAM Certificate Store.</li> <li>AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for 8 services.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/FSx/","title":"FSx","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/FSx/#fsx","title":"FSx","text":"<ul> <li>Allows us to launch 3rd party high-performance file systems on AWS</li> <li>Useful when we don\u2019t want to use an AWS managed file system like S3</li> <li>Can be accessed from your on-premise infrastructure</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/FSx/#fsx-for-windows","title":"FSx for Windows","text":"<ul> <li>Shared File System for Windows (like EFS but for Windows)</li> <li>Supports SMB protocol, Windows NTFS, Microsoft Active Directory integration, ACLs, user quotas</li> <li>Built on SSD, scale up to 10s of GB/s, millions of IOPS, 100s PB of data</li> <li>Supports Multi-AZ (high availability)</li> <li>Data is backed-up daily to S3</li> <li>Does not integrate with S3 (cannot store cold data)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/FSx/#fsx-for-lustre","title":"FSx for Lustre","text":"<ul> <li>Parallel distributed file system for HPC (like EFS but for HPC)</li> <li>Scales up to 100s GB/s, millions of IOPS, sub-ms latencies</li> <li>Only works with Linux</li> <li>Seamless integration with S3<ul> <li>Can read S3 buckets as a file system (through FSx)</li> <li>Can write the output back to S3 (through FSx)</li> </ul> </li> <li>Ability to both process the hot data in a parallel and distributed fashion as well as easily store the cold data on Amazon S3</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/FSx/#fsx-deployment-options","title":"FSx Deployment Options","text":"<ul> <li>Scratch File System<ul> <li>Temporary storage (cheaper)</li> <li>Data is not replicated (data lost if the file server fails)</li> <li>High burst (6x faster than persistent file system)</li> <li>Usage: short-term processing </li> </ul> </li> <li>Persistent File System<ul> <li>Long-term storage (expensive)</li> <li>Data is replicated within same AZ</li> <li>Failed files are replaced within minutes</li> <li>Usage: long-term processing, sensitive data</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Firewall%20Manager/","title":"Firewall Manager","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Firewall%20Manager/#firewall-manager","title":"Firewall Manager","text":"<ul> <li>Manage firewall across all the accounts of an [[AWS Organizations|AWS Organization]]<ul> <li>[[Web Application Firewall (WAF)]]</li> <li>[[AWS Shield]]</li> <li>[[Elastic Compute Cloud (EC2)#Security Groups|Security Groups]]</li> </ul> </li> </ul> <p>Does not support NACL as of now</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Global%20Accelerator/","title":"Global Accelerator","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Global%20Accelerator/#global-accelerator","title":"Global Accelerator","text":"<ul> <li>Global service</li> <li>Improves availability of the application for global users</li> <li>Leverages the private AWS network to route requests to the application (faster)</li> <li>Supports globally distributed application endpoints</li> <li>Does not cache anything at the edge location</li> <li>Endpoint could be public or private (could span multiple regions)<ul> <li>Elastic IP</li> <li>EC2 instances</li> <li>ALB</li> <li>NLB</li> </ul> </li> <li>Endpoint Weights and Traffic Dials are used in [[Blue-Green Deployment]]</li> <li>Not affected by client's DNS caching because the 2 anycast IPs are static (traffic dials and endpoint weights changes are effective within seconds)</li> <li>Good for <ul> <li>non-HTTP use cases:<ul> <li>Gaming (UDP)</li> <li>IoT (MQTT)</li> <li>Voice over IP (VoIP)</li> </ul> </li> <li>HTTP use cases that require static IP addresses or fast regional failover</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Global%20Accelerator/#working","title":"Working","text":"<ul> <li>2 [[Concepts#Unicast vs Anycast IP|anycast]] public IPs (static) are created for your application globally. Requests from clients hitting these IPs will automatically be routed to the nearest edge location. The Edge locations send the traffic to your application through the private AWS network. </li> <li>Traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed)</li> <li>Endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Global%20Accelerator/#disaster-recovery","title":"Disaster Recovery","text":"<ul> <li>Global Accelerator performs health checks for the application</li> <li>Failover in less than 1 minute for unhealthy endpoints</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Global%20Accelerator/#security","title":"Security","text":"<ul> <li>Only 2 static IP need to be whitelisted by the clients</li> <li>Can be integrated with AWS Shield for DDoS protection</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Glue/","title":"Glue","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Glue/#glue","title":"Glue","text":"<ul> <li>Serverless Extract, Transform &amp; Load (ETL) service</li> <li>Used to prepare &amp; transform data for analytics</li> <li>Used to get data from a store, process and put it in another store (could be the same store)</li> </ul> <p>Glue ETL job can write the transformed data using a compressed file format to save storage cost</p> <p>Using Glue involves significant development efforts to write custom migration scripts to copy the database data into the target database.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Glue/#-attachmentspasted-image-20220514111221png","title":"- ![[attachments/Pasted image 20220514111221.png]]","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Glue/#glue-data-catalog","title":"Glue Data Catalog","text":"<ul> <li>Glue Data Crawlers crawl databases and collect metadata which is populated in Glue Data Catalog</li> <li>Metadata can be used for analytics</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/GuardDuty/","title":"GuardDuty","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/GuardDuty/#guardduty","title":"GuardDuty","text":"<ul> <li>Intelligent threat discovery using ML to protect AWS account</li> <li>No management required (just enable)</li> <li>Monitors:<ul> <li>CloudTrail Logs (unusual API calls, unauthorized deployments)</li> <li>VPC Flow Logs (unusual internal traffic, unusual IP address)</li> <li>DNS Logs (compromised EC2 instances sending encoded data within DNS queries)</li> <li>EKS Audit Logs (suspicious activities and potential EKS cluster compromises)</li> </ul> </li> <li>Setup CloudWatch Event rules to target AWS Lambda or SNS for automation</li> </ul> <p>Features a dedicated finding for Crypto attacks</p> <p>Disabling GuardDuty will delete all remaining data, including your findings and configurations</p> <p>![[attachments/Pasted image 20220511232409.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/High%20Performance%20Computing%20%28HPC%29/","title":"High Performance Computing (HPC)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/High%20Performance%20Computing%20%28HPC%29/#high-performance-computing-hpc","title":"High Performance Computing (HPC)","text":"<ul> <li>Cloud is perfect for HPC</li> <li>Cluster placement group for low latency inter-nodal communication</li> <li>EC2 Enhanced Networking (SR-IOV)<ul> <li>Elastic Network Adapter (ENA)<ul> <li>Supported in both Linux &amp; Windows</li> </ul> </li> <li>Elastic Fabric Adapter (EFA)<ul> <li>Enhanced for HPC</li> <li>Supported in Linux only</li> <li>Leverages Message Passing Interface (MPI) standard</li> <li>Bypasses the underlying Linux OS to provide low-latency networking</li> </ul> </li> </ul> </li> <li>AWS Batch<ul> <li>Used to run single jobs that span multiple EC2 instances (multi-node)</li> </ul> </li> <li>AWS Parallel Cluster<ul> <li>Open-source cluster management tool to deploy HPC on AWS</li> <li>Configure with text files</li> <li>Automate creation of VPC, Subnet, cluster type and instance types</li> <li>Ability to enable EFA on the cluster</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/","title":"Identity & Access Management (IAM)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#identity-access-management-iam","title":"Identity &amp; Access Management (IAM)","text":"<ul> <li>Global Service (IAM entities like roles can be used in any region without recreation)</li> </ul> <p>IAM Query API can be used to make direct calls to the IAM web service (using access key ID and secret access key for authentication)</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#users-groups","title":"Users &amp; Groups","text":"<ul> <li>Groups are collections of users and have policies attached to them</li> <li>Groups cannot be nested</li> <li>User can belong to multiple groups</li> <li>User doesn't have to belong to a group</li> <li>Root User has full access to the account </li> <li>IAM User has limited permission to the account</li> <li>You should log in as an IAM user with admin access even if you have root access. This is just to be sure that nothing goes wrong by accident.</li> </ul> <ul> <li>An IAM Group is not an identity and cannot be identified as a principal in an IAM policy</li> <li>Only users and services can assume a role (not groups)</li> <li>A new IAM user created using the AWS CLI or AWS API has no AWS credentials</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#policies","title":"Policies","text":"<ul> <li>Policies are JSON documents that outline permissions for users, groups or roles</li> <li>Two types<ul> <li>User based policies<ul> <li>IAM policies define which API calls should be allowed for a specific user</li> </ul> </li> <li>Resource based policies<ul> <li>Control access to an AWS resource</li> <li>Grant the specified principal permission to perform actions on the resource and define under what conditions this applies</li> </ul> </li> </ul> </li> <li>An IAM principal can access a resource if the user policy ALLOWS it OR the resource policy ALLOWS it AND there\u2019s no explicit DENY. </li> <li>Policies assigned to a user are called inline policies</li> <li>Follow least privilege principle for IAM Policies</li> <li>Policy Structure<ul> <li>![[attachments/Pasted image 20220505202904.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#trust-policies","title":"Trust Policies","text":"<ul> <li>Defines which principal entities (accounts, users, roles, federated users) can assume the role </li> <li>An IAM role is both an identity and a resource that supports resource-based policies. </li> <li>You must attach both a trust policy and an identity-based policy to an IAM role. </li> <li>The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#roles","title":"Roles","text":"<ul> <li>Collection of policies for AWS services</li> </ul> <p>If you are going to use an IAM Service Role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. When you create an IAM service role for EC2, the role automatically has EC2 identified as a trusted entity.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#protect-iam-accounts","title":"Protect IAM Accounts","text":"<ul> <li>Password Policy<ul> <li>Used to enforce standards for password<ul> <li>password rotation</li> <li>password reuse</li> </ul> </li> <li>Prevents brute force attack</li> </ul> </li> <li>Multi Factor Authentication (MFA)<ul> <li>Both root user and IAM users should use MFA</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#reporting-tools","title":"Reporting Tools","text":"<ul> <li>Credentials Report<ul> <li>lists all the users and the status of their credentials (MFA, password rotation, etc.)</li> <li>account level - used to audit security for all the users</li> </ul> </li> <li>Access Advisor<ul> <li>shows the service permissions granted to a user and when those services were last accessed</li> <li>user-level</li> <li>used to revise policies for a specific user</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#access-keys","title":"Access Keys","text":"<ul> <li>Need to use access keys for AWS CLI and SDK</li> <li>Don't share access keys with anyone (every user can generate their own access keys)</li> <li>Access keys are only shown once and if you lose them you need to generate a new access key</li> <li>Access Key ID ~ username</li> <li>Secret Access Key ~ password</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#guidelines","title":"Guidelines","text":"<ul> <li>Use root account only for account setup</li> <li>1 physical user = 1 IAM user</li> <li>Enforce MFA for both root and IAM users</li> <li>Never share lAM credentials &amp; Access Keys</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#policy-simulator","title":"Policy Simulator","text":"<ul> <li>Online tool that allows us to check what API calls an IAM User, Group or Role is allowed to perform based on the permissions they have.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#permission-boundaries","title":"Permission Boundaries","text":"<ul> <li>Set the maximum permissions an IAM entity can get</li> <li>Can be applied to users and roles (not groups)</li> <li>Used to ensure some users can\u2019t escalate their privileges (make themselves admin)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20%26%20Access%20Management%20%28IAM%29/#assume-role-vs-resource-based-policy","title":"Assume Role vs Resource-based Policy","text":"<ul> <li>When you assume an IAM Role, you give up your original permissions and take the permissions assigned to the role</li> <li>When using a resource based policy, the principal doesn\u2019t have to give up their permissions</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20Federation%20in%20AWS/","title":"Identity Federation in AWS","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20Federation%20in%20AWS/#identity-federation-in-aws","title":"Identity Federation in AWS","text":"<ul> <li>Federation lets users outside of AWS to assume temporary role for accessing AWS resources using a third-party identity provider</li> <li>Need to configure IAM roles with the required policies that users will assume</li> <li>No need to create lAM users (user management by the third-party identity provider)</li> <li>Need to setup a trust between identity provider and IAM</li> <li>Flavors<ul> <li>SAML 2.0</li> <li>Custom Identity Broker</li> <li>Web Identity Federation with Amazon Cognito</li> <li>Web Identity Federation without Amazon Cognito (not recommended)</li> <li>Single Sign On</li> <li>Non-SAML with AWS Microsoft AD</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20Federation%20in%20AWS/#saml-20-federation","title":"SAML 2.0 Federation","text":"<ul> <li>Used to integrate Active Directory / ADFS with AWS using SAML compatible IDP</li> <li>Client exchanges SAML assertion for security credentials from STS using the STS <code>AssumeRoleWithSAML</code> API</li> <li>Flow diagram<ul> <li>![[attachments/Pasted image 20220511103954.png]]</li> </ul> </li> </ul> <p>SSO can be implemented using an existing IDP like AD using SAML 2.0 Federation</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20Federation%20in%20AWS/#custom-identity-broker-federation","title":"Custom Identity Broker Federation","text":"<ul> <li>Used when the identity provider is not compatible with SAML 2.0 or OIDC</li> <li>Identity broker gets security credentials from STS using the STS <code>AssumeRole</code> or <code>GetFederationToken</code> API</li> <li>Flow diagram<ul> <li>![[attachments/Pasted image 20220511122254.png]]</li> </ul> </li> </ul> <p>[!info]- Steps in Custom Identity Broker Federation -   Verify that the user is authenticated by your local IDP (could be AD) -   Call the STS AssumeRole or GetFederationToken API to obtain temporary security credentials for the user -   Call the AWS Federation Endpoint and supply the temporary security credentials to request a sign-in token</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Identity%20Federation%20in%20AWS/#web-identity-federation-with-cognito","title":"Web Identity Federation with Cognito","text":"<ul> <li>Use for OpenID Connect (OIDC) compatible IDP like CUP, FaceBook, Google etc.</li> <li>Example: provide temporary access to write to S3 bucket using Facebook Login</li> <li>Steps<ul> <li>Log in to federated identity provider to get JWT</li> <li>Use the JWT to authenticate to Federated Identity Pool</li> <li>Get temporary AWS credentials back from the Federated Identity Pool</li> </ul> </li> <li>Flow diagram<ul> <li>![[attachments/Pasted image 20220511124149.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Important%20Ports/","title":"Important Ports","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Important%20Ports/#important-ports","title":"Important Ports","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Important%20Ports/#networking-ports","title":"Networking Ports","text":"<ul> <li>FTP:\u00a021</li> <li>SSH: 22</li> <li>SFTP: 22 (same as SSH) - used for uploading files using SSH</li> <li>HTTP:\u00a080</li> <li>HTTPS:\u00a0443</li> <li>RDP (Remote Desktop Protocol): 3389</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Important%20Ports/#database-ports","title":"Database Ports","text":"<ul> <li>PostgreSQL:\u00a05432</li> <li>MySQL:\u00a03306</li> <li>Oracle RDS: 1521</li> <li>MSSQL Server:\u00a01433</li> <li>MariaDB:\u00a03306 (same as MySQL)</li> <li>Aurora:\u00a05432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Inspector/","title":"Inspector","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Inspector/#inspector","title":"Inspector","text":"<ul> <li>Detect vulnerabilities in<ul> <li>EC2 instances using System Manager (SSM) Agent running on EC2 instances</li> <li>Amazon ECR - Assessment of containers as they are pushed to ECR</li> </ul> </li> <li>Integration with AWS Security Hub</li> <li>Send findings to [[EventBridge]]</li> <li>Gives a risk score associated with all vulnerabilities for prioritization</li> <li>Detects vulnerabilities which could cause threats (detected by [[GuardDuty]])</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Instance%20Store/","title":"Instance Store","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]] [[Elastic Compute Cloud (EC2)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Instance%20Store/#instance-store","title":"Instance Store","text":"<ul> <li>Hardware storage directly attached to EC2 instance (cannot be detached and attached to another instance)</li> <li>Highest IOPS of any available storage (millions of IOPS)</li> <li>Ephemeral storage (loses data when the instance is stopped, hibernated or terminated)</li> <li>Good for buffer / cache / scratch data / temporary content</li> <li>AMI created from an instance does not have its instance store volume preserved</li> </ul> <p>You can specify the instance store volumes only when you launch an instance. You can\u2019t attach instance store volumes to an instance after you\u2019ve launched it.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Instance%20Store/#raid","title":"RAID","text":"<ul> <li>RAID 0<ul> <li>Improve performance of a storage volume by distributing reads &amp; writes in a stripe across attached volumes</li> <li>If you add a storage volume, you get the straight addition of throughput and IOPS</li> <li>For high performance applications</li> </ul> </li> <li>RAID 1<ul> <li>Improve data availability by mirroring data in multiple volumes</li> <li>For critical applications</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Intro%20to%20AWS/","title":"Intro to AWS","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Intro%20to%20AWS/#intro-to-aws","title":"Intro to AWS","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Intro%20to%20AWS/#region","title":"Region","text":"<ul> <li>Cluster of Availability Zones (usually 3, min 2, max 6) within a geographic region</li> <li>Ex: <code>us-east-1</code>, <code>ap-south-1</code></li> <li>Consideration when selecting a region<ul> <li>Compliance (some countries require the data to be present in a data centre present in that country by law)</li> <li>Proximity to customers: reduced latency</li> <li>Available services within a Region: new services and new features aren\u2019t available in every Region</li> <li>Pricing: pricing varies region to region and is transparent in the service pricing page</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Intro%20to%20AWS/#availability-zone-az","title":"Availability Zone (AZ)","text":"<ul> <li>Each AZ is one or more discrete data centers with redundant power, networking, and connectivity</li> <li>Ex: <code>us-east-1a</code>, <code>us-east-1b</code> &amp; <code>us-east-1c</code></li> <li>AZs are separated from each other (isolated from disasters)</li> <li>They\u2019re connected with high bandwidth, ultra-low latency networking</li> </ul> <p>AZ name (eg. us-east-1a) is linked to an AWS account. Same AZ name for two AWS accounts might not refer to the same physical AZ. Use AZ ID (unique ID for each AZ) to coordinate AZ across accounts.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/","title":"Key Management Service (KMS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#key-management-service-kms","title":"Key Management Service (KMS)","text":"<ul> <li>Regional service (keys are bound to a region)</li> <li>Provides encryption and decryption of data and manages keys required for it</li> <li>Encrypted secrets can be stored in the code or environment variables</li> <li>Encrypt up to 4KB of data per call (if data &gt; 4 KB, use envelope encryption)</li> <li>Integrated with lAM for authorization</li> <li>Audit key usage with CloudTrail</li> <li>Need to set IAM Policy &amp; Key Policy to allow a user or role to access a KMS key</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#customer-master-key-cmk","title":"Customer Master Key (CMK)","text":"<ul> <li>Symmetric keys (necessary for envelope encryption)</li> <li>Must call KMS API to encrypt data</li> <li>Two types:<ul> <li>AWS Owned CMK (free)<ul> <li>Default KMS key for each supported service</li> <li>Fully managed by AWS (cannot view, rotate or delete them)</li> </ul> </li> <li>Customer Owned CMK (1$/month)<ul> <li>AWS Managed CMK<ul> <li>Generated in KMS</li> <li>Option to enable automatic yearly rotation</li> </ul> </li> <li>Customer Managed CMK<ul> <li>Generated and imported from outside</li> <li>Must be 256-bit symmetric key</li> <li>Not recommended</li> </ul> </li> <li>Deletion has a waiting period (pending deletion state) between 7 - 30 days (default 30 days). The key can be recovered during the pending deletion state.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#asymmetric-keys","title":"Asymmetric Keys","text":"<ul> <li>Public (Encrypt) and Private Key (Decrypt) pair</li> <li>Used for Encrypt/Decrypt, or Sign/Verify operations</li> <li>The public key is downloadable, but you can\u2019t access the Private Key unencrypted</li> <li>No need to call the KMS API to encrypt data (data can be encrypted by the client)</li> <li>Not eligible for automatic rotation (use manual rotation)</li> <li>Use case: encryption outside of AWS by users who can\u2019t call the KMS API</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#key-policies","title":"Key Policies","text":"<ul> <li>Cannot access KMS keys without a key policy</li> <li>Default Key Policy<ul> <li>Created if you don\u2019t provide a specific Key Policy</li> <li>Complete access to the key for the root user \u21d2 any user or role can access the key (most permissible)</li> </ul> </li> <li>Custom Key Policy<ul> <li>Define users, roles that can access the KMS key</li> <li>Define who can administer the key</li> <li>Useful for cross-account access of your KMS key</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#cross-region-encrypted-snapshot-migration","title":"Cross-region Encrypted Snapshot Migration","text":"<ul> <li>Copy the snapshot to another region with re-encryption option using a new key in the new region (keys are bound to a region)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#cross-account-encrypted-snapshot-migration","title":"Cross-account Encrypted Snapshot Migration","text":"<ul> <li>Attach a Key Policy to the main CMK to authorize access to an IAM role in the target account (cross-account access)<ul> <li>![[attachments/Pasted image 20220511214328.png]]</li> </ul> </li> <li>Share the encrypted snapshot with the new account</li> <li>In the target account, create a copy of the snapshot (decryption will use the main CMK)</li> <li>Encrypt it with a new KMS Key in your account</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Key%20Management%20Service%20%28KMS%29/#key-rotation","title":"Key Rotation","text":"<ul> <li>Automatic<ul> <li>Supported only in Customer-owned CMK</li> <li>Automatic yearly key rotation</li> <li>Previous key is kept active (to decrypt old data)</li> <li>New key has the same CMK ID (only the backing key is changed)</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220511215802.png]]</li> </ul> </li> </ul> </li> <li>Manual<ul> <li>New Key has a different CMK ID</li> <li>Keep the previous key active to decrypt old data</li> <li>Use aliases as CMK id changes after rotation (to hide the key change for the application). After rotation, use UpdateAlias API to point the alias to the new key.</li> <li>Good for asymmetric keys (automatic rotation not supported)</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220511215742.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Kinesis/","title":"Kinesis","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Kinesis/#kinesis","title":"Kinesis","text":"<p>Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Kinesis/#kinesis-data-stream-kds","title":"Kinesis Data Stream (KDS)","text":"<ul> <li>Real-time data streaming service</li> <li>Used to ingest data in real time directly from source</li> <li>Throughput<ul> <li>Publishing: 1MB/sec per shard or 1000 msg/sec per shard</li> <li>Consuming: <ul> <li>2MB/sec per shard (throughput shared between all consumers)</li> <li>Enhanced Fanout: 2MB/sec per shard per consumer (dedicated throughput for each consumer)</li> </ul> </li> </ul> </li> <li>Throughput scales with shards (manual scaling)</li> <li>Not Serverless</li> <li>Billing per shard (provisioned)</li> <li>Data Retention: 1 day (default) to 365 days</li> <li>A record consists of a partition key (used to partition data coming from multiple publishers) and data blob (max 1MB)</li> <li>Records will be ordered in each shard</li> <li>Producers use SDK, Kinesis Producer Library (KPL) or Kinesis Agent to publish records</li> <li>Consumers use SDK or Kinesis Client Library (KCL) to consume the records</li> <li>Once data is inserted in Kinesis, it can\u2019t be modified or deleted (immutability)</li> <li>Ability to reprocess (replay) data</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509221100.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Kinesis/#kinesis-data-firehose-kdf","title":"Kinesis Data Firehose (KDF)","text":"<ul> <li>Used to load streaming data into a target location</li> <li>Writes data in batches efficiently (near real time)</li> <li>Can ingest data in real time directly from source</li> <li>Greater the batch size, higher the write efficiency</li> <li>Auto-scaling</li> <li>Serverless</li> <li>Destinations:<ul> <li>AWS: Redshift, S3, ElasticSearch</li> <li>3rd party: Splunk, MongoDB, DataDog, NewRelic, etc.</li> <li>Custom: send to any HTTP endpoint</li> </ul> </li> <li>Pay for data going through Firehose (no provisioning)</li> <li>Supports custom data transformation using Lambda functions</li> <li>Max record size: 1MB</li> <li>No replay capability</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509221812.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Kinesis/#kinesis-data-analytics-kda","title":"Kinesis Data Analytics (KDA)","text":"<ul> <li>Perform real-time analytics on Kinesis streams using SQL</li> <li>Creates streams from SQL query response</li> <li>Cannot ingest data directly from source (ingests data from KDS or KDF)</li> <li>Auto-scaling</li> <li>Serverless</li> <li>Pay for the data processed (no provisioning)</li> <li>Use cases:<ul> <li>Time-series analytics</li> <li>Real-time dashboards</li> <li>Real-time metrics</li> </ul> </li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509222350.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/","title":"Lambda","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#lambda","title":"Lambda","text":"<ul> <li>Function as a  Service (FaaS)</li> <li>Serverless</li> <li>Auto-scaling</li> <li>Pay per request (number of invocations) and compute time</li> <li>Integrated with CloudWatch for monitoring</li> <li>Not good for running containerized applications</li> <li>Can package and deploy Lambda functions as container images</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#performance","title":"Performance","text":"<ul> <li>Increasing RAM will improve CPU and network</li> <li>RAM: 128 MB - 10GB</li> <li>Max execution time: 15 mins</li> <li>Disk capacity in function container (<code>/tmp</code>): 512 MB</li> <li>Environment variables: 4 KB</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#deployment","title":"Deployment","text":"<ul> <li>Deployment size<ul> <li>Compressed: 50MB</li> <li>Uncompressed: 250 MB</li> </ul> </li> <li>If you intend to reuse code in more than one Lambda function, consider creating a Lambda Layer (a ZIP archive that contains libraries) for the reusable code. With layers, you can use libraries in your function without including them in the deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#networking","title":"Networking","text":"<ul> <li>By default, Lambda functions operate from an AWS-owned VPC and hence have access to any public internet or public AWS API (ex. lambda functions can interact with AWS DynamoDB APIs to PutItem)</li> <li>Once a Lambda function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, it will need a [[Virtual Private Cloud (VPC)#NAT Gateway|NAT Gateway]]. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet (ex. RDS database)</li> </ul> <p>To enable your Lambda function to access resources inside your private VPC, you must provide VPC subnet IDs and Security Group IDs. AWS Lambda uses this information to set up ENIs.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#supported-languages","title":"Supported Languages","text":"<ul> <li>Node.js (JavaScript)</li> <li>Python</li> <li>Java</li> <li>C#</li> <li>Golang</li> <li>Ruby</li> <li>Any other language using Custom Runtime API (community supported, example Rust)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#use-cases","title":"Use cases","text":"<ul> <li>Serverless thumbnail creation using S3 &amp; Lambda</li> <li>Serverless CRON job using EventBridge &amp; Lambda</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Lambda/#lambdaedge","title":"Lambda@Edge","text":"<ul> <li>Deploy Lambda functions alongside your [[CloudFront]] CDN for computing at edge locations</li> <li>Customize the CDN content using Lambda at the edge location (responsive)</li> <li>No server management (Lambda is deployed globally)</li> <li>Pay for what you use (no provisioning)</li> <li>Can be used to modify CloudFront requests &amp; responses<ul> <li>![[attachments/Pasted image 20220510194136.png]]</li> </ul> </li> <li>We can create a global application using Lambda@Edge where S3 hosts a static website which uses client side JS to send requests to CF which will process the request in a lambda function in that edge location to perform some operation like fetching data from DynamoDB.<ul> <li>![[attachments/Pasted image 20220510194430.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Macie/","title":"Macie","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Macie/#macie","title":"Macie","text":"<ul> <li>Amazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover and protect your sensitive data in AWS, such as Personally Identifiable Information (PII) in an [[Simple Storage Service (S3)|S3]] bucket.</li> <li>No management required (just enable)</li> <li>Notifies through an [[EventBridge]] event</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Neptune/","title":"Neptune","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Neptune/#neptune","title":"Neptune","text":"<ul> <li>AWS managed graph database</li> <li>Used for high relationship data (eg. social networking)</li> <li>Highly available across 3 AZ with up to 15 read replicas</li> <li>Point-in-time recovery due to continuous backup to S3</li> <li>Support for KMS encryption at rest + HTTPS for in-flight encryption</li> <li>Need to provision nodes in advance (pay for the provisioned nodes)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Networking%20Costs/","title":"Networking Costs","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Networking%20Costs/#networking-costs","title":"Networking Costs","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Networking%20Costs/#inter-az-and-inter-region-networking","title":"Inter-AZ and Inter-Region Networking","text":"<ul> <li>Traffic entering the AWS is free</li> <li>Use Private IP instead of Public IP for good savings and better network performance</li> <li>Traffic leaving an AWS region is paid</li> <li>Use same AZ for maximum savings (at the cost of availability) ![[attachments/Pasted image 20220513100858.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Networking%20Costs/#minimizing-egress-traffic","title":"Minimizing Egress Traffic","text":"<ul> <li>Try to keep as much internet traffic within AWS to minimize costs</li> <li>Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network ![[attachments/Pasted image 20220513101339.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Networking%20Costs/#s3-data-transfer","title":"S3 Data Transfer","text":"<ul> <li>S3 ingress (uploading to S3): free</li> <li>S3 to Internet: $0.09 per GB</li> <li>S3 Transfer Acceleration:<ul> <li>Additional cost on top of Data Transfer (+$0.04 to $0.08 per GB)</li> </ul> </li> <li>S3 to CloudFront: free (internal network)</li> <li>CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)<ul> <li>Caching capability (lower latency)</li> <li>Reduced costs of S3 Requests (cheaper than just S3)</li> </ul> </li> <li>S3 Cross Region Replication: $0.02 per GB ![[attachments/Pasted image 20220513101700.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/OpsWorks/","title":"OpsWorks","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/OpsWorks/#opsworks","title":"OpsWorks","text":"<ul> <li>Chef &amp; Puppet are two open-source softwares that help you perform server configuration automatically, or repetitive actions</li> <li>AWS OpsWorks is nothing but AWS Managed Chef &amp; Puppet</li> <li>They work great with EC2 &amp; On Premise VM</li> <li>It\u2019s an alternative to AWS SSM</li> <li>Exam tip: Chef &amp; Puppet \u21d2 AWS OpsWorks</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/PrivateLink/","title":"PrivateLink","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/PrivateLink/#privatelink","title":"PrivateLink","text":"<ul> <li>Used to expose services in one VPC to multiple other VPCs, possibly in different accounts</li> <li>Should not use VPC peering as we only want to expose a few services</li> <li>Requires a NLB (common) or GWLB in the service VPC and ENI in the consumer VPC</li> <li>Use multi-AZ NLB and ENIs in multiple AZ for fault-tolerance</li> </ul> <p>![[attachments/Pasted image 20220512235655.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/PrivateLink/#exposing-ecs-tasks","title":"Exposing ECS tasks","text":"<ul> <li>ECS tasks require an ALB. So, we can connect the ALB to the NLB for PrivateLink.</li> <li>Corporate Data Centers will still connect through the VPN or Direct Connect.</li> </ul> <p>![[attachments/Pasted image 20220513000308.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Reachability%20Analyzer/","title":"Reachability Analyzer","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Reachability%20Analyzer/#reachability-analyzer","title":"Reachability Analyzer","text":"<ul> <li>A network diagnostics tool that troubleshoots network connectivity between two endpoints in your VPC</li> <li>It builds a model of the network configuration, then checks the reachability based on these configurations (doesn\u2019t send packets, just tests the configurations)</li> <li>When the destination is:<ul> <li>Reachable - it produces hop-by-hop details of the virtual network path</li> <li>Not reachable - it identifies the blocking components (eg. configuration issues In SGs, NACLs, Route Tables, etc.)</li> </ul> </li> <li>Use cases:<ul> <li>Troubleshoot connectivity issues</li> <li>Ensure network configuration is as intended</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Redshift/","title":"Redshift","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Redshift/#redshift","title":"Redshift","text":"<ul> <li>AWS managed data warehouse (10x better performance than other data warehouses)</li> <li>Based on PostgreSQL</li> <li>Used for Online Analytical Processing (OLAP) and high performance querying</li> <li>Columnar storage of data with massively parallel query execution in SQL</li> <li>Faster querying than [[Athena]] due to indexes</li> <li>Can be used for both long complex queries as well as short fast queries</li> <li>Need to provision instances as a part of the Redshift cluster (pay for the instances provisioned)</li> <li>Integrated with Business Intelligence (BI) tools such as QuickSight or Tableau</li> <li>Redshift Cluster can have 1 to 128 nodes (128TB per node)<ul> <li>Leader Node: query planning &amp; result aggregation</li> <li>Compute Nodes: execute queries &amp; send the result to leader node</li> </ul> </li> <li>No multi-AZ support (all the nodes will be in the same AZ)</li> <li>Auto-healing feature</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Redshift/#loading-data-into-redshift","title":"Loading data into Redshift","text":"<ul> <li>S3<ul> <li>Use COPY command to load data from an S3 bucket into Redshift</li> <li>Without Enhanced VPC Routing<ul> <li>data goes through the public internet</li> </ul> </li> <li>Enhanced VPC Routing<ul> <li>data goes through the VPC without traversing the public internet</li> </ul> </li> </ul> </li> <li>Kinesis Data Firehose<ul> <li>Sends data to S3 and issues a COPY command to load it into Redshift</li> </ul> </li> <li>EC2 Instance<ul> <li>Using JDBC driver</li> <li>Used when an application needs to write data to Redshift</li> <li>Optimal to write data in batches</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Redshift/#snapshots","title":"Snapshots","text":"<ul> <li>Stored internally in S3</li> <li>Incremental (only changes are saved)</li> <li>Can be restored into a new Redshift cluster</li> <li>Automated<ul> <li>based on a schedule or storage size (every 5 GB)</li> <li>set retention</li> </ul> </li> <li>Manual<ul> <li>retains until you delete them</li> </ul> </li> <li>Feature to automatically copy snapshots into another region</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Redshift/#redshift-spectrum","title":"Redshift Spectrum","text":"<ul> <li>Query data present in S3 without loading it into Redshift</li> <li>Need to have a Redshift cluster to use this feature</li> <li>Query is executed by 1000s of Redshift Spectrum nodes</li> <li>Consumes much less of your cluster's processing capacity than other queries</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220514110209.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/","title":"Relational Database Service (RDS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#relational-database-service-rds","title":"Relational Database Service (RDS)","text":"<ul> <li>Regional Service</li> <li>Supports Multi AZ</li> <li>AWS Managed SQL Database</li> <li>Supported Engines<ul> <li>Postgres</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle</li> <li>Microsoft SQL Server</li> <li>Aurora (AWS Proprietary database)</li> </ul> </li> <li>Backed by [[Elastic Compute Cloud (EC2)|EC2]] instances with [[Elastic Block Storage (EBS)|EBS]] storage</li> <li>We don't have access to the underlying instance</li> <li>DB connection is made on port 3306</li> <li>Security Groups are used for network security (must allow incoming TCP traffic on port 3306 from specific IPs)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#backups","title":"Backups","text":"<ul> <li>Automated Backups (enabled by default)<ul> <li>Daily full backup of the database (during the defined maintenance window)</li> <li>Backup retention: 7 days (max 35 days)</li> <li>Transaction logs are backed-up every 5 minutes (point in time recovery)</li> </ul> </li> <li>DB Snapshots:<ul> <li>Manually triggered</li> <li>Backup retention: unlimited</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#auto-scaling","title":"Auto Scaling","text":"<ul> <li>Automatically scales the RDS storage within the max limit</li> <li>Condition for automatic storage scaling:<ul> <li>Free storage is less than 10% of allocated storage</li> <li>Low-storage lasts at least 5 minutes</li> <li>6 hours have passed since last modification</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#read-replicas","title":"Read Replicas","text":"<ul> <li>Allows us to scale the read operation (SELECT) on RDS<ul> <li>![[attachments/Pasted image 20220507005920.png]]</li> </ul> </li> <li>Up to 5 read replicas (within AZ, cross AZ or cross region)</li> <li>Asynchronous Replication (seconds)</li> <li>Replicas can be promoted to their own DB</li> <li>Applications must update the connection string to leverage read replicas</li> <li>Network fee for replication<ul> <li>Same region: free</li> <li>Cross region: paid</li> </ul> </li> </ul> <p>You can create a read replica as a Multi-AZ DB instance. A standby of the replica will be created in another AZ for failover support for the replica.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#multi-az","title":"Multi AZ","text":"<ul> <li>Increase availability of the RDS database by replicating it to another AZ<ul> <li>![[attachments/Pasted image 20220507010002.png]]</li> </ul> </li> <li>Synchronous Replication</li> <li>Connection string does not require to be updated (both the databases can be accessed by one DNS name, which allows for automatic DNS failover to standby database)</li> <li>When failing over, RDS flips the CNAME record for the DB instance to point at the standby, which is in turn promoted to become the new primary.</li> <li>Cannot be used for scaling as the standby database cannot take read/write operation</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#encryption","title":"Encryption","text":"<ul> <li>At rest encryption<ul> <li>KMS AES-256 encryption</li> <li>Encrypted DB  =&gt; Encrypted Snapshots, Encrypted Replicas and vice versa</li> </ul> </li> <li>In flight encryption<ul> <li>SSL certificates</li> <li>Force all connections to your DB instance to use SSL by setting the\u00a0<code>rds.force_ssl</code>\u00a0parameter to <code>true</code></li> <li>To enable encryption in transit, download the AWS-provided root certificates &amp; used them when connecting to DB</li> </ul> </li> <li>To encrypt an un-encrypted RDS database:<ul> <li>Create a snapshot of the un-encrypted database</li> <li>Copy the snapshot and enable encryption for the snapshot</li> <li>Restore the database from the encrypted snapshot</li> <li>Migrate applications to the new database, and delete the old database</li> </ul> </li> <li>To create an encrypted cross-region read replica from a non-encrypted master:<ul> <li>Encrypt a snapshot from the unencrypted master DB instance</li> <li>Create a new encrypted master DB instance</li> <li>Create an encrypted cross-region Read Replica from the new encrypted master</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#access-management","title":"Access Management","text":"<ul> <li>Username and Password can be used to login into the database</li> <li>EC2 instances &amp; [[Lambda]] functions should access the DB using IAM DB Authentication (AWSAuthenticationPlugin with IAM) - token based access<ul> <li>EC2 instance or Lambda function has an IAM role which allows is to make an API call to the RDS service to get the auth token which it uses to access the MySQL database.<ul> <li>![[attachments/Pasted image 20220507011632.png]]</li> </ul> </li> <li>Only works with MySQL and PostgreSQL</li> <li>Auth token is valid for 15 mins</li> <li>Network traffic is encrypted in-flight using SSL</li> <li>Central access management using IAM (instead of doing it for each DB individually)</li> </ul> </li> <li>EC2 &amp; Lambda can also get DB credentials from [[SSM Parameter Store]] to authenticate to the DB - credentials based access</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#rds-events","title":"RDS Events","text":"<ul> <li>RDS events only provide operational events on the DB instance (not the data)</li> <li>To capture data modification events, use native functions or stored procedures to invoke a Lambda function.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#monitoring","title":"Monitoring","text":"<ul> <li>CloudWatch Metrics for RDS<ul> <li>Gathers metrics from the hypervisor of the DB instance<ul> <li>CPU Utilization</li> <li>Database Connections</li> <li>Freeable Memory</li> </ul> </li> </ul> </li> <li>Enhanced Monitoring<ul> <li>Gathers metrics from an agent running on the RDS instance<ul> <li>OS processes</li> <li>RDS child processes</li> </ul> </li> <li>Used to monitor different processes or threads on a DB instance (ex. percentage of the CPU bandwidth and total memory consumed by each database process in your RDS instance</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Relational%20Database%20Service%20%28RDS%29/#maintenance-upgrade","title":"Maintenance &amp; Upgrade","text":"<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete. This is why it should be done during the maintenance window.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Resource%20Access%20Manager%20%28RAM%29/","title":"Resource Access Manager (RAM)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Resource%20Access%20Manager%20%28RAM%29/#resource-access-manager-ram","title":"Resource Access Manager (RAM)","text":"<ul> <li>Share AWS resources with other AWS accounts to avoid resource duplication</li> <li>Each participating account manage their own resources</li> <li>Participating accounts can\u2019t view, modify, delete resources that belong to other participants or the owner</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Resource%20Access%20Manager%20%28RAM%29/#vpc-sharing","title":"VPC Sharing","text":"<ul> <li>Allows to share one or more subnets with other accounts within the same organization</li> <li>Allows multiple accounts to create resources into shared and centrally-managed VPCs</li> <li>Cannot share the whole VPC</li> <li>Network is shared (high degree of interconnectivity)<ul> <li>every resource deployed in the subnet can talk to each other using private IP</li> <li>security groups from other accounts can be referenced</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220511203336.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/","title":"Route 53","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#route-53","title":"Route 53","text":"<ul> <li>Global Service</li> <li>AWS managed Authoritative DNS (customer can update the DNS records and have full control over the [[Concepts#DNS|DNS]]) </li> <li>Also a Domain Registrar (for registering domain names)</li> <li>Only AWS service which provides 100% availability SLA</li> <li>Affected by client's DNS caching (not suitable for [[Blue-Green Deployment]] if the client caches DNS queries)</li> </ul> <p>It is recommended to use DNS names or URLs instead of IPs wherever possible</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#hosted-zone","title":"Hosted Zone","text":"<ul> <li>A container for DNS records that define how to route traffic to a domain and its subdomains. </li> <li>Hosted zone is queried to get the IP address from the hostname</li> <li>Two types:<ul> <li>Public Hosted Zone<ul> <li>resolves public domain names</li> <li>can be queried by anyone on the internet</li> </ul> </li> <li>Private Hosted Zone<ul> <li>resolves private domain names</li> <li>can only be queried from within the VPC</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#record-types","title":"Record Types","text":"<ul> <li>A - maps a hostname to IPv4</li> <li>AAAA - maps a hostname to IPv6</li> <li>CNAME - maps a hostname to another hostname<ul> <li>The target is a domain name which must have an A or AAAA record</li> <li>Cannot point to root domains (Zone Apex)   Ex: you can\u2019t create a CNAME record for <code>example.com</code>, but you can create for <code>something.example.com</code></li> </ul> </li> <li>NS (Name Servers) - controls how traffic is routed for a domain</li> <li>Alias - maps a hostname to an AWS resource<ul> <li>AWS proprietary</li> <li>Can point to root (zone apex) and non-root domains</li> <li>Alias Record is of type A or AAAA (IPv4 / IPv6)</li> <li>Targets can be<ul> <li>Elastic Load Balancers</li> <li>CloudFront Distributions</li> <li>API Gateway</li> <li>Elastic Beanstalk environments</li> <li>S3 Websites</li> <li>VPC Interface Endpoints</li> <li>Global Accelerator accelerator</li> <li>Route 53 record in the same hosted zone</li> </ul> </li> <li>Target cannot be EC2</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#routing-policies","title":"Routing Policies","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#simple","title":"Simple","text":"<ul> <li>Route to one or more resources </li> <li>If multiple values are returned, client chooses one at random (client-side load balancing)</li> <li>No health check (if returning multiple resources, some of them might be unhealthy)</li> <li>Image<ul> <li>![[attachments/Pasted image 20220507122603.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#weighted","title":"Weighted","text":"<ul> <li>Route a fraction of request to multiple resources</li> <li>Health checks</li> <li>Use case: testing a new application version by sending a small amount of traffic</li> <li>Can be used for Active-Active failover strategy</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#latency-based","title":"Latency-based","text":"<ul> <li>Redirect to the resource that has the lowest latency</li> <li>Health checks</li> <li>Can be used for Active-Active failover strategy</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#failover","title":"Failover","text":"<ul> <li>Primary &amp; Secondary Records (if the primary application is down, route to secondary application)</li> <li>Health check must be associated with the primary record</li> <li>Used for Active-Passive failover strategy</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#geolocation","title":"Geolocation","text":"<ul> <li>Routing based on the client's location</li> <li>Should create a \u201cDefault\u201d record (in case there\u2019s no match on location)</li> <li>Use cases: restrict content distribution &amp; language preference</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#geoproximity","title":"Geoproximity","text":"<ul> <li>Route traffic to your resources based on the proximity of clients to the resources</li> <li>Ability to shift more traffic to resources based on the defined bias.<ul> <li>To expand (bias: 1 to 99) \u2192 more traffic to the resource</li> <li>To shrink (bias: -1 to-99) \u2192 less traffic to the resource</li> </ul> </li> <li>Resources can be:<ul> <li>AWS resources (specify AWS region)</li> <li>Non-AWS resources (specify Latitude and Longitude)</li> </ul> </li> <li>Uses Route 53 Traffic Flow</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#multi-value","title":"Multi-value","text":"<ul> <li>Route traffic to multiple resources (max 8)</li> <li>Health Checks (only healthy resources will be returned)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#health-checks","title":"Health Checks","text":"<ul> <li>HTTP Health Checks are only for public resources</li> <li>Allows for Automated DNS Failover</li> <li>Three types:<ul> <li>Monitor an endpoint (application or other AWS resource)<ul> <li>Multiple global health checkers check the endpoint health</li> <li>Must configure the application firewall to allow incoming requests from the IPs of Route 53 Health Checkers</li> <li>Supported protocols: HTTP, HTTPS and TCP</li> </ul> </li> <li>Monitor other health checks (Calculated Health Checks)<ul> <li>Combine the results of multiple Health Checks into one (AND, OR, NOT)</li> <li>Specify how many of the health checks need to pass to make the parent pass</li> <li>Usage: perform maintenance to your website without causing all health checks to fail</li> </ul> </li> <li>Monitor CloudWatch Alarms (to perform health check on private resources)<ul> <li>Route 53 health checkers are outside the VPC. They can\u2019t access private endpoints (private VPC or on-premises resources). </li> <li>Create a CloudWatch Metric and associate a CloudWatch Alarm to it, then create a Health Check that checks the CW alarm.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#godaddy-with-route-53","title":"GoDaddy with Route 53","text":"<ul> <li>Use GoDaddy as registrar and Route 53 as DNS<ul> <li>Once we register a hostname at GoDaddy, we need to update the name servers (NS) of GoDaddy to match the name servers of a public hosted zone created in Route 53. This way, GoDaddy will use Route 53\u2019s DNS.</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Route%2053/#dns-resolution-in-hybrid-cloud","title":"DNS Resolution in Hybrid Cloud","text":"<ul> <li>To resolve DNS queries for resources in the VPC from the on-premises network, create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint.</li> <li>To resolve DNS queries for resources in the on-premises network from the VPC, create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/SSM%20Parameter%20Store/","title":"SSM Parameter Store","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/SSM%20Parameter%20Store/#ssm-parameter-store","title":"SSM Parameter Store","text":"<ul> <li>Serverless</li> <li>Used to store parameters &amp; secrets</li> <li>Parameter versioning</li> <li>Seamless Encryption with KMS for encryption and decryption of stored secrets</li> <li>Parameters are stored in hierarchical fashion</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/SSM%20Parameter%20Store/#tiers","title":"Tiers","text":"Standard Tier Advanced Tier Number of parameters 10,000 100,000 Max parameter size 4KB 8KB Parameter Policy Not supported Supported Cost Free Paid"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/SSM%20Parameter%20Store/#parameter-policies","title":"Parameter Policies","text":"<ul> <li>Only supported in advanced tier</li> <li>Assign policies to a parameter for additional features<ul> <li>Expire the parameter after some time (TTL)</li> <li>Parameter expiration notification</li> <li>Parameter change notification</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Secrets%20Manager/","title":"Secrets Manager","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Secrets%20Manager/#secrets-manager","title":"Secrets Manager","text":"<ul> <li>For storing secrets only</li> <li>Ability to force rotation of secrets every n days (not available in [[SSM Parameter Store|Parameter Store]])</li> <li>A secret consists of multiple key-value pairs</li> <li>Secrets are encrypted using [[Key Management Service (KMS)|KMS]]</li> <li>Mostly used for [[Relational Database Service (RDS)|RDS]] authentication<ul> <li>need to specify the username and password to access the database</li> <li>link the secret to the database to allow for automatic rotation of database login info</li> </ul> </li> <li>Can create custom secrets</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Security%20Token%20Service%20%28STS%29/","title":"Security Token Service (STS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Security%20Token%20Service%20%28STS%29/#security-token-service-sts","title":"Security Token Service (STS)","text":"<ul> <li>Used to grant limited and temporary access to AWS resources</li> <li>Token is valid for up to 1h</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Security%20Token%20Service%20%28STS%29/#assumerole","title":"AssumeRole","text":"<ul> <li>Allows IAM Users to assume an IAM Role</li> <li>Steps to assume a role<ul> <li>Create an lAM Role (within your account or cross-account)</li> <li>[[Identity &amp; Access Management (IAM)#Trust Policies|Trust Policy]]: define which principals should be allowed to assume this role</li> <li>Use STS <code>AssumeRole</code> API to retrieve temporary credentials for the IAM role</li> <li>STS will check with IAM whether or not the user is allowed to assume that role</li> <li>![[attachments/Pasted image 20220511095712.png]]</li> </ul> </li> <li>Use cases:<ul> <li>Safety: deleting a resource first requires users to temporarily assume a role</li> <li>Cross-account access: assume role in target account to perform actions there<ul> <li>![[attachments/Pasted image 20220511095752.png]]</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Security%20Token%20Service%20%28STS%29/#assumerolewithsaml","title":"AssumeRoleWithSAML","text":"<ul> <li>Allow non IAM users logged in with SAML to assume an IAM role</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Security%20Token%20Service%20%28STS%29/#assumerolewithwebldentity","title":"AssumeRoleWithWebldentity","text":"<ul> <li>Allow non IAM users logged in via an identity provider (Facebook, Google, etc.) to assume an IAM role</li> <li>AWS recommends using [[Cognito]] instead</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Application%20Model%20%28SAM%29/","title":"Serverless Application Model (SAM)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Application%20Model%20%28SAM%29/#serverless-application-model-sam","title":"Serverless Application Model (SAM)","text":"<ul> <li>Extension of [[CloudFormation]]</li> <li>Framework for developing and deploying serverless applications</li> <li>Configuration is YAML</li> <li>Can run Lambda, API Gateway, DynamoDB locally for development and debugging</li> <li>Can use CodeDeploy to deploy Lambda functions</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Blogging%20Website/","title":"Serverless Blogging Website","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Blogging%20Website/#serverless-blogging-website","title":"Serverless Blogging Website","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Blogging%20Website/#requirements","title":"Requirements","text":"<ul> <li>This website should scale globally</li> <li>Blogs are rarely written, but often read</li> <li>Some of the website is purely static files, the rest is a dynamic REST API (public)</li> <li>Caching must be implement where possible</li> <li>Any new users that subscribes should receive a welcome email</li> <li>Any photo uploaded to the blog should have a thumbnail generated</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Blogging%20Website/#architecture","title":"Architecture","text":"<p>![[attachments/Pasted image 20220510213450.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Premium%20Stock%20Video%20Website/","title":"Serverless Premium Stock Video Website","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Premium%20Stock%20Video%20Website/#serverless-premium-stock-video-website","title":"Serverless Premium Stock Video Website","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Premium%20Stock%20Video%20Website/#requirements","title":"Requirements","text":"<ul> <li>We sell videos online and users have to pay to buy videos</li> <li>Each video can be bought by many different customers</li> <li>We only want to distribute videos to users who are premium users</li> <li>We have a database of premium users</li> <li>Links we send to premium users should be short lived</li> <li>Our application is global</li> <li>We want to be fully serverless</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20Premium%20Stock%20Video%20Website/#architecture","title":"Architecture","text":"<ul> <li>Since the user must login to view premium videos, we can use Cognito for authentication. If the user is authenticated, API gateway will send the login info to Lambda which can query the DynamoDB to check whether the authenticated user is premium or not.</li> <li>We need to use another API endpoint to get signed URL from CloudFront. The API gateway after verifying the authentication of the client using Cognito, will invoke a lambda function that will query the DB to check if the user is premium. If so, it will use SDK to generate CF pre-signed URL and return it to client. The client will use the signed URL to access paid content via CF.</li> </ul> <p>![[attachments/Pasted image 20220510214413.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20ToDo%20List%20App/","title":"Serverless ToDo List App","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20ToDo%20List%20App/#serverless-todo-list-app","title":"Serverless ToDo List App","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20ToDo%20List%20App/#requirements","title":"Requirements","text":"<ul> <li>Expose as REST API with HTTPS</li> <li>Serverless architecture</li> <li>Users should be able to directly interact with their own folder in S3</li> <li>Users should authenticate through a managed serverless service</li> <li>Users can write and read to-dos, but they mostly read them</li> <li>The database should scale, and have some high read throughput</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Serverless%20ToDo%20List%20App/#architecture","title":"Architecture","text":"<ul> <li>Giving users access to a folder in S3<ul> <li>Cognito Identity Pool can be used to get temporary credentials after authenticating using CUP.</li> <li>Pre-signed URL isn\u2019t used since we need to provide access to the bucket and not an object.</li> </ul> </li> <li>Improving read throughputs<ul> <li>Implement a DAX layer to cache DynamoDB queries.</li> <li>Caching can also be implemented as the API gateway level if the read responses don\u2019t change much.</li> </ul> </li> </ul> <p>![[attachments/Pasted image 20220510213033.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Shared%20Responsibility%20Model/","title":"Shared Responsibility Model","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Shared%20Responsibility%20Model/#shared-responsibility-model","title":"Shared Responsibility Model","text":"<p>![[attachments/Pasted image 20220511234345.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/","title":"Simple Notification Service (SNS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#simple-notification-service-sns","title":"Simple Notification Service (SNS)","text":"<ul> <li>Used to broadcast messages</li> <li>Pub-Sub model (publisher publishes messages to a topic, subscribers listen to the topic)</li> <li>Instant message delivery (does not queue messages)</li> <li>Subscribers can be:<ul> <li>SQS queues</li> <li>HTTP / HTTPS endpoints</li> <li>Lambda functions</li> <li>Emails (using SNS)</li> <li>SMS messages</li> <li>Mobile Notifications</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#encryption","title":"Encryption","text":"<ul> <li>In-flight encryption by default using HTTPS API</li> <li>At-rest encryption using KMS keys (optional)</li> <li>Client-side encryption</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#access-management","title":"Access Management","text":"<ul> <li>lAM policies to regulate access to the SNS API</li> <li>SNS Access Policies (resource based policy)<ul> <li>Used for cross-account access to SNS topics</li> <li>Used for allowing other AWS services to publish to an SNS topic</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#fifo-topics","title":"FIFO Topics","text":"<ul> <li>Guaranteed ordering of messages in that topic</li> <li>Publishing messages to a FIFO topic requires:<ul> <li>Group ID: messages will be ordered and grouped for each group ID</li> <li>Message deduplication ID: for deduplication of messages</li> </ul> </li> <li>Can only have SQS FIFO queues as subscribers</li> <li>Limited throughput (same as SQS FIFO) because only SQS FIFO queues can read from FIFO topics</li> <li>The topic name must end with <code>.fifo</code></li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#sns-simple-queue-service-sqssqs-fanout-pattern","title":"SNS + [[Simple Queue Service (SQS)|SQS]] Fanout Pattern","text":"<ul> <li>Fully decoupled, no data loss</li> <li>SQS allows for: data persistence, delayed processing and retries of work</li> <li>Make sure your SQS queue access policy allows for SNS to write</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509205738.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#message-filtering","title":"Message Filtering","text":"<ul> <li>JSON policy used to filter messages sent to SNS topic\u2019s subscriptions</li> <li>Each subscriber will have its own filter policy (if a subscriber doesn\u2019t have a filter policy, it receives every message)</li> <li>Ex: filter messages sent to each queue by the order status</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Notification%20Service%20%28SNS%29/#sns-lambda-dlq","title":"SNS + Lambda + DLQ","text":"<ul> <li>Lambda retries each failed message 3 times after which it is sent to the DLQ by lambda<ul> <li>![[attachments/Pasted image 20220513220228.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/","title":"Simple Queue Service (SQS)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#simple-queue-service-sqs","title":"Simple Queue Service (SQS)","text":"<ul> <li>Used to asynchronously decouple applications</li> <li>Supports multiple producers &amp; consumers</li> <li>When the consumer has successfully processed a message, it is removed from the queue</li> <li>Unlimited number of messages in queue</li> <li>Max 10 messages per batch</li> <li>Max message size: 256KB</li> <li>Default message retention: 4 days (max: 14 days)</li> <li>Consumers could be EC2 instances or Lambda functions</li> </ul> <p>SQS cannot ingest data, it must be sent to the queue by the producer (use [[Kinesis#Kinesis Data Stream KDS|Kinesis Data Stream]] or [[Kinesis#Kinesis Data Firehose KDF|Kinesis Data Firehose]] instead)</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#queue-types","title":"Queue Types","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#standard-queue","title":"Standard Queue","text":"<ul> <li>Unlimited throughput (publish any number of message per second into the queue)</li> <li>Low latency (&lt;10 ms on publish and receive)</li> <li>Can have duplicate messages (at least once delivery)</li> <li>Can have out of order messages (best effort ordering)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#fifo-queue","title":"FIFO Queue","text":"<ul> <li>Limited throughput<ul> <li>300 msg/s without batching (batch size = 1)</li> <li>3000 msg/s with batching (batch size = 10)</li> </ul> </li> <li>No duplicate messages</li> <li>Guaranteed ordering of messages</li> <li>The queue name must end with <code>.fifo</code> to be considered a FIFO queue</li> <li>Sending messages to a FIFO queue requires:<ul> <li>Group ID: messages will be ordered and grouped for each group ID</li> <li>Message deduplication ID: for deduplication of messages</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#consumer-auto-scaling","title":"Consumer Auto Scaling","text":"<p>We can attach an [[Auto Scaling Group (ASG)|ASG]] to the consumer instances which will scale based on the CW metric = Queue length / Number of EC2 instances. CW alarms can be triggered to step scale the consumer application.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#encryption","title":"Encryption","text":"<ul> <li>In-flight encryption using HTTPS API</li> <li>At-rest encryption using KMS keys</li> <li>Client-side encryption</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#access-management","title":"Access Management","text":"<ul> <li>IAM Policies to regulate access to the SQS API</li> <li>SQS Access Policies (resource based policy)<ul> <li>Used for cross-account access to SQS queues</li> <li>Used for allowing other AWS services to send messages to an SQS queue</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#configurations","title":"Configurations","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#message-visibility-timeout","title":"Message Visibility Timeout","text":"<ul> <li>Once a message is polled by a consumer, it becomes invisible to other consumers for the duration of message visibility timeout. After the message visibility timeout is over, the message is visible in the queue.</li> <li>If a consumer dies while processing the message, it will be visible in the queue after the visibility timeout</li> <li>If a message is not processed within the visibility timeout, it will be processed again (by another consumer). However, a consumer could call the <code>ChangeMessageVisibility</code> API to change the visibility timeout for that specific message. This will give the consumer more time to process the message.</li> <li>Default: 30s </li> <li>Can be configured for the entire queue<ul> <li>High: if the consumer crashes, re-processing will take long</li> <li>Low: may get duplicate processing of messages</li> </ul> </li> <li>Image<ul> <li>![[attachments/Pasted image 20220509201807.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<ul> <li>An SQS queue used to store failing to be processed messages in another queue</li> <li>After the <code>MaximumReceives</code> threshold is exceeded, the message goes into the DLQ</li> <li>Prevents resource wastage</li> <li>Recommended to set a high retention period for DLQ (14 days)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#queue-delay","title":"Queue Delay","text":"<ul> <li>Consumers see the message after some delay</li> <li>Default: 0 (Max: 15 min)</li> <li>Can be set at the queue level</li> <li>Can override the default queue delay for a specific message using the <code>DelaySeconds</code> parameter in the message (message timer)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#long-polling","title":"Long Polling","text":"<ul> <li>Poll the queue for longer</li> <li>Decreases the number of API calls made to SQS (cheaper)</li> <li>Reduces latency (incoming messages during the polling will be read instantaneously)</li> <li>Polling time: 1 sec to 20 sec</li> <li>Long Polling is preferred over Short Polling</li> <li>Can be enabled at the queue level or at the consumer level by using <code>WaitTimeSeconds</code></li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#request-response-system","title":"Request-Response System","text":"<ul> <li>The idea is to build a request-response system where both the requesters and responders can scale independently. The requester sends the request into a request queue with attributes \u201ccorrelation ID\u201d and \u201creply to\u201d. This request will be picked by one of many responders in an ASG. The request will be processed and it will be sent to the right response queue along with the same \u201ccorrelation ID\u201d. The \u201ccorrelation ID\u201d will help the requester identify which response corresponds to their request.</li> <li>To implement this pattern: use the SQS Temporary Queue Client which leverages virtual queues instead of creating / deleting SQS queues (cost-effective).</li> <li>![[attachments/Pasted image 20220509203244.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#sqs-lambda-dlq","title":"SQS + Lambda + DLQ","text":"<ul> <li>Failed messages (after the set number of retries) are sent to the DLQ by the SQS queue<ul> <li>![[attachments/Pasted image 20220513220430.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Queue%20Service%20%28SQS%29/#handling-priority","title":"Handling Priority","text":"<p>Use two SQS queues, one for low priority (ex. free) and the other for high priority (ex. paid). Configure your consuming application to only poll the low priority queue if the high priority queue is empty.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/","title":"Simple Storage Service (S3)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#simple-storage-service-s3","title":"Simple Storage Service (S3)","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#intro","title":"Intro","text":"<ul> <li>Global Service</li> <li>Object store (key-value pairs)</li> <li>Buckets must have a globally unique name</li> <li>Buckets are defined at the regional level</li> <li>Objects have a key (full path to the object):     <code>s3://my_bucket/my_folder/another_folder/my_file.txt</code></li> <li>The key is composed of bucket + prefix + object name     s3://my_bucket /my_folder/another_folder/ my_file.txt</li> <li>There\u2019s no concept of directories within buckets (just keys with very long names that contain slashes)</li> <li>Max Object Size: 5TB</li> <li>Durability: 99.999999999% (total 11 9's)</li> <li>SYNC command can be used to copy data between buckets, possibly in different regions</li> </ul> <ul> <li>S3 delivers strong read-after-write consistency (if an object is overwritten and immediately read, S3 always returns the latest version of the object)</li> <li>S3 is strongly consistent for all GET, PUT and LIST operations</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#bucket-versioning","title":"Bucket Versioning","text":"<ul> <li>Enabled at the bucket level</li> <li>Protects against unintended deletes</li> <li>Ability to restore to a previous version</li> <li>Any file that is not versioned prior to enabling versioning will have version \u201cnull\u201d</li> <li>Suspending versioning does not delete the previous versions, just disables it for the future</li> <li>To restore a deleted object, delete it's \"delete market\"</li> </ul> <ul> <li>Versioning can only be suspended once it has been enabled.</li> <li>Once you version-enable a bucket, it can never return to an unversioned state.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#encryption","title":"Encryption","text":"<ul> <li>Can be enabled at the bucket level or at the object level</li> <li>Server Side Encryption (SSE)<ul> <li>SSE-S3<ul> <li>Keys managed by S3</li> <li>AES-256 encryption</li> <li>HTTP or HTTPS can be used</li> <li>Must set header: <code>\"x-amz-server-side-encryption\": \"AES256\"</code></li> </ul> </li> <li>SSE-KMS<ul> <li>Keys managed by KMS</li> <li>HTTP or HTTPS can be used</li> <li>KMS provides control over who has access to what keys as well as audit trails</li> <li>Must set header: <code>\"x-amz-server-side-encryption\": \"aws:kms\"</code></li> </ul> </li> <li>SSE-C<ul> <li>Keys managed by the client</li> <li>Client sends the key in HTTPS headers for encryption/decryption (S3 discards the key after the operation)</li> <li>HTTPS must be used as key (secret) is being transferred</li> </ul> </li> </ul> </li> <li>Client Side Encryption<ul> <li>Keys managed by the client</li> <li>Client encrypts the object before sending it to S3 and decrypts it after retrieving it from S3</li> </ul> </li> </ul> <p>[!info]- Enforcing Encryption  - Default encryption: encrypt the files using the default encryption (specify the encryption for the file while uploading to override the default) - Bucket policy can be used to force a specific type of encryption on the objects uploaded to S3 </p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#access-management","title":"Access Management","text":"<ul> <li>User based security<ul> <li>IAM policies define which API calls should be allowed for a specific user</li> <li>Preferred over bucket policy for fine-grained access control</li> </ul> </li> <li>Resource based security (Bucket Policy)<ul> <li>Grant public access to the bucket</li> <li>Force objects to be encrypted at upload</li> <li>Cross-account access</li> <li>Object Access Control List (ACL) - applies to the objects while uploading</li> <li>Bucket Access Control List (ACL) - access policy that applies to the bucket</li> </ul> </li> </ul> <p>By default, an S3 object is owned by the account that uploaded it even if the bucket is owned by another account. To get full access to the object, the object owner must explicitly grant the bucket owner access. As a bucket owner, you can create a bucket policy to require external users to grant\u00a0<code>bucket-owner-full-control</code>\u00a0when uploading objects so the bucket owner can have full access to the objects.</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#s3-static-websites","title":"S3 Static Websites","text":"<ul> <li>Host static websites (may contain client-side scripts) and have them accessible on the public internet over HTTP only (for HTTPS, use [[CloudFront]] with S3 bucket as the origin)</li> <li>The website URL will be either of the following:<ul> <li><code>&lt;bucket-name&gt;.s3-website-&lt;region&gt;.amazonaws.com</code></li> <li><code>&lt;bucket-name&gt;.s3-website.&lt;region&gt;.amazonaws.com</code></li> </ul> </li> <li>If you get a <code>403 (Forbidden)</code> error, make sure the bucket policy allows public reads</li> <li>For cross-origin access to the S3 bucket, we need to enable [[Concepts#Cross-Origin Resource Sharing CORS|CORS]] on the bucket<ul> <li>![[attachments/Pasted image 20220507175558.png]]</li> </ul> </li> <li>To host an S3 static website on a custom domain using Route 53, the bucket name should be the same as your domain or subdomain    Ex. for subdomain <code>portal.tutorialsdojo.com</code>, the name of the bucket must be <code>portal.tutorialsdojo.com</code></li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#mfa-delete","title":"MFA Delete","text":"<ul> <li>MFA required to<ul> <li>permanently delete an object version</li> <li>suspend versioning on the bucket</li> </ul> </li> <li>Bucket Versioning must be enabled</li> <li>Can only be enabled or disabled by the root user</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#server-access-logging","title":"Server Access Logging","text":"<ul> <li>Most detailed way of logging access to S3 buckets (better than [[CloudTrail]])</li> <li>Does not support Data Events &amp; Log File Validation (use [[CloudTrail]] for that)</li> <li>Store S3 access logs into another bucket</li> <li>Logging bucket should not be the same as monitored bucket (logging loop)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#replication","title":"Replication","text":"<ul> <li>Asynchronous replication</li> <li>Objects are replicated with the same version ID</li> <li>Supports cross-region and cross-account replication</li> <li>Versioning must be enabled for source and destination buckets</li> <li>For DELETE operations:<ul> <li>Replicate delete markers from source to target (optional)</li> <li>Permanent deletes are not replicated</li> </ul> </li> <li>There is no chaining of replication. So, if bucket 1 has replication into bucket 2, which has replication into bucket 3. Then objects created in bucket 1 are not replicated to bucket 3.</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#pre-signed-url","title":"Pre-signed URL","text":"<ul> <li>Pre-signed URLs for S3 have temporary access token as query string parameters which allow anyone with the URL to temporarily access the resource before the URL expires (default 1h)</li> <li>Pre-signed URLs inherit the permission of the user who generated it</li> <li>Uses:<ul> <li>Allow only logged-in users to download a premium video</li> <li>Allow users to upload files to a precise location in the bucket</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#storage-classes","title":"Storage Classes","text":"<ul> <li>Data can be transitioned between storage classes manually or automatically using lifecycle rules</li> <li>Data can be put directly into any storage class</li> <li>Standard<ul> <li>99.99% availability</li> <li>Most expensive</li> <li>Instant retrieval</li> <li>No cost on retrieval (only storage cost)</li> <li>For frequently accessed data</li> </ul> </li> <li>Infrequent Access<ul> <li>For data that is infrequently accessed, but requires rapid access when needed</li> <li>Lower storage cost than Standard but cost on retrieval</li> <li>Can move data into IA from Standard only after 30 days</li> <li>Two types:<ul> <li>Standard IA<ul> <li>99.9% Availability</li> </ul> </li> <li>One-Zone IA<ul> <li>99.5% Availability</li> <li>Data is lost if AZ fails</li> <li>Storage for infrequently accessed data that can be easily recreated</li> </ul> </li> </ul> </li> </ul> </li> <li>Glacier<ul> <li>For data archival</li> <li>Cost for storage and retrieval</li> <li>Can move data into Glacier from Standard anytime</li> <li>Objects cannot be directly accessed, they first need to be restored which could take some time (depending on the tier) to fetch the object.</li> <li>Default encryption for data at rest and in-transit</li> <li>Three types:<ul> <li>Glacier Instant Retrieval<ul> <li>99.9% availability</li> <li>Millisecond retrieval</li> <li>Minimum storage duration of 90 days</li> <li>Great for data accessed once a quarter</li> </ul> </li> <li>Glacier Flexible Retrieval<ul> <li>99.99% availability</li> <li>3 retrieval flexibility (decreasing order of cost):<ul> <li>Expedited (1 to 5 minutes)<ul> <li>Can provision retrieval capacity for reliability</li> <li>Without provisioned capacity expedited retrievals might be rejected in situations of high demand</li> </ul> </li> <li>Standard (3 to 5 hours)</li> <li>Bulk (5 to 12 hours)</li> </ul> </li> <li>Minimum storage duration of 90 days</li> </ul> </li> <li>Glacier Deep Archive<ul> <li>99.99% availability</li> <li>2 flexible retrieval:<ul> <li>Standard (12 hours)</li> <li>Bulk (48 hours)</li> </ul> </li> <li>Minimum storage duration of 180 days</li> <li>Lowest cost</li> </ul> </li> </ul> </li> </ul> </li> <li>Intelligent Tiering<ul> <li>99.9% availability</li> <li>Moves objects automatically between Access Tiers based on usage</li> <li>Small monthly monitoring and auto-tiering fee</li> <li>No retrieval charges</li> </ul> </li> <li> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#moving-between-storage-classes","title":"Moving between Storage Classes","text":"<ul> <li>In the diagram below, transition can only happen in the downward direction</li> <li>![[attachments/Pasted image 20220514201314.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#lifecycle-rules","title":"Lifecycle Rules","text":"<ul> <li>Used to automate transition or expiration actions on S3 objects</li> <li>Transition Action (transitioned to another storage class)</li> <li>Expiration Action (delete objects after some time)<ul> <li>delete a version of an object</li> <li>delete incomplete multi-part uploads</li> </ul> </li> <li>Lifecycle Rules can be created for a prefix (ex <code>s3://mybucket/mp3/*</code>) or objects tags (ex Department: Finance)</li> </ul> <ul> <li>When you apply a retention period to an object version explicitly, you specify a\u00a0<code>Retain Until Date</code>\u00a0for the object version</li> <li>When you use bucket default settings, you don't specify a <code>Retain Until Date</code>. Instead, you specify a duration, for which every object version placed in the bucket should be protected.</li> <li>Different versions of a single object can have different retention modes and periods</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#s3-analytics","title":"S3 Analytics","text":"<ul> <li>Provides analytics to determine when to transition data into different storage classes</li> <li>Does not work for ONEZONE_IA &amp; GLACIER</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#performance","title":"Performance","text":"<ul> <li>3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix</li> <li>Recommended to spread data across prefixes for maximum performance</li> <li>SSE-KMS may create bottleneck in S3 performance</li> <li>Performance Optimizations<ul> <li>Multi-part Upload<ul> <li>parallelizes upload</li> <li>recommended for files &gt; 100MB</li> <li>must use for files &gt; 5GB</li> </ul> </li> <li>Byte-range fetches<ul> <li>Parallelize download requests by fetching specific byte ranges in each request</li> <li>Better resilience in case of failures since we only need to refetch the failed byte range and not the whole file</li> </ul> </li> <li>S3 Transfer Acceleration<ul> <li>Speed up upload and download for large objects (&gt;1GB) for global users </li> <li>Data is ingested at the nearest edge location and is transferred over AWS private network (uses [[CloudFront]] internally)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#s3-select","title":"S3 Select","text":"<ul> <li>Select a subset of data from S3 using SQL queries (server-side filtering)</li> <li>Less network cost</li> <li>Less CPU cost on the client-side</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#data-transfer-costs","title":"Data Transfer Costs","text":"<ul> <li>Uploads to S3 are free</li> <li>Downloads from S3 are paid</li> <li>Using S3 Transfer Acceleration, you pay only for transfers that are accelerated</li> <li>Since buckets are defined within a region, data transfer within a region is free</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#s3-notification-events","title":"S3 Notification Events","text":"<ul> <li>Optional</li> <li>Generates events for operations performed on the bucket or objects</li> <li>Object name filtering using prefix and suffix matching</li> <li>Targets<ul> <li>SNS topics</li> <li>SQS Standard queues (not FIFO queues)</li> <li>Lambda functions</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#requester-pays-buckets","title":"Requester Pays Buckets","text":"<ul> <li>Requester pays the cost of the request and the data downloaded from the bucket. The bucket owner only pays for the storage.</li> <li>Used to share large datasets with other AWS accounts</li> <li>The requester must be authenticated in AWS (cannot be anonymous)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#object-lock","title":"Object Lock","text":"<ul> <li>WORM (Write Once Read Many) model</li> <li>Block an object version modification or deletion for a specified amount of time</li> <li>Modes:<ul> <li>Governance mode<ul> <li>Only users with special permissions can overwrite or delete the object version or alter its lock settings</li> </ul> </li> <li>Compliance mode<ul> <li>A protected object version cannot be overwritten or deleted by any user, including the root user</li> <li>The object's retention mode can\u2019t be changed, and the retention period can\u2019t be shortened</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Simple%20Storage%20Service%20%28S3%29/#glacier-vault-lock","title":"Glacier Vault Lock","text":"<ul> <li>WORM (Write Once Read Many) model for Glacier</li> <li>For compliance and data retention</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Single%20Sign-On%20%28SSO%29/","title":"Single Sign On (SSO)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Single%20Sign-On%20%28SSO%29/#aws-single-sign-on-sso","title":"AWS Single Sign-On (SSO)","text":"<ul> <li>Free service</li> <li>Integrated with AWS Organizations (login once for your org and you can access all the accounts within that org)</li> <li>Integration with on-premise Active Directory</li> <li>Supports SAML 2.0</li> <li>Centralized auditing with CloudTrail</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Single%20Sign-On%20%28SSO%29/#federated-sso-vs-aws-sso","title":"Federated SSO vs AWS SSO","text":"<ul> <li>With Federated SSO, we need to maintain a 3rd party identity provider (AD/ADFS, CUP, Google, Facebook, custom identity broker) login portal. The IDP returns the JWT or SAML Assertion which the client needs to exchange with STS for login credentials. </li> <li>With AWS SSO, we don\u2019t need to manage the login portal, it is done through the AWS SSO. It returns the credentials directly. ![[attachments/Pasted image 20220511204548.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Site-to-Site%20VPN/","title":"Site to Site VPN","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Site-to-Site%20VPN/#site-to-site-vpn","title":"Site-to-Site VPN","text":"<ul> <li>Easiest and most cost-effective way to connect a VPC to an on-premise data center</li> <li>IPSec Encrypted connection through the public internet</li> <li>Virtual Private Gateway (VGW): VPN concentrator on the VPC side of the VPN connection</li> <li>Customer Gateway (CGW): Software application or physical device on customer side of the VPN connection</li> <li>If you need to ping EC2 instances from on-premises, make sure you add the ICMP protocol on the inbound rules of your security groups ![[attachments/Pasted image 20220512224053.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Site-to-Site%20VPN/#vpn-cloudhub","title":"VPN CloudHub","text":"<ul> <li>Low-cost hub-and-spoke model for network connectivity between a VPC and multiple on-premise data centers</li> <li>Every participating network can communicate with one another through the VPN connection ![[attachments/Pasted image 20220512224648.png]]</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Snow%20Family/","title":"Snow Family","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Snow%20Family/#snow-family","title":"Snow Family","text":"<ul> <li>Offline data migration to [[Simple Storage Service (S3)|S3]] or [[Simple Storage Service (S3)#Storage Classes|Glacier]]</li> <li>Used when it takes a long time to transfer data over the network</li> <li>Takes around 2 weeks to transfer the data</li> <li>Snowball cannot import to Glacier directly (transfer to S3, configure a lifecycle policy to transition the data into Glacier)</li> <li>Pay per data transfer job</li> <li>Hardware devices for<ul> <li>Data Migration (between AWS &amp; on-premise data center)</li> <li>Edge Computing</li> </ul> </li> <li>Need to install OpsHub software on your computer to manage Snow Family devices</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Snow%20Family/#devices","title":"Devices","text":"<ul> <li>Snowcone<ul> <li>2 CPUs, 4GB RAM, wired or wireless access</li> <li>8 TB storage</li> <li>USB-C power using a cord or the optional battery</li> <li>Good for space-constrained environment</li> <li>DataSync Agent is preinstalled</li> <li>Does not support Storage Clustering</li> </ul> </li> <li>Snowball Edge<ul> <li>Compute Optimized<ul> <li>52 vCPUs, 208 GB of RAM</li> <li>42 TB storage</li> <li>Optional GPU (useful for video processing or machine learning)</li> <li>Supports Storage Clustering</li> </ul> </li> <li>Storage Optimized<ul> <li>Up to 40 CPUs, 80 GB of RAM</li> <li>80 TB storage</li> <li>Supports Storage Clustering (up to 15 nodes)</li> <li>Transfer up to petabytes</li> </ul> </li> </ul> </li> <li>Snowmobile<ul> <li>100 PB storage</li> <li>Used when transferring &gt; 10PB</li> <li>Transfer up to exabytes</li> <li>Does not support Storage Clustering</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Snow%20Family/#data-migration","title":"Data Migration","text":"<ul> <li>Provides block storage and Amazon S3-compatible object storage</li> <li>Usage process<ol> <li>Request Snowball devices from the AWS console for delivery</li> <li>Install the snowball client / AWS OpsHub on your servers</li> <li>Connect the snowball to your servers and copy files using the client</li> <li>Ship back the device when you\u2019re done (goes to the right AWS facility)</li> <li>Data will be loaded into an S3 bucket</li> <li>Snowball is completely wiped</li> </ol> </li> <li>Devices for data migration<ul> <li>Snowcone</li> <li>Snowball Edge - Storage Optimized</li> <li>Snowmobile</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Snow%20Family/#edge-computing","title":"Edge Computing","text":"<ul> <li>Process data while it\u2019s being created on an edge location (could be anything that doesn\u2019t have internet or access to cloud)</li> <li>Long-term deployment options for reduced cost (1 and 3 years discounted pricing)</li> <li>Devices for edge computing<ul> <li>Snowcone</li> <li>Snowball Edge</li> </ul> </li> <li>Can run EC2 Instances &amp; AWS Lambda functions locally on Snow device (using AWS loT Greengrass)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/","title":"Storage Gateway","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#storage-gateway","title":"Storage Gateway","text":"<ul> <li>Bridge between on-premises data and [[Simple Storage Service (S3)|S3]] for Hybrid Cloud</li> <li>Not suitable for one-time sync of large amounts of data (use [[DataSync]] instead)</li> <li>Optimizes data transfer by sending only changed data</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#types-of-storage-gateway","title":"Types of Storage Gateway","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#s3-file-gateway","title":"S3 File Gateway","text":"<ul> <li>Used to expand on-premise NFS by leveraging S3</li> <li>Configured S3 buckets are accessible on premises using the NFS and SMB protocol</li> <li>Data is cached at the file gateway for low latency access</li> <li>Can be mounted on many servers on-premises</li> <li>Integrated with Active Directory (AD) for user authentication</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509131940.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#volume-gateway","title":"Volume Gateway","text":"<ul> <li>Used for on-premise storage volumes</li> <li>Uses iSCSI protocol</li> <li>Two kinds of volumes:<ul> <li>Cached volumes: storage extension using S3 with caching at the volume gateway</li> <li>Stored volumes: entire dataset is on premise, scheduled backups to S3 as EBS snapshots</li> </ul> </li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509132530.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#tape-gateway","title":"Tape Gateway","text":"<ul> <li>Used to backup on-premises data using tape-based process to S3 as Virtual Tapes</li> <li>Uses iSCSI protocol</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509132930.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#fsx-file-gateway","title":"FSx File Gateway","text":"<ul> <li>Used to expand on-premise Windows-based storage by leveraging FSx for Windows</li> <li>Windows native compatibility (SMB, NTFS, Active Directory)</li> <li>Data is cached at the file gateway for low latency access</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220509133356.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Storage%20Gateway/#storage-gateway-hardware-appliance","title":"Storage Gateway - Hardware Appliance","text":"<ul> <li>Storage Gateway requires on-premises virtualization. If you don\u2019t have virtualization available, you can use a Storage Gateway - Hardware Appliance. It is a mini server that you need to install on-premises.</li> <li>Does not work with FSx File Gatway</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Traffic%20Mirroring/","title":"Traffic Mirroring","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Traffic%20Mirroring/#traffic-mirroring","title":"Traffic Mirroring","text":"<ul> <li>Capture and inspect network traffic in your [[Virtual Private Cloud (VPC)|VPC]] without disturbing the normal flow of traffic</li> <li>Inbound and outbound traffic through ENIs (eg. attached to EC2 instances) will be mirrored to the destination (NLB) for inspection without affecting the original traffic.</li> <li>Capture the traffic<ul> <li>From (Source) ENIs</li> <li>To (Targets) an ENI or [[Elastic Load Balancer (ELB)#Network Load Balancer NLB|NLB]]</li> </ul> </li> <li>Source and Target can be in the same or different VPCs (VPC Peering)</li> <li>Use cases: content inspection, threat monitoring, troubleshooting, etc.</li> </ul> <p>![[attachments/Pasted image 20220513004847.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transfer%20Family/","title":"Transfer Family","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transfer%20Family/#transfer-family","title":"Transfer Family","text":"<ul> <li>AWS managed service to transfer files in and out of [[Simple Storage Service (S3)|S3]] or [[Elastic File System (EFS)|EFS]] using FTP (instead of using proprietary methods)</li> <li>Supported Protocols<ul> <li>FTP (File Transfer Protocol) - unencrypted in flight</li> <li>FTPS (File Transfer Protocol over SSL) - encrypted in flight</li> <li>SFTP (Secure File Transfer Protocol) - encrypted in flight</li> </ul> </li> <li>Supports Multi AZ</li> <li>Pay per provisioned endpoint per hour + fee per GB data transfers</li> <li>Clients can either connect directly to the FTP endpoint or optionally through Route 53</li> <li>Transfer Family will need permission to read or put data into S3 or EFS</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transit%20Gateway/","title":"Transit Gateway","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transit%20Gateway/#transit-gateway","title":"Transit Gateway","text":"<ul> <li>Transitive peering between thousands of VPCs and on-premise data centers using hub-and-spoke (star) topology</li> <li>Works with Direct Connect Gateway, VPN Connection and VPC</li> <li>Bound to a region </li> <li>Transitive peering between VPCs in same region &amp; account</li> <li>Route Tables to control communication within the transitive network</li> <li>Supports IP Multicast (not supported by any other AWS service)</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220513002130.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transit%20Gateway/#increasing-bw-of-site-to-site-vpn-connection","title":"Increasing BW of Site-to-Site VPN connection","text":"<ul> <li>ECMP (equal-cost multi-path) routing is a routing strategy to allow to forward a packet over multiple best path</li> <li>To increase the bandwidth of the connection between Transit Gateway and corporate data center, create multiple site-to-site VPN connections, each with 2 tunnels (2 x 1.25 = 2.5 Gbps per VPN connection).<ul> <li>![[attachments/Pasted image 20220513002301.png]]</li> </ul> </li> <li>Only one VPN connection to a VPC having 2 tunnels out of which only 1 is used (1.25 Gbps)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Transit%20Gateway/#share-dx-between-multiple-accounts","title":"Share DX between multiple Accounts","text":"<p>Share Transit Gateway across accounts using [[Resource Access Manager (RAM)|Resource Access Manager]] to share [[Direct Connect (DX)|Direct Connect]] connection between VPCs in the same region but different accounts</p> <p>![[attachments/Pasted image 20220513003853.png]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Trusted%20Advisor/","title":"Trusted Advisor","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Trusted%20Advisor/#trusted-advisor","title":"Trusted Advisor","text":"<ul> <li>Service that analyzes your AWS accounts and provides recommendations on:<ul> <li>Cost Optimization<ul> <li>low utilization EC2 instances, EBS volumes, idle load balancers, etc.</li> <li>Reserved instances &amp; savings plans optimizations</li> </ul> </li> <li>Performance<ul> <li>High utilization EC2 instances, CloudFront CDN optimizations</li> <li>EC2 to EBS throughput optimizations, Alias records recommendations</li> </ul> </li> <li>Security:<ul> <li>MFA enabled on Root Account, IAM key rotation, exposed Access Keys</li> <li>S3 Bucket Permissions for public access, security groups with unrestricted ports</li> </ul> </li> <li>Fault Tolerance:<ul> <li>EBS snapshots age, Availability Zone Balance</li> <li>ASG Multi-AZ, RDS Multi-AZ, ELB configuration, etc.</li> </ul> </li> <li>Service Limits<ul> <li>whether or not you are reaching the service limit for a service and suggest you to increase the limit beforehand</li> </ul> </li> </ul> </li> <li>No installation needed</li> <li>Weekly email notifications</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/","title":"Virtual Private Cloud (VPC)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#virtual-private-cloud-vpc","title":"Virtual Private Cloud (VPC)","text":"<ul> <li>Soft limit of 5 VPCs per region</li> <li>Only the Private IPv4 ranges are allowed</li> <li>AWS accounts have a default VPC</li> <li>Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses and public and a private IPv4 DNS names</li> </ul> <p>New EC2 instances are launched into the default VPC if no subnet is specified</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#classless-inter-domain-routing-cidr","title":"Classless Inter-Domain Routing (CIDR)","text":"<ul> <li>Way to define a range of IP addresses<ul> <li>![[attachments/Pasted image 20220512094258.png]]</li> </ul> </li> <li>Two parts<ul> <li>Base IP - 192.168.0.0</li> <li>Subnet Mask (defines how many bits are frozen from the left side) - /16</li> </ul> </li> <li>Private IP ranges:<ul> <li>10.0.0.0 - 10.255.255.255 (10.0.0.0/8) \u21d2 used in big networks (24 bits can change)</li> <li>172.16.0.0 - 172.31.255.255 (172.16.0.0/12) \u21d2 AWS default VPC</li> <li>192.168.0.0 - 192.168.255.255 (192.168.0.0/16) \u21d2 home networks</li> </ul> </li> <li>Rest of the IP ranges are Public</li> <li>Max 5 CIDR ver VPC<ul> <li>Min. size is /28 (16 IP addresses)</li> <li>Max. size is /16 (65536 IP addresses)</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#subnets","title":"Subnets","text":"<ul> <li>Sub-ranges of IP addresses within the VPC</li> <li>Each subnet is bound to an AZ</li> <li>Subnets in a VPC cannot have overlapping CIDRs</li> <li>AWS reserves 5 IP addresses (first 4 &amp; last 1) in each subnet. These 5 IP addresses are not available for use.    Example: if CIDR block 10.0.0.0/24, then reserved IP addresses are 10.0.0.0, 10.0.0.1, 10.0.0.2, 10.0.0.3 &amp; 10.0.0.255</li> </ul> <p>To make the EC2 instances running in private subnets accessible on the internet, place them behind an internet-facing (running in public subnets) Elastic Load Balancer.</p> <p>Public subnets are subnets that have: -   \u201cAuto-assign public IPv4 address\u201d set to \u201cYes\u201d -   The subnet route table has an attached Internet Gateway</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#internet-gateway-igw","title":"Internet Gateway (IGW)","text":"<ul> <li>Allows resources in a VPC to connect to the Internet<ul> <li>![[attachments/Pasted image 20220512100002.png]]</li> </ul> </li> <li>Should be used to connect public resources to the internet (use NAT gateway for private resources)</li> <li>Route table of the public subnets must be edited to allow requests destined outside the VPC to be routed to the IGW<ul> <li>![[attachments/Pasted image 20220512222218.png]]</li> </ul> </li> <li>One IGW per VPC</li> </ul> <p>IGW performs network address translation (NAT) for a public EC2 instance</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#bastion-hosts","title":"Bastion Hosts","text":"<ul> <li>A EC2 instance running in the public subnet (accessible from public internet), to allow users to SSH into the instances in the private subnet.<ul> <li>![[attachments/Pasted image 20220512100455.png]]</li> </ul> </li> <li>Security groups of the private instances should only allow traffic from the bastion host.</li> <li>Bastion host should only allow port 22 traffic from the IP address you need (small instances are enough)</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#high-availability","title":"High Availability","text":"<ul> <li>HA options for Bastion Host<ul> <li>Run 2 Bastion Hosts across 2 AZ</li> <li>Run 1 Bastion Host across 2 AZ with ASG 1:1:1</li> </ul> </li> <li>Routing to the bastion host<ul> <li>If 1 bastion host, use an elastic IP with EC2 user-data script to access it</li> <li>If 2 bastion hosts, use a public-facing NLB (layer 4) deployed in multiple AZ. Bastion hosts can live in the private subnet (more secure)</li> </ul> </li> <li>Can\u2019t use ALB as it works in layer 7 (HTTP protocol) and SSH works with TCP</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220513222559.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#network-address-translation-nat-instance","title":"Network Address Translation (NAT) Instance","text":"<ul> <li>An [[Elastic Compute Cloud (EC2)|EC2]] instance launched in the public subnet which allows EC2 instances in private subnets to connect to the Internet without being connected from the internet (blocks inbound connection)<ul> <li>![[attachments/Pasted image 20220512101152.png]]</li> </ul> </li> <li>Must disable EC2 setting: source / destination IP check on the NAT instance as the IPs can change.</li> <li>Must have an Elastic IP attached to it</li> <li>Route Tables for private subnets must be configured to route internet-destined traffic to the NAT instance (its elastic IP)</li> <li>Can be used as a Bastion Host</li> <li>Disadvantages<ul> <li>Not highly available or resilient out of the box. Need to create an ASG in multi-AZ + resilient user-data script</li> <li>Internet traffic bandwidth depends on EC2 instance type</li> <li>You must manage [[Elastic Compute Cloud (EC2)#Security Groups|Security Groups]] &amp; rules:<ul> <li>Inbound:<ul> <li>Allow HTTP / HTTPS traffic coming from Private Subnets</li> <li>Allow SSH from your home network (access is provided through Internet Gateway)</li> </ul> </li> <li>Outbound:<ul> <li>Allow HTTP / HTTPS traffic to the Internet</li> </ul> </li> </ul> </li> </ul> </li> <li>Architecture<ul> <li>![[attachments/Pasted image 20220512101803.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#nat-gateway","title":"NAT Gateway","text":"<ul> <li>AWS managed NAT with bandwidth autoscaling (up to 45Gbps)</li> <li>Preferred over NAT instances</li> <li>Uses an Elastic IP behind the scenes</li> <li>Bound to an AZ</li> <li>Cannot be used by EC2 instances in the same subnet (only from other subnets)</li> <li>Cannot be used as a Bastion Host</li> <li>Route Tables for private subnets must be configured to route internet-destined traffic to the NAT gateway<ul> <li>![[attachments/Pasted image 20220512222148.png]]</li> </ul> </li> <li>No Security Groups to manage</li> <li>Pay per hour</li> <li>Architecture<ul> <li>![[attachments/Pasted image 20220512204306.png]]</li> </ul> </li> <li>High Availability<ul> <li>Create NAT gateways in public subnets bound to different AZ all routing outbound connections to the IGW (attached to the VPC)</li> <li>No cross-AZ failover needed because if an AZ goes down, all of the instances in that AZ also go down.</li> <li>![[attachments/Pasted image 20220512204520.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#dns-resolution-in-vpc","title":"DNS Resolution in VPC","text":"<ul> <li>Two settings need to be enabled to allow DNS resolution within a VPC:<ul> <li>DNS Support (enableDnsSupport)<ul> <li>Allows the resources within the VPC to query the DNS provided by Route 53 Resolver</li> <li>Enabled by default</li> <li>If disabled, we need to provide a custom DNS server otherwise we won\u2019t be able to reach hostnames</li> <li>Diagram<ul> <li>![[attachments/Pasted image 20220512210247.png]]</li> </ul> </li> </ul> </li> <li>DNS Hostnames (enableDnsHostnames)<ul> <li>Assigns public hostname to EC2 instances in our VPC if they have a public IPv4</li> <li>Doesn't work until <code>enableDnsSupport=true</code></li> <li>By default<ul> <li>Default VPC - Enabled</li> <li>Custom VPC - Disabled</li> </ul> </li> <li>When disabled, instances in the VPC will have a public IP but no public DNS</li> </ul> </li> </ul> </li> <li>If you use custom domain names in a Private Hosted Zone in Route 53, you must enable both of these settings<ul> <li>![[attachments/Pasted image 20220512210613.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#network-access-control-list-nacl","title":"Network Access Control List (NACL)","text":"<ul> <li>NACL are a firewall at the subnet level</li> <li>One NACL per subnet but a NACL can be attached to multiple subnets</li> <li>New subnets are assigned the Default NACL</li> <li>Default NACL allows all inbound &amp; outbound requests<ul> <li>![[attachments/Pasted image 20220512215745.png]]</li> </ul> </li> <li>NACL Rules<ul> <li>Rules number: 1-32766 (lower number has higher precedence)</li> <li>First rule match will drive the decision</li> <li>The last rule denies the request (only when no previous rule matches)</li> </ul> </li> <li>NACL vs [[Elastic Compute Cloud (EC2)#Security Groups|Security Group]]<ul> <li>NACL<ul> <li>Firewall for subnets</li> <li>Supports both Allow and Deny rules</li> <li>Stateless (both request and response will be evaluated against the NACL rules)</li> <li>Only the first matched rule is considered</li> </ul> </li> <li>Security Group: <ul> <li>Firewall for EC2 instances</li> <li>Supports only Allow rules</li> <li>Stateful (only request will be evaluated against the SG rules)</li> <li>All rules are evaluated</li> </ul> </li> <li>![[attachments/Pasted image 20220512220220.png]]</li> </ul> </li> <li>NACL with Ephemeral Ports<ul> <li>In the example below, the client EC2 instance needs to connect to DB instance. Since the ephemeral port can be randomly assigned from a range of ports, the Web Subnets\u2019s NACL must allow inbound traffic from that range of ports and similarly DB Subnet\u2019s NACL must allow outbound traffic on the same range of ports.</li> <li>![[attachments/Pasted image 20220512220647.png]]</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#vpc-peering","title":"VPC Peering","text":"<ul> <li>Connect two VPCs (could be in different region or account) using the AWS private network<ul> <li>![[attachments/Pasted image 20220512221728.png]]</li> </ul> </li> <li>Participating VPCs must have non-overlapping CIDR</li> <li>VPC Peering connection is non-transitive (A - B, B - C != A - C)</li> <li>Must update route tables in each VPC\u2019s subnets to ensure requests destined to the peered VPC can be routed through the peering connection<ul> <li>![[attachments/Pasted image 20220512221946.png]]</li> <li>![[attachments/Pasted image 20220512221954.png]]</li> </ul> </li> <li>You can reference a security group in a peered VPC across account or region. This allows us to use SG instead of CIDR when configuring rules.</li> </ul> <p>VPC Peering does not facilitate centrally-managed VPC like [[Resource Access Manager (RAM)#VPC Sharing|VPC Sharing]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#vpc-endpoints","title":"VPC Endpoints","text":"<ul> <li>Private endpoints within your VPC that allow AWS services to privately connect to resources within your VPC without traversing the public internet (cheaper)<ul> <li>![[attachments/Pasted image 20220512222517.png]]</li> </ul> </li> <li>Powered by AWS PrivateLink</li> <li>Route table is updated automatically</li> <li>Bound to a region (do not support inter-region communication)</li> <li>Two types:<ul> <li>Interface Endpoint<ul> <li>Provisions an ENI (private IP) as an entry point per subnet</li> <li>Need to attach a security group to the interface endpoint to control access</li> <li>Supports most AWS services</li> </ul> </li> <li>Gateway Endpoint<ul> <li>Provisions a gateway</li> <li>Must be used as a target in a route table</li> <li>Supports only S3 and DynamoDB</li> </ul> </li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#vpc-flow-logs","title":"VPC Flow Logs","text":"<ul> <li>Captures information about IP traffic going into your interfaces</li> <li>Three levels:<ul> <li>VPC Flow Logs</li> <li>Subnet Flow Logs</li> <li>ENI Flow Logs</li> </ul> </li> <li>Can be configured to show accepted, rejected or all traffic</li> <li>Flow logs data can be sent to S3 (bulk analytics) or CloudWatch Logs (near real-time decision making)</li> <li>Query VPC flow logs using Athena in S3 or CloudWatch Logs Insights</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#ipv6-support","title":"IPv6 Support","text":"<ul> <li>IPv4 cannot be disabled for your VPC</li> <li>Enable IPv6 to operate in dual-stack mode in which your EC2 instances will get at least a private IPv4 and a public IPv6. They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway.<ul> <li>![[attachments/Pasted image 20220513005218.png]]</li> </ul> </li> <li>If you cannot launch an EC2 instance in your subnet, It\u2019s not because it cannot acquire an IPv6 (the space is very large). It\u2019s because there are no available IPv4 in your subnet.    Solution: create a new IPv4 CIDR in your subnet</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#egress-only-internet-gateway","title":"Egress-only Internet Gateway","text":"<ul> <li>Allows instances in your VPC to initiate outbound connections over IPv6 while preventing inbound IPv6 connections to your private instances.<ul> <li>![[attachments/Pasted image 20220513005755.png]]</li> </ul> </li> <li>Similar to [[Virtual Private Cloud (VPC)#NAT Gateway|NAT Gateway]] but for IPv6</li> <li>Must update Route Tables</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Virtual%20Private%20Cloud%20%28VPC%29/#vpc-console-wizard","title":"VPC Console Wizard","text":"<ul> <li>Supported Configurations:</li> <li>VPC with a single public subnet</li> <li>VPC with public and private subnets (NAT)</li> <li>VPC with public and private subnets and AWS Site-to-Site VPN access</li> <li>VPC with a private subnet only and AWS Site-to-Site VPN access</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Web%20Application%20Firewall%20%28WAF%29/","title":"Web Application Firewall (WAF)","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Web%20Application%20Firewall%20%28WAF%29/#web-application-firewall-waf","title":"Web Application Firewall (WAF)","text":"<ul> <li>Protects your application from common layer 7 web exploits such as SQL Injection and Cross-Site Scripting (XSS)</li> <li>Layer 7 has more data about the structure of the incoming request than layer 4 (used by [[AWS Shield]])</li> <li>Can only be deployed on<ul> <li>[[Elastic Load Balancer (ELB)#Application Load Balancer ALB|Application Load Balancer]]</li> <li>[[API Gateway]]</li> <li>[[CloudFront]]</li> </ul> </li> <li>WAF contains Web ACL (Access Control List) containing rules to filter requests based on:<ul> <li>IP addresses</li> <li>HTTP headers</li> <li>HTTP body</li> <li>URI strings</li> <li>Size constraints (ex. max 5kb)</li> <li>Geo-match (block countries)</li> <li>Rate-based rules (to count occurrences of events per IP) for DDoS protection</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Well%20Architected%20Framework/","title":"Well Architected Framework","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Well%20Architected%20Framework/#well-architected-framework","title":"Well Architected Framework","text":"<ul> <li>Stop guessing your capacity needs (use auto-scaling)</li> <li>Test systems at production scale (for resiliency)</li> <li>Allow for evolutionary architectures</li> <li>Design based on changing requirements</li> <li>Drive architectures using data</li> <li>Load test your applications</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Well%20Architected%20Framework/#well-architected-pillars","title":"Well Architected Pillars","text":"<ol> <li>Operational Excellence</li> <li>Security</li> <li>Reliability</li> <li>Performance Efficiency</li> <li>Cost Optimization</li> <li>Sustainability</li> </ol>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Well%20Architected%20Framework/#aws-well-architected-tool","title":"AWS Well Architected Tool","text":"<ul> <li>Free tool to review your architectures against the 6 pillars framework and adopt architectural best practices</li> <li>How does it work?<ul> <li>Select your workload and answer questions</li> <li>Review your answers against the 6 pillars</li> <li>Obtain advice: get videos and documentations, generate a report, see the results in a dashboard</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Workflows/","title":"Workflows","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Workflows/#workflows","title":"Workflows","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Workflows/#step-functions","title":"Step Functions","text":"<ul> <li>Used to build serverless workflows to orchestrate Lambda functions</li> <li>Represent flow as a JSON state machine</li> <li>Maximum workflow execution time: 1 year</li> <li>Features: sequence, parallel, conditions, timeouts, error handling, etc.</li> <li>Provides a visual graph showing the current state and which path the workflow has taken</li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Workflows/#-attachmentspasted-image-20220513231346png","title":"- ![[attachments/Pasted image 20220513231346.png]]","text":""},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/Workflows/#simple-workflow-service-swf","title":"Simple Workflow Service (SWF)","text":"<ul> <li>Outdated service (step functions are preferred instead)</li> <li>Code runs on EC2 (not serverless)</li> <li>Ensures that a task is never duplicated (could replace standard SQS queues)</li> <li>1 year max runtime</li> <li>Built-in human intervention step</li> <li>Step Functions are recommended to be used for new applications, except:<ul> <li>If you need external signals to intervene in the processes</li> <li>If you need child processes that return values to parent processes</li> </ul> </li> </ul>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/X-Ray/","title":"X Ray","text":"<p>[[AWS Solutions Architect Associate (SAA-C02)]]</p>"},{"location":"Notes/AWS%20Solutions%20Architect%20Associate/X-Ray/#x-ray","title":"X-Ray","text":"<ul> <li>Provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components.</li> <li>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-services architecture. </li> <li>Helps understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. </li> <li>Can collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</li> </ul>"},{"location":"Notes/Programming/Asynchronous%20Programming/","title":"Asynchronous Programming","text":"<p>[[Notes]]</p>"},{"location":"Notes/Programming/Asynchronous%20Programming/#asynchronous-programming","title":"Asynchronous Programming","text":"<ul> <li>In synchronous programming, the code runs from top of the file to the bottom in a single thread</li> <li>In asynchronous programming, the code runs from the top of the file to the bottom but if it encounters some asynchronous pieces of code in file, a new thread will be given to execute each asynchronous task whereas the main thread continues executing the main code. </li> <li>A file containing asynchronous tasks will run in multiple threads (multi-threading). So, if a some task is taking some time to execute, it will not block other asynchronous tasks or the main code.</li> <li>Because each asynchronous task is handled by a separate thread, the code may not execute in order. ![[attachments/Pasted image 20220603182929.png]]</li> </ul>"},{"location":"Notes/Programming/Asynchronous%20Programming/#callback","title":"Callback","text":"<ul> <li>A callback is a function passed as an argument to another function which the calling function can invoke once it has done some task.  ```JS const order = (production: Function) =&gt; {     console.log('Order placed, calling production...')     production() }</li> </ul> <p>const production = () =&gt; {     console.log('Starting production') }</p> <p>// call the order function and tell what to do once the order is placed order(production); ```</p> <ul> <li>If we have multiple steps, each with a callback, then the resulting code has a ladder like appearance. This is called as the callback hell. To overcome this, we use promises and async-await syntax.<ul> <li>![[attachments/Pasted image 20220613194732.png]]</li> </ul> </li> </ul>"},{"location":"Notes/Programming/Asynchronous%20Programming/#resources","title":"Resources","text":"<p>Asynchronous JS | YouTube Asynchronous Vs Synchronous Programming - YouTube</p>"},{"location":"Notes/Programming/Programming/","title":"Programming","text":"<p>[[../Notes]]</p>"},{"location":"Notes/Programming/Programming/#programming","title":"Programming","text":"<p>[[Asynchronous Programming]]</p>"},{"location":"Notes/Static%20Site%20Generators/Gatsby/","title":"Gatsby","text":"<p>[[../Notes|Notes]]</p>"},{"location":"Notes/Static%20Site%20Generators/Gatsby/#gatsby","title":"Gatsby","text":"<ul> <li>Gatsby is a static site generator which generates HTML files from MarkDown files. These HTML files can be deployed on a remote server to serve as a website.  ^e967b3</li> <li>Great for personal portfolios </li> <li>Gatsby</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Gatsby/#themes","title":"Themes","text":"<ul> <li>Using a Gatsby theme, all of your website configuration is abstracted out of your site content, and into an installable package.</li> <li>The installation steps will be provided in the GitHub repo of the theme.</li> <li>Gatsby Casper Theme</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Gatsby/#gatsby-cli","title":"Gatsby CLI","text":""},{"location":"Notes/Static%20Site%20Generators/Gatsby/#installation","title":"Installation","text":"<pre><code>npm install -g gatsby-cli\n</code></pre>"},{"location":"Notes/Static%20Site%20Generators/Gatsby/#commands","title":"Commands","text":"<ul> <li>Create a new site <code>gatsby new site_name</code></li> <li>Start development server <code>gatsby develop</code></li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Hugo/","title":"Hugo","text":"<p>[[../Notes|Notes]]</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#hugo","title":"Hugo","text":"<p>Open-source static site generator based on GoLang</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#quick-start-guide","title":"Quick Start Guide","text":"<p>Quick Start | Hugo</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#points","title":"Points","text":"<ul> <li>Steps to install a theme is provided in the theme's GitHub repo</li> <li>Configurations depend upon the theme and are updated in <code>config.yml</code> file</li> <li><code>draft: true</code> makes a post only available during the development server and it will not be built for the final website</li> <li><code>baseURL</code> in <code>config.yml</code> is the base URL for your website</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#installation","title":"Installation","text":"<pre><code>brew install hugo\n</code></pre>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#themes","title":"Themes","text":"<p>Hugo PaperMod Theme</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#modifying-themes","title":"Modifying Themes","text":"<ul> <li>In the layouts folder, create folders to match the exact path of the file that you want to modify from the theme's layout folder (<code>themes/themeName/layout</code>)</li> <li>Copy the file to that path in the layouts folder and edit it</li> </ul> <p>If we edit the layouts of the theme directly, then the updates will get overwritten if we update the theme</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#commands","title":"Commands","text":""},{"location":"Notes/Static%20Site%20Generators/Hugo/#create-a-new-site","title":"Create a new site","text":"<p>With <code>.toml</code> config file <pre><code>hugo new site my-blog\n</code></pre> With <code>.yaml</code> config file <pre><code>hugo new site &lt;name of site&gt; -f yaml\n</code></pre></p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#create-a-new-post","title":"Create a new post","text":"<p><pre><code>hugo new posts/first-post.md\n</code></pre> This automatically adds front-matter to the created MD file</p>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#start-development-server","title":"Start development server","text":"<pre><code>hugo server -D\n</code></pre>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#build-static-files-for-deployment","title":"Build static files for deployment","text":"<pre><code>hugo\n</code></pre>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#folder-structure","title":"Folder Structure","text":"<ul> <li>Whenever you create a new Hugo site, these folders will be automatically created:<ul> <li><code>content</code> - for MD files</li> <li><code>layouts</code> - to overwrite default theme layouts</li> <li><code>static</code> - for static assets like images</li> <li><code>themes</code> - contains all the themes</li> <li><code>public</code> - static files are generated when building the site using <code>hugo</code> command</li> </ul> </li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#publish-to-netlify","title":"Publish to Netlify","text":"<ul> <li>We can link the GitHub repo for the site to Netlify to allow for automated deployment when we commit a change to the repo</li> <li>Netlify will auto build the site using the <code>hugo</code> command and publish the static files generated in the <code>public</code> folder</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#setup-steps","title":"Setup Steps","text":"<ul> <li>Go to Netlify</li> <li>Add new site &gt;&gt; Import an existing project &gt;&gt; GitHub</li> <li>Select Repo</li> <li>Configuration<ul> <li>Build command: <code>hugo</code></li> <li>Publish directory: <code>public</code></li> </ul> </li> <li>Click on Deploy</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Hugo/#resources","title":"Resources","text":"<p>Hugo Tutorial with PaperMod Theme | YouTube</p>"},{"location":"Notes/Static%20Site%20Generators/Note%20Sharing/","title":"Note Sharing","text":"<p>[[Notes]]</p>"},{"location":"Notes/Static%20Site%20Generators/Note%20Sharing/#note-sharing","title":"Note Sharing","text":""},{"location":"Notes/Static%20Site%20Generators/Note%20Sharing/#documents","title":"Documents","text":"<p>Getting started - Material for MkDocs (squidfunk.github.io)</p>"},{"location":"Notes/Static%20Site%20Generators/Note%20Sharing/#source-code","title":"Source Code","text":"<p>obsidian-to-mkdocs | GitHub</p>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/","title":"Portfolio","text":"<p>[[../Notes|Notes]]</p>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#portfolio","title":"Portfolio","text":"<ul> <li>Built using [[Hugo]]</li> <li>Hugo PaperMod Theme in Profile Info mode</li> <li>GitHub Repo</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#setup","title":"Setup","text":"<p>Create a new site <pre><code>hugo new site portfolio -f yaml\n</code></pre> <pre><code>cd portfolio\n</code></pre> Initialize Git and install Hugo PaperMod Theme <pre><code>git init\n</code></pre> <pre><code>git submodule add https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1\n</code></pre> Commit changes and publish repo.</p>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#updating-theme","title":"Updating Theme","text":"<pre><code>git submodule update --remote --merge\n</code></pre>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#favicon","title":"Favicon","text":"<ul> <li>Use vecteezy.com for logos</li> <li>Remove background using slazzer.com</li> <li>Convert to Favicon using Favicon Generator</li> <li>Put <code>favicon.ico</code> under the <code>static</code> folder</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#publish-to-netlify","title":"Publish to Netlify","text":"<ul> <li>Push the changes to remote to trigger auto build and deploy to Netlify</li> <li>Setup using [[Hugo#Publish to Netlify]]</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#custom-domain-in-netlify","title":"Custom Domain in Netlify","text":"<ul> <li>Open Netlify</li> <li>In the site's domain settings, add a custom domain <code>arkalim.org</code></li> <li>For the custom domain, go to DNS panel<ul> <li>![[../attachments/Pasted image 20220526205603.png]]</li> </ul> </li> <li>Copy the NS records provided and update the domain registrar's NS to these<ul> <li>![[../attachments/Pasted image 20220526205710.png]]</li> </ul> </li> <li>Wait for the NS records to change in the domain registrar</li> <li>In the HTTPS section, enable SSL encryption for free using Let's Encrypt</li> <li>Resource: How to connect a domain name to Netlify - YouTube</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#important","title":"Important","text":"<ul> <li>Use <code>#center</code> to center an image in MD <code>![name](path/to/image.png#center)</code></li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Portfolio/#theme-modifications","title":"Theme Modifications","text":"<ul> <li>Added a custom param <code>hideDescriptionInList</code>  to show or hide description in list view</li> <li>Added a new variable <code>dateString</code> to allow us to write date as string</li> <li>Changed footer text to: \u00a9 2022\u00a0Abdur Rahman</li> <li>Added empty character and a space in front of TOC</li> <li>Change cursor to pointer when expanding or collapsing TOC</li> <li>Changed font size of Post Meta to 14 and Description in List View to 18</li> <li>Made the header sticky (required update in the Mobile view)</li> </ul>"},{"location":"Notes/Static%20Site%20Generators/Static%20Site%20Generators/","title":"Static Site Generators","text":"<p>[[../Notes|Notes]]</p>"},{"location":"Notes/Static%20Site%20Generators/Static%20Site%20Generators/#static-site-generators","title":"Static Site Generators","text":""},{"location":"Notes/Static%20Site%20Generators/Static%20Site%20Generators/#packages","title":"Packages","text":"<p>[[Gatsby]] [[Hugo]]</p>"},{"location":"Notes/Static%20Site%20Generators/Static%20Site%20Generators/#applications","title":"Applications","text":"<p>[[Portfolio]] [[Note Sharing]]</p>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./","title":"Getting Started.","text":""},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#history","title":"History","text":"<ul> <li>Thompson and Dennis Richie of Bell Laboratories developed UNIX operating system in 1969.</li> <li>Linux Torvalds develop the Linux kernel in 1991</li> <li>Kernal control pretty much everything that happens on the system.</li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#choosing-a-linux-distribution","title":"Choosing a Linux Distribution","text":"<ul> <li>Linux system is divided into three main parts:<ul> <li>Hardware - This includes all hardware that your system runs on as well as memory, CPU, disks, etc.</li> <li>Linux Kernal - This is a core of the operating system, which manages hardware and tells it how to interact with the system.</li> <li>User Space - This is where users like us will directly interact with the system.</li> </ul> </li> <li>There are many Linux distributions to choose from, here are the most popular options.</li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#debian","title":"Debian","text":"<ul> <li>Overview<ul> <li>Debian is an operating system compressed entirely of free and open-source software.</li> <li>it has three branches Stable, Testing, and Unstable.</li> </ul> </li> <li>Package Management<ul> <li>Debian also uses Debian Package Management tools.</li> <li>Every Linux distribution has different package management tools.</li> </ul> </li> <li>Configurability<ul> <li>Debian may not get the latest updates, but it's extremely stable.</li> <li>This is the one if we want a good \"core\" operating system.</li> </ul> </li> <li>Uses<ul> <li>Debian is an overall operating system for any platform.</li> </ul> </li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#red-hat-enterprise-linux","title":"Red Hat Enterprise Linux","text":"<ul> <li>Overview<ul> <li>Red hat enterprise Linux is commonly referred to as RHEL and is developed by red hat.</li> <li>It has strict rules to restrict free re-distribution but still, source codes are free.</li> </ul> </li> <li>Package Management<ul> <li>RHEL uses a different package manager than Debian, RPM Package Manager.</li> </ul> </li> <li>Configurability<ul> <li>RHEL-based operating systems will differ slightly from Debian-Based systems.</li> <li>most noticeably in package management.</li> </ul> </li> <li>Uses:<ul> <li>As per the name described it is mostly used in the enterprise, so if we need a solid server OS this will be one.</li> </ul> </li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#ubuntu","title":"Ubuntu","text":"<ul> <li>Overview<ul> <li>One of the most popular Linux distributions for personal machines.</li> <li>Ubuntu also release its desktop environment manager.</li> </ul> </li> <li>Package Management<ul> <li>Ubuntu is Debian based operating system developed by Canonical, so it uses a core Debian package management system.</li> </ul> </li> <li>Configurability<ul> <li>Great choice for beginners to get into Linux.</li> <li>It offers a great user interface and which is why the wide adoption of Linux happens.</li> <li>It most liked other operating systems like OSX and windows in terms of uses</li> </ul> </li> <li>Uses<ul> <li>Great for any platform, Desktop, Laptop, and server.</li> </ul> </li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#fedora","title":"Fedora","text":"<ul> <li>Overview<ul> <li>Backed by a Red hat.</li> <li>Fedora is a community-driven containing open source and free software.</li> <li>Red hat enterprise Linux branches off a fedora.</li> <li>it's like upstream of the RHEL operating system.</li> <li>Eventually, REHL will get updates from fedora after testing and quality assurance.</li> <li>Fedora is an Ubuntu equivalent that uses Red Hat backend instead Debian.</li> </ul> </li> <li>Package Management<ul> <li>Uses Red hat package management.</li> </ul> </li> <li>configurable<ul> <li>If we want to use a red hat-based operating system, then this is a user-friendly version.</li> </ul> </li> <li>Uses<ul> <li>Fedora is great if we want to use a red hat-based OS without pricing tags, recommended for desktop and laptop</li> </ul> </li> </ul>"},{"location":"Parth%20Notes/Pre-Requisites/Linux%20Journeys/Grasshopper/Getting%20Started./#other-os","title":"Other OS","text":"<ul> <li>There is other os Like Linux Mint, Gentoo, Arch Linux, CentOS, Kali Linux, etc, but based on our use we will eventually have anyone from tons of Linux os.</li> </ul>"}]}